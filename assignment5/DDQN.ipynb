{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Mount"
      ],
      "metadata": {
        "id": "e4yRJWh0JjLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you will be prompted with a window asking to grant permissions\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejXavyldJipS",
        "outputId": "e4a34ce4-ceef-485f-8eea-306810dabcf0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the path in your Google Drive in the string below. Note: do not escape slashes or spaces\n",
        "import os\n",
        "datadir = \"/content/drive/My Drive/assignment5/\"\n",
        "if not os.path.exists(datadir):\n",
        "  !ln -s \"/content/drive/My Drive/assignment5/\" $datadir\n",
        "os.chdir(datadir)\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "depJqT4JJrMJ",
        "outputId": "89c20ff1-8e77-4daf-c067-89b307604c74"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/assignment5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojR8HIjLJFay"
      },
      "source": [
        "# Deep Q-Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_qbX5sSJFa2"
      },
      "source": [
        "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2UAI2BAJFa3",
        "outputId": "4706ec4f-c455-4635-a4a0-e4c5c1c75735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-2build1).\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gym pyvirtualdisplay\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO7CIp4HJFa4",
        "outputId": "3fd1e625-d8c3-49c0-9fa1-7bff3888f70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ez_setup in /usr/local/lib/python3.10/dist-packages (0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (1.22.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.27.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.3)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade setuptools --user\n",
        "!pip3 install ez_setup \n",
        "!pip3 install gym[atari] \n",
        "!pip3 install gym[accept-rom-license] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69KccY_OJFa5"
      },
      "source": [
        "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wNtHLwulJFa5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "from model import DQN\n",
        "from config import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY5m6LJVJFa6"
      },
      "source": [
        "## Understanding the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yi_DvVjJFa6"
      },
      "source": [
        "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
        "\n",
        "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jr_xOyc8JFa6"
      },
      "outputs": [],
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gXrFaJuUJFa7"
      },
      "outputs": [],
      "source": [
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVqODT6EJFa7"
      },
      "source": [
        "## Creating a DDQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYinTk6MJFa7"
      },
      "source": [
        "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
        "\n",
        "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
        "\n",
        "__Frame__ : Number of frames processed in total.\n",
        "\n",
        "__Memory Size__ : The current size of the replay memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KbSWkcecJFa_"
      },
      "outputs": [],
      "source": [
        "double_dqn = True # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "    from agent_double import Agent\n",
        "else:\n",
        "    from agent import Agent\n",
        "\n",
        "agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Training Loop"
      ],
      "metadata": {
        "id": "495PGA8R-iXB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoqPlKu9JFa8"
      },
      "source": [
        "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state, HISTORY_SIZE)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['lives'])\n",
        "\n",
        "        life = info['lives']\n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_ddqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_ddqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ghmv7l4G-x1o",
        "outputId": "dc2a8d79-b859-4998-b5ca-9c94f3c81ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0   score: 5.0   memory length: 344   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 5.0\n",
            "episode: 1   score: 1.0   memory length: 514   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 2   score: 5.0   memory length: 877   epsilon: 1.0    steps: 363    lr: 0.0001     evaluation reward: 3.6666666666666665\n",
            "episode: 3   score: 1.0   memory length: 1029   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 4   score: 1.0   memory length: 1202   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 5   score: 2.0   memory length: 1421   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 6   score: 2.0   memory length: 1640   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 2.4285714285714284\n",
            "episode: 7   score: 1.0   memory length: 1810   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 8   score: 3.0   memory length: 2055   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 2.3333333333333335\n",
            "episode: 9   score: 3.0   memory length: 2285   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 2.4\n",
            "episode: 10   score: 2.0   memory length: 2486   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 2.3636363636363638\n",
            "episode: 11   score: 4.0   memory length: 2748   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 12   score: 6.0   memory length: 3087   epsilon: 1.0    steps: 339    lr: 0.0001     evaluation reward: 2.769230769230769\n",
            "episode: 13   score: 3.0   memory length: 3353   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 2.7857142857142856\n",
            "episode: 14   score: 2.0   memory length: 3552   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 2.7333333333333334\n",
            "episode: 15   score: 2.0   memory length: 3750   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.6875\n",
            "episode: 16   score: 1.0   memory length: 3921   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 2.588235294117647\n",
            "episode: 17   score: 0.0   memory length: 4045   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 2.4444444444444446\n",
            "episode: 18   score: 1.0   memory length: 4196   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 2.3684210526315788\n",
            "episode: 19   score: 1.0   memory length: 4348   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 20   score: 3.0   memory length: 4574   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 2.3333333333333335\n",
            "episode: 21   score: 1.0   memory length: 4746   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 2.272727272727273\n",
            "episode: 22   score: 1.0   memory length: 4918   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 2.217391304347826\n",
            "episode: 23   score: 2.0   memory length: 5117   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 2.2083333333333335\n",
            "episode: 24   score: 2.0   memory length: 5334   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 25   score: 0.0   memory length: 5457   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.1153846153846154\n",
            "episode: 26   score: 2.0   memory length: 5658   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 2.111111111111111\n",
            "episode: 27   score: 2.0   memory length: 5877   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 2.107142857142857\n",
            "episode: 28   score: 0.0   memory length: 6000   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.0344827586206895\n",
            "episode: 29   score: 3.0   memory length: 6245   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 2.066666666666667\n",
            "episode: 30   score: 0.0   memory length: 6368   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 31   score: 1.0   memory length: 6538   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.96875\n",
            "episode: 32   score: 3.0   memory length: 6767   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 33   score: 1.0   memory length: 6940   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.9705882352941178\n",
            "episode: 34   score: 0.0   memory length: 7063   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9142857142857144\n",
            "episode: 35   score: 3.0   memory length: 7334   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.9444444444444444\n",
            "episode: 36   score: 3.0   memory length: 7560   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.972972972972973\n",
            "episode: 37   score: 0.0   memory length: 7684   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.9210526315789473\n",
            "episode: 38   score: 2.0   memory length: 7883   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.9230769230769231\n",
            "episode: 39   score: 2.0   memory length: 8081   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.925\n",
            "episode: 40   score: 2.0   memory length: 8300   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.9268292682926829\n",
            "episode: 41   score: 0.0   memory length: 8424   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.880952380952381\n",
            "episode: 42   score: 0.0   memory length: 8547   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8372093023255813\n",
            "episode: 43   score: 1.0   memory length: 8718   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.8181818181818181\n",
            "episode: 44   score: 3.0   memory length: 8989   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.8444444444444446\n",
            "episode: 45   score: 1.0   memory length: 9141   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.826086956521739\n",
            "episode: 46   score: 0.0   memory length: 9265   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7872340425531914\n",
            "episode: 47   score: 1.0   memory length: 9435   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7708333333333333\n",
            "episode: 48   score: 0.0   memory length: 9558   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7346938775510203\n",
            "episode: 49   score: 0.0   memory length: 9681   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 50   score: 2.0   memory length: 9879   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7058823529411764\n",
            "episode: 51   score: 3.0   memory length: 10141   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.7307692307692308\n",
            "episode: 52   score: 2.0   memory length: 10362   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.7358490566037736\n",
            "episode: 53   score: 2.0   memory length: 10561   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7407407407407407\n",
            "episode: 54   score: 0.0   memory length: 10684   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.709090909090909\n",
            "episode: 55   score: 0.0   memory length: 10808   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6785714285714286\n",
            "episode: 56   score: 0.0   memory length: 10932   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6491228070175439\n",
            "episode: 57   score: 2.0   memory length: 11131   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6551724137931034\n",
            "episode: 58   score: 1.0   memory length: 11283   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.6440677966101696\n",
            "episode: 59   score: 3.0   memory length: 11510   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
            "episode: 60   score: 2.0   memory length: 11729   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6721311475409837\n",
            "episode: 61   score: 0.0   memory length: 11853   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6451612903225807\n",
            "episode: 62   score: 2.0   memory length: 12055   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.6507936507936507\n",
            "episode: 63   score: 0.0   memory length: 12179   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.625\n",
            "episode: 64   score: 2.0   memory length: 12378   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6307692307692307\n",
            "episode: 65   score: 0.0   memory length: 12502   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.606060606060606\n",
            "episode: 66   score: 2.0   memory length: 12701   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6119402985074627\n",
            "episode: 67   score: 0.0   memory length: 12824   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.588235294117647\n",
            "episode: 68   score: 5.0   memory length: 13133   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.6376811594202898\n",
            "episode: 69   score: 0.0   memory length: 13256   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6142857142857143\n",
            "episode: 70   score: 2.0   memory length: 13476   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.619718309859155\n",
            "episode: 71   score: 2.0   memory length: 13675   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.625\n",
            "episode: 72   score: 0.0   memory length: 13798   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6027397260273972\n",
            "episode: 73   score: 1.0   memory length: 13970   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5945945945945945\n",
            "episode: 74   score: 2.0   memory length: 14186   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 75   score: 3.0   memory length: 14414   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.618421052631579\n",
            "episode: 76   score: 0.0   memory length: 14538   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5974025974025974\n",
            "episode: 77   score: 0.0   memory length: 14661   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5769230769230769\n",
            "episode: 78   score: 3.0   memory length: 14928   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.5949367088607596\n",
            "episode: 79   score: 3.0   memory length: 15175   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6125\n",
            "episode: 80   score: 1.0   memory length: 15347   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.6049382716049383\n",
            "episode: 81   score: 3.0   memory length: 15594   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6219512195121952\n",
            "episode: 82   score: 2.0   memory length: 15793   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6265060240963856\n",
            "episode: 83   score: 2.0   memory length: 16014   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.630952380952381\n",
            "episode: 84   score: 0.0   memory length: 16138   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.611764705882353\n",
            "episode: 85   score: 0.0   memory length: 16262   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5930232558139534\n",
            "episode: 86   score: 3.0   memory length: 16492   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.6091954022988506\n",
            "episode: 87   score: 4.0   memory length: 16790   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
            "episode: 88   score: 2.0   memory length: 16988   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6404494382022472\n",
            "episode: 89   score: 2.0   memory length: 17210   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.6444444444444444\n",
            "episode: 90   score: 4.0   memory length: 17485   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.6703296703296704\n",
            "episode: 91   score: 2.0   memory length: 17667   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.673913043478261\n",
            "episode: 92   score: 0.0   memory length: 17791   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6559139784946237\n",
            "episode: 93   score: 3.0   memory length: 18042   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.6702127659574468\n",
            "episode: 94   score: 3.0   memory length: 18290   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.6842105263157894\n",
            "episode: 95   score: 1.0   memory length: 18443   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.6770833333333333\n",
            "episode: 96   score: 1.0   memory length: 18595   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.6701030927835052\n",
            "episode: 97   score: 3.0   memory length: 18843   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.683673469387755\n",
            "episode: 98   score: 2.0   memory length: 19023   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.6868686868686869\n",
            "episode: 99   score: 0.0   memory length: 19147   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 100   score: 3.0   memory length: 19395   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 101   score: 3.0   memory length: 19622   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 102   score: 2.0   memory length: 19821   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 103   score: 1.0   memory length: 19973   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 104   score: 2.0   memory length: 20172   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 105   score: 0.0   memory length: 20295   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 106   score: 0.0   memory length: 20418   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 107   score: 1.0   memory length: 20588   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 108   score: 3.0   memory length: 20854   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 109   score: 1.0   memory length: 21023   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 110   score: 0.0   memory length: 21147   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 111   score: 4.0   memory length: 21437   epsilon: 1.0    steps: 290    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 112   score: 2.0   memory length: 21638   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 113   score: 2.0   memory length: 21859   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 114   score: 0.0   memory length: 21983   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 115   score: 2.0   memory length: 22181   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 116   score: 2.0   memory length: 22382   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 117   score: 2.0   memory length: 22602   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 118   score: 3.0   memory length: 22849   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 119   score: 0.0   memory length: 22973   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 120   score: 1.0   memory length: 23124   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 121   score: 1.0   memory length: 23275   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 122   score: 1.0   memory length: 23427   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 123   score: 1.0   memory length: 23597   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 124   score: 1.0   memory length: 23767   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 125   score: 2.0   memory length: 23966   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 126   score: 0.0   memory length: 24089   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 127   score: 2.0   memory length: 24288   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 128   score: 0.0   memory length: 24412   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 129   score: 1.0   memory length: 24564   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 130   score: 3.0   memory length: 24832   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 131   score: 1.0   memory length: 25001   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 132   score: 2.0   memory length: 25200   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 133   score: 0.0   memory length: 25324   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 134   score: 3.0   memory length: 25551   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 135   score: 2.0   memory length: 25752   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 136   score: 7.0   memory length: 26018   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 137   score: 2.0   memory length: 26239   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 138   score: 1.0   memory length: 26412   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 139   score: 2.0   memory length: 26636   epsilon: 1.0    steps: 224    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 140   score: 1.0   memory length: 26788   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 141   score: 0.0   memory length: 26912   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 142   score: 2.0   memory length: 27113   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 143   score: 2.0   memory length: 27312   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 144   score: 0.0   memory length: 27436   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 145   score: 2.0   memory length: 27655   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 146   score: 9.0   memory length: 27978   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 147   score: 0.0   memory length: 28102   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 148   score: 2.0   memory length: 28301   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 149   score: 0.0   memory length: 28425   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 150   score: 2.0   memory length: 28624   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 151   score: 2.0   memory length: 28822   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 152   score: 0.0   memory length: 28945   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 153   score: 2.0   memory length: 29126   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 154   score: 1.0   memory length: 29296   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 155   score: 1.0   memory length: 29466   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 156   score: 3.0   memory length: 29692   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 157   score: 3.0   memory length: 29919   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 158   score: 1.0   memory length: 30088   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 159   score: 4.0   memory length: 30347   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 160   score: 0.0   memory length: 30470   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 161   score: 3.0   memory length: 30704   epsilon: 1.0    steps: 234    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 162   score: 0.0   memory length: 30827   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 163   score: 3.0   memory length: 31071   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 164   score: 3.0   memory length: 31298   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 165   score: 1.0   memory length: 31470   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 166   score: 0.0   memory length: 31593   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 167   score: 1.0   memory length: 31763   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 168   score: 0.0   memory length: 31886   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 169   score: 0.0   memory length: 32010   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 170   score: 2.0   memory length: 32209   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 171   score: 4.0   memory length: 32503   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 172   score: 2.0   memory length: 32706   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 173   score: 0.0   memory length: 32829   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 174   score: 1.0   memory length: 32999   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 175   score: 0.0   memory length: 33123   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 176   score: 2.0   memory length: 33325   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 177   score: 0.0   memory length: 33448   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 178   score: 9.0   memory length: 33837   epsilon: 1.0    steps: 389    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 179   score: 0.0   memory length: 33961   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 180   score: 3.0   memory length: 34226   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 181   score: 1.0   memory length: 34377   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 182   score: 0.0   memory length: 34501   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 183   score: 3.0   memory length: 34748   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 184   score: 3.0   memory length: 34993   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 185   score: 4.0   memory length: 35295   epsilon: 1.0    steps: 302    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 186   score: 3.0   memory length: 35524   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 187   score: 2.0   memory length: 35722   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 188   score: 1.0   memory length: 35874   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 189   score: 1.0   memory length: 36028   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 190   score: 1.0   memory length: 36199   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 191   score: 4.0   memory length: 36457   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 192   score: 5.0   memory length: 36781   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 193   score: 3.0   memory length: 37010   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 194   score: 1.0   memory length: 37179   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 195   score: 1.0   memory length: 37330   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 196   score: 1.0   memory length: 37502   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 197   score: 3.0   memory length: 37729   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 198   score: 0.0   memory length: 37852   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 199   score: 1.0   memory length: 38025   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 200   score: 3.0   memory length: 38251   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 201   score: 2.0   memory length: 38471   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 202   score: 0.0   memory length: 38594   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 203   score: 1.0   memory length: 38764   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 204   score: 0.0   memory length: 38888   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 205   score: 2.0   memory length: 39090   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 206   score: 4.0   memory length: 39366   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 207   score: 0.0   memory length: 39490   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 208   score: 3.0   memory length: 39736   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 209   score: 5.0   memory length: 40073   epsilon: 1.0    steps: 337    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 210   score: 1.0   memory length: 40242   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 211   score: 1.0   memory length: 40393   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 212   score: 2.0   memory length: 40612   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 213   score: 2.0   memory length: 40811   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 214   score: 1.0   memory length: 40981   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 215   score: 0.0   memory length: 41105   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 216   score: 1.0   memory length: 41275   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 217   score: 8.0   memory length: 41730   epsilon: 1.0    steps: 455    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 218   score: 0.0   memory length: 41853   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 219   score: 2.0   memory length: 42052   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 220   score: 2.0   memory length: 42236   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 221   score: 3.0   memory length: 42505   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 222   score: 1.0   memory length: 42658   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 223   score: 0.0   memory length: 42782   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 224   score: 4.0   memory length: 43097   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 225   score: 3.0   memory length: 43345   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 226   score: 5.0   memory length: 43653   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 227   score: 3.0   memory length: 43900   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 228   score: 2.0   memory length: 44119   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 229   score: 1.0   memory length: 44289   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 230   score: 0.0   memory length: 44413   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 231   score: 0.0   memory length: 44536   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 232   score: 1.0   memory length: 44708   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 233   score: 3.0   memory length: 44938   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 234   score: 1.0   memory length: 45090   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 235   score: 1.0   memory length: 45242   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 236   score: 2.0   memory length: 45441   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 237   score: 0.0   memory length: 45565   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 238   score: 4.0   memory length: 45856   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 239   score: 3.0   memory length: 46104   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 240   score: 6.0   memory length: 46458   epsilon: 1.0    steps: 354    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 241   score: 2.0   memory length: 46675   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 242   score: 4.0   memory length: 46932   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 243   score: 0.0   memory length: 47056   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 244   score: 2.0   memory length: 47254   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 245   score: 6.0   memory length: 47647   epsilon: 1.0    steps: 393    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 246   score: 0.0   memory length: 47771   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 247   score: 0.0   memory length: 47894   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 248   score: 2.0   memory length: 48093   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 249   score: 1.0   memory length: 48265   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 250   score: 2.0   memory length: 48464   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 251   score: 2.0   memory length: 48665   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 252   score: 2.0   memory length: 48882   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 253   score: 4.0   memory length: 49160   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 254   score: 3.0   memory length: 49408   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 255   score: 1.0   memory length: 49577   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 256   score: 0.0   memory length: 49700   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 257   score: 2.0   memory length: 49899   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 258   score: 1.0   memory length: 50072   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 259   score: 2.0   memory length: 50292   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 260   score: 2.0   memory length: 50493   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 261   score: 1.0   memory length: 50645   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 262   score: 3.0   memory length: 50911   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 263   score: 1.0   memory length: 51063   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 264   score: 0.0   memory length: 51187   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 265   score: 2.0   memory length: 51405   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 266   score: 2.0   memory length: 51604   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 267   score: 3.0   memory length: 51871   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 268   score: 1.0   memory length: 52041   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 269   score: 2.0   memory length: 52240   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 270   score: 1.0   memory length: 52391   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 271   score: 3.0   memory length: 52662   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 272   score: 3.0   memory length: 52889   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 273   score: 2.0   memory length: 53089   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 274   score: 2.0   memory length: 53287   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 275   score: 6.0   memory length: 53540   epsilon: 1.0    steps: 253    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 276   score: 4.0   memory length: 53838   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 277   score: 2.0   memory length: 54037   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 278   score: 0.0   memory length: 54161   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 279   score: 2.0   memory length: 54380   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 280   score: 4.0   memory length: 54692   epsilon: 1.0    steps: 312    lr: 0.0001     evaluation reward: 2.03\n",
            "episode: 281   score: 0.0   memory length: 54816   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 282   score: 0.0   memory length: 54939   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 283   score: 1.0   memory length: 55091   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 284   score: 1.0   memory length: 55263   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 285   score: 2.0   memory length: 55480   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 286   score: 0.0   memory length: 55604   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 287   score: 1.0   memory length: 55756   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 288   score: 2.0   memory length: 55975   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 289   score: 0.0   memory length: 56099   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 290   score: 2.0   memory length: 56298   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 291   score: 4.0   memory length: 56569   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 292   score: 1.0   memory length: 56721   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 293   score: 2.0   memory length: 56937   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 294   score: 2.0   memory length: 57158   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 295   score: 2.0   memory length: 57375   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 296   score: 2.0   memory length: 57594   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 297   score: 0.0   memory length: 57718   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 298   score: 1.0   memory length: 57889   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 299   score: 1.0   memory length: 58061   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 300   score: 3.0   memory length: 58311   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 301   score: 1.0   memory length: 58481   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 302   score: 5.0   memory length: 58825   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 303   score: 4.0   memory length: 59119   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 304   score: 0.0   memory length: 59242   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 305   score: 2.0   memory length: 59461   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 306   score: 0.0   memory length: 59585   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 307   score: 0.0   memory length: 59709   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 308   score: 0.0   memory length: 59833   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 309   score: 2.0   memory length: 60031   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 310   score: 2.0   memory length: 60211   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 311   score: 0.0   memory length: 60335   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 312   score: 1.0   memory length: 60504   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 313   score: 0.0   memory length: 60628   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 314   score: 1.0   memory length: 60781   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 315   score: 1.0   memory length: 60954   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 316   score: 0.0   memory length: 61078   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 317   score: 0.0   memory length: 61202   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 318   score: 3.0   memory length: 61446   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 319   score: 0.0   memory length: 61569   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 320   score: 0.0   memory length: 61693   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 321   score: 1.0   memory length: 61845   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 322   score: 1.0   memory length: 61997   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 323   score: 1.0   memory length: 62167   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 324   score: 0.0   memory length: 62291   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 325   score: 2.0   memory length: 62492   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 326   score: 2.0   memory length: 62691   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 327   score: 0.0   memory length: 62815   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 328   score: 0.0   memory length: 62939   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 329   score: 2.0   memory length: 63138   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 330   score: 2.0   memory length: 63357   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 331   score: 2.0   memory length: 63556   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 332   score: 1.0   memory length: 63708   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 333   score: 1.0   memory length: 63879   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 334   score: 4.0   memory length: 64155   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 335   score: 1.0   memory length: 64324   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 336   score: 0.0   memory length: 64448   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 337   score: 4.0   memory length: 64722   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 338   score: 5.0   memory length: 65052   epsilon: 1.0    steps: 330    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 339   score: 0.0   memory length: 65175   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 340   score: 2.0   memory length: 65394   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 341   score: 0.0   memory length: 65518   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 342   score: 0.0   memory length: 65641   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 343   score: 1.0   memory length: 65792   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 344   score: 2.0   memory length: 65994   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 345   score: 5.0   memory length: 66330   epsilon: 1.0    steps: 336    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 346   score: 3.0   memory length: 66601   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 347   score: 2.0   memory length: 66801   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 348   score: 1.0   memory length: 66953   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 349   score: 2.0   memory length: 67172   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 350   score: 7.0   memory length: 67548   epsilon: 1.0    steps: 376    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 351   score: 0.0   memory length: 67672   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 352   score: 1.0   memory length: 67844   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 353   score: 2.0   memory length: 68043   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 354   score: 3.0   memory length: 68270   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 355   score: 3.0   memory length: 68539   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 356   score: 2.0   memory length: 68760   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 357   score: 0.0   memory length: 68884   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 358   score: 2.0   memory length: 69105   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 359   score: 0.0   memory length: 69228   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 360   score: 2.0   memory length: 69427   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 361   score: 1.0   memory length: 69599   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 362   score: 2.0   memory length: 69798   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 363   score: 2.0   memory length: 69997   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 364   score: 0.0   memory length: 70121   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 365   score: 2.0   memory length: 70340   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 366   score: 0.0   memory length: 70463   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 367   score: 1.0   memory length: 70632   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 368   score: 1.0   memory length: 70803   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 369   score: 0.0   memory length: 70926   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 370   score: 1.0   memory length: 71096   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 371   score: 0.0   memory length: 71220   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 372   score: 1.0   memory length: 71371   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 373   score: 3.0   memory length: 71599   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 374   score: 0.0   memory length: 71723   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 375   score: 0.0   memory length: 71847   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 376   score: 2.0   memory length: 72066   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 377   score: 6.0   memory length: 72385   epsilon: 1.0    steps: 319    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 378   score: 0.0   memory length: 72508   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 379   score: 2.0   memory length: 72707   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 380   score: 0.0   memory length: 72831   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 381   score: 0.0   memory length: 72954   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 382   score: 2.0   memory length: 73152   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 383   score: 4.0   memory length: 73432   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 384   score: 2.0   memory length: 73633   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 385   score: 4.0   memory length: 73912   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 386   score: 2.0   memory length: 74111   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 387   score: 2.0   memory length: 74332   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 388   score: 4.0   memory length: 74608   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 389   score: 2.0   memory length: 74827   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 390   score: 0.0   memory length: 74950   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 391   score: 2.0   memory length: 75131   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 392   score: 2.0   memory length: 75330   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 393   score: 2.0   memory length: 75529   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 394   score: 2.0   memory length: 75752   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 395   score: 1.0   memory length: 75904   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 396   score: 0.0   memory length: 76028   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 397   score: 2.0   memory length: 76248   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 398   score: 1.0   memory length: 76420   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 399   score: 0.0   memory length: 76544   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 400   score: 1.0   memory length: 76714   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 401   score: 4.0   memory length: 76975   epsilon: 1.0    steps: 261    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 402   score: 3.0   memory length: 77204   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 403   score: 1.0   memory length: 77373   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 404   score: 2.0   memory length: 77571   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 405   score: 1.0   memory length: 77723   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 406   score: 0.0   memory length: 77847   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 407   score: 0.0   memory length: 77971   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 408   score: 0.0   memory length: 78094   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 409   score: 2.0   memory length: 78312   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 410   score: 1.0   memory length: 78465   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 411   score: 1.0   memory length: 78634   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 412   score: 3.0   memory length: 78902   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 413   score: 6.0   memory length: 79155   epsilon: 1.0    steps: 253    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 414   score: 6.0   memory length: 79525   epsilon: 1.0    steps: 370    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 415   score: 0.0   memory length: 79649   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 416   score: 0.0   memory length: 79772   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 417   score: 0.0   memory length: 79896   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 418   score: 2.0   memory length: 80095   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 419   score: 3.0   memory length: 80321   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 420   score: 2.0   memory length: 80520   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 421   score: 0.0   memory length: 80644   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 422   score: 0.0   memory length: 80767   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 423   score: 2.0   memory length: 80986   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 424   score: 1.0   memory length: 81156   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 425   score: 1.0   memory length: 81328   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 426   score: 0.0   memory length: 81452   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 427   score: 3.0   memory length: 81720   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 428   score: 0.0   memory length: 81843   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 429   score: 2.0   memory length: 82041   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 430   score: 1.0   memory length: 82193   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 431   score: 2.0   memory length: 82391   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 432   score: 5.0   memory length: 82743   epsilon: 1.0    steps: 352    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 433   score: 3.0   memory length: 82970   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 434   score: 0.0   memory length: 83093   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 435   score: 1.0   memory length: 83245   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 436   score: 2.0   memory length: 83445   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 437   score: 3.0   memory length: 83714   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 438   score: 3.0   memory length: 83926   epsilon: 1.0    steps: 212    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 439   score: 2.0   memory length: 84144   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 440   score: 2.0   memory length: 84365   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 441   score: 2.0   memory length: 84564   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 442   score: 2.0   memory length: 84786   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 443   score: 1.0   memory length: 84956   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 444   score: 3.0   memory length: 85188   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 445   score: 0.0   memory length: 85312   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 446   score: 0.0   memory length: 85436   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 447   score: 0.0   memory length: 85559   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 448   score: 0.0   memory length: 85683   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 449   score: 2.0   memory length: 85882   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 450   score: 0.0   memory length: 86006   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 451   score: 3.0   memory length: 86272   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 452   score: 2.0   memory length: 86474   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 453   score: 8.0   memory length: 86824   epsilon: 1.0    steps: 350    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 454   score: 2.0   memory length: 87023   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 455   score: 2.0   memory length: 87222   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 456   score: 2.0   memory length: 87439   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 457   score: 1.0   memory length: 87611   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 458   score: 1.0   memory length: 87781   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 459   score: 1.0   memory length: 87954   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 460   score: 2.0   memory length: 88153   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 461   score: 0.0   memory length: 88277   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 462   score: 4.0   memory length: 88595   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 463   score: 1.0   memory length: 88748   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 464   score: 2.0   memory length: 88946   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 465   score: 1.0   memory length: 89097   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 466   score: 2.0   memory length: 89315   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 467   score: 1.0   memory length: 89467   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 468   score: 2.0   memory length: 89687   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 469   score: 0.0   memory length: 89811   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 470   score: 2.0   memory length: 90010   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 471   score: 0.0   memory length: 90134   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 472   score: 1.0   memory length: 90303   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 473   score: 0.0   memory length: 90427   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 474   score: 0.0   memory length: 90551   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 475   score: 0.0   memory length: 90675   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 476   score: 1.0   memory length: 90826   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 477   score: 1.0   memory length: 90978   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 478   score: 2.0   memory length: 91196   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 479   score: 2.0   memory length: 91397   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 480   score: 1.0   memory length: 91567   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 481   score: 0.0   memory length: 91691   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 482   score: 1.0   memory length: 91864   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 483   score: 2.0   memory length: 92080   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 484   score: 0.0   memory length: 92204   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 485   score: 1.0   memory length: 92373   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 486   score: 2.0   memory length: 92593   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 487   score: 1.0   memory length: 92762   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 488   score: 1.0   memory length: 92932   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 489   score: 3.0   memory length: 93182   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 490   score: 0.0   memory length: 93306   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 491   score: 1.0   memory length: 93458   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 492   score: 1.0   memory length: 93629   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 493   score: 0.0   memory length: 93752   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 494   score: 2.0   memory length: 93950   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 495   score: 2.0   memory length: 94131   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 496   score: 1.0   memory length: 94283   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 497   score: 0.0   memory length: 94407   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 498   score: 3.0   memory length: 94675   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 499   score: 1.0   memory length: 94827   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 500   score: 0.0   memory length: 94951   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 501   score: 0.0   memory length: 95074   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 502   score: 2.0   memory length: 95272   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 503   score: 2.0   memory length: 95492   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 504   score: 2.0   memory length: 95691   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 505   score: 1.0   memory length: 95863   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 506   score: 2.0   memory length: 96084   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 507   score: 2.0   memory length: 96304   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 508   score: 1.0   memory length: 96456   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 509   score: 3.0   memory length: 96721   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 510   score: 1.0   memory length: 96873   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 511   score: 1.0   memory length: 97025   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 512   score: 3.0   memory length: 97273   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 513   score: 0.0   memory length: 97397   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 514   score: 0.0   memory length: 97521   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 515   score: 4.0   memory length: 97806   epsilon: 1.0    steps: 285    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 516   score: 2.0   memory length: 98005   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 517   score: 2.0   memory length: 98204   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 518   score: 3.0   memory length: 98451   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 519   score: 1.0   memory length: 98603   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 520   score: 2.0   memory length: 98822   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 521   score: 2.0   memory length: 99042   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 522   score: 2.0   memory length: 99260   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 523   score: 2.0   memory length: 99480   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 524   score: 1.0   memory length: 99632   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 525   score: 3.0   memory length: 99879   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 526   score: 1.0   memory length: 100031   epsilon: 0.9999366400000014    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 527   score: 1.0   memory length: 100183   epsilon: 0.9996356800000079    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 528   score: 1.0   memory length: 100353   epsilon: 0.9992990800000152    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 529   score: 3.0   memory length: 100579   epsilon: 0.9988516000000249    steps: 226    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 530   score: 3.0   memory length: 100825   epsilon: 0.9983645200000355    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 531   score: 0.0   memory length: 100949   epsilon: 0.9981190000000408    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 532   score: 1.0   memory length: 101119   epsilon: 0.9977824000000481    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 533   score: 1.0   memory length: 101289   epsilon: 0.9974458000000554    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 534   score: 0.0   memory length: 101412   epsilon: 0.9972022600000607    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 535   score: 5.0   memory length: 101775   epsilon: 0.9964835200000763    steps: 363    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 536   score: 0.0   memory length: 101898   epsilon: 0.9962399800000816    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 537   score: 1.0   memory length: 102068   epsilon: 0.9959033800000889    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 538   score: 1.0   memory length: 102237   epsilon: 0.9955687600000962    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 539   score: 0.0   memory length: 102361   epsilon: 0.9953232400001015    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 540   score: 0.0   memory length: 102485   epsilon: 0.9950777200001069    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 541   score: 4.0   memory length: 102781   epsilon: 0.9944916400001196    steps: 296    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 542   score: 3.0   memory length: 103051   epsilon: 0.9939570400001312    steps: 270    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 543   score: 2.0   memory length: 103249   epsilon: 0.9935650000001397    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 544   score: 1.0   memory length: 103401   epsilon: 0.9932640400001462    steps: 152    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 545   score: 0.0   memory length: 103524   epsilon: 0.9930205000001515    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 546   score: 0.0   memory length: 103648   epsilon: 0.9927749800001568    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 547   score: 0.0   memory length: 103772   epsilon: 0.9925294600001622    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 548   score: 0.0   memory length: 103896   epsilon: 0.9922839400001675    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 549   score: 2.0   memory length: 104115   epsilon: 0.9918503200001769    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 550   score: 2.0   memory length: 104315   epsilon: 0.9914543200001855    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 551   score: 1.0   memory length: 104488   epsilon: 0.991111780000193    steps: 173    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 552   score: 4.0   memory length: 104743   epsilon: 0.9906068800002039    steps: 255    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 553   score: 0.0   memory length: 104867   epsilon: 0.9903613600002092    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 554   score: 0.0   memory length: 104991   epsilon: 0.9901158400002146    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 555   score: 0.0   memory length: 105115   epsilon: 0.9898703200002199    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 556   score: 2.0   memory length: 105313   epsilon: 0.9894782800002284    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 557   score: 1.0   memory length: 105486   epsilon: 0.9891357400002359    steps: 173    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 558   score: 2.0   memory length: 105685   epsilon: 0.9887417200002444    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 559   score: 0.0   memory length: 105809   epsilon: 0.9884962000002497    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 560   score: 2.0   memory length: 106007   epsilon: 0.9881041600002582    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 561   score: 4.0   memory length: 106305   epsilon: 0.9875141200002711    steps: 298    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 562   score: 5.0   memory length: 106596   epsilon: 0.9869379400002836    steps: 291    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 563   score: 4.0   memory length: 106911   epsilon: 0.9863142400002971    steps: 315    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 564   score: 2.0   memory length: 107127   epsilon: 0.9858865600003064    steps: 216    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 565   score: 1.0   memory length: 107279   epsilon: 0.9855856000003129    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 566   score: 4.0   memory length: 107596   epsilon: 0.9849579400003265    steps: 317    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 567   score: 2.0   memory length: 107795   epsilon: 0.9845639200003351    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 568   score: 1.0   memory length: 107967   epsilon: 0.9842233600003425    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 569   score: 0.0   memory length: 108091   epsilon: 0.9839778400003478    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 570   score: 2.0   memory length: 108289   epsilon: 0.9835858000003563    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 571   score: 4.0   memory length: 108587   epsilon: 0.9829957600003691    steps: 298    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 572   score: 3.0   memory length: 108814   epsilon: 0.9825463000003789    steps: 227    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 573   score: 2.0   memory length: 109012   epsilon: 0.9821542600003874    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 574   score: 1.0   memory length: 109183   epsilon: 0.9818156800003948    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 575   score: 0.0   memory length: 109307   epsilon: 0.9815701600004001    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 576   score: 0.0   memory length: 109431   epsilon: 0.9813246400004054    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 577   score: 0.0   memory length: 109554   epsilon: 0.9810811000004107    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 578   score: 2.0   memory length: 109754   epsilon: 0.9806851000004193    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 579   score: 2.0   memory length: 109974   epsilon: 0.9802495000004288    steps: 220    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 580   score: 3.0   memory length: 110241   epsilon: 0.9797208400004402    steps: 267    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 581   score: 2.0   memory length: 110442   epsilon: 0.9793228600004489    steps: 201    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 582   score: 1.0   memory length: 110612   epsilon: 0.9789862600004562    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 583   score: 1.0   memory length: 110764   epsilon: 0.9786853000004627    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 584   score: 3.0   memory length: 111012   epsilon: 0.9781942600004734    steps: 248    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 585   score: 0.0   memory length: 111135   epsilon: 0.9779507200004787    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 586   score: 2.0   memory length: 111353   epsilon: 0.977519080000488    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 587   score: 3.0   memory length: 111605   epsilon: 0.9770201200004989    steps: 252    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 588   score: 0.0   memory length: 111729   epsilon: 0.9767746000005042    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 589   score: 3.0   memory length: 112000   epsilon: 0.9762380200005158    steps: 271    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 590   score: 3.0   memory length: 112230   epsilon: 0.9757826200005257    steps: 230    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 591   score: 2.0   memory length: 112450   epsilon: 0.9753470200005352    steps: 220    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 592   score: 2.0   memory length: 112649   epsilon: 0.9749530000005437    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 593   score: 1.0   memory length: 112819   epsilon: 0.974616400000551    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 594   score: 6.0   memory length: 113159   epsilon: 0.9739432000005657    steps: 340    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 595   score: 0.0   memory length: 113283   epsilon: 0.973697680000571    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 596   score: 4.0   memory length: 113582   epsilon: 0.9731056600005838    steps: 299    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 597   score: 3.0   memory length: 113809   epsilon: 0.9726562000005936    steps: 227    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 598   score: 0.0   memory length: 113933   epsilon: 0.9724106800005989    steps: 124    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 599   score: 0.0   memory length: 114056   epsilon: 0.9721671400006042    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 600   score: 2.0   memory length: 114255   epsilon: 0.9717731200006128    steps: 199    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 601   score: 1.0   memory length: 114427   epsilon: 0.9714325600006202    steps: 172    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 602   score: 7.0   memory length: 114822   epsilon: 0.9706504600006371    steps: 395    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 603   score: 1.0   memory length: 114991   epsilon: 0.9703158400006444    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 604   score: 0.0   memory length: 115115   epsilon: 0.9700703200006497    steps: 124    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 605   score: 1.0   memory length: 115288   epsilon: 0.9697277800006572    steps: 173    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 606   score: 2.0   memory length: 115486   epsilon: 0.9693357400006657    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 607   score: 2.0   memory length: 115705   epsilon: 0.9689021200006751    steps: 219    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 608   score: 2.0   memory length: 115922   epsilon: 0.9684724600006844    steps: 217    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 609   score: 1.0   memory length: 116074   epsilon: 0.968171500000691    steps: 152    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 610   score: 1.0   memory length: 116226   epsilon: 0.9678705400006975    steps: 152    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 611   score: 3.0   memory length: 116454   epsilon: 0.9674191000007073    steps: 228    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 612   score: 2.0   memory length: 116673   epsilon: 0.9669854800007167    steps: 219    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 613   score: 0.0   memory length: 116796   epsilon: 0.966741940000722    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 614   score: 0.0   memory length: 116919   epsilon: 0.9664984000007273    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 615   score: 0.0   memory length: 117043   epsilon: 0.9662528800007326    steps: 124    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 616   score: 0.0   memory length: 117167   epsilon: 0.966007360000738    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 617   score: 3.0   memory length: 117415   epsilon: 0.9655163200007486    steps: 248    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 618   score: 3.0   memory length: 117664   epsilon: 0.9650233000007593    steps: 249    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 619   score: 0.0   memory length: 117788   epsilon: 0.9647777800007646    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 620   score: 0.0   memory length: 117912   epsilon: 0.96453226000077    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 621   score: 4.0   memory length: 118229   epsilon: 0.9639046000007836    steps: 317    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 622   score: 2.0   memory length: 118447   epsilon: 0.963472960000793    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 623   score: 3.0   memory length: 118714   epsilon: 0.9629443000008044    steps: 267    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 624   score: 0.0   memory length: 118838   epsilon: 0.9626987800008098    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 625   score: 2.0   memory length: 119037   epsilon: 0.9623047600008183    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 626   score: 1.0   memory length: 119207   epsilon: 0.9619681600008256    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 627   score: 0.0   memory length: 119331   epsilon: 0.961722640000831    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 628   score: 0.0   memory length: 119454   epsilon: 0.9614791000008363    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 629   score: 1.0   memory length: 119606   epsilon: 0.9611781400008428    steps: 152    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 630   score: 3.0   memory length: 119855   epsilon: 0.9606851200008535    steps: 249    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 631   score: 2.0   memory length: 120075   epsilon: 0.9602495200008629    steps: 220    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 632   score: 0.0   memory length: 120199   epsilon: 0.9600040000008683    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 633   score: 2.0   memory length: 120418   epsilon: 0.9595703800008777    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 634   score: 0.0   memory length: 120542   epsilon: 0.959324860000883    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 635   score: 0.0   memory length: 120666   epsilon: 0.9590793400008883    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 636   score: 2.0   memory length: 120884   epsilon: 0.9586477000008977    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 637   score: 3.0   memory length: 121150   epsilon: 0.9581210200009092    steps: 266    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 638   score: 2.0   memory length: 121349   epsilon: 0.9577270000009177    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 639   score: 0.0   memory length: 121473   epsilon: 0.957481480000923    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 640   score: 4.0   memory length: 121748   epsilon: 0.9569369800009349    steps: 275    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 641   score: 0.0   memory length: 121872   epsilon: 0.9566914600009402    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 642   score: 1.0   memory length: 122023   epsilon: 0.9563924800009467    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 643   score: 1.0   memory length: 122193   epsilon: 0.956055880000954    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 644   score: 2.0   memory length: 122411   epsilon: 0.9556242400009634    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 645   score: 0.0   memory length: 122535   epsilon: 0.9553787200009687    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 646   score: 2.0   memory length: 122737   epsilon: 0.9549787600009774    steps: 202    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 647   score: 2.0   memory length: 122936   epsilon: 0.9545847400009859    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 648   score: 0.0   memory length: 123060   epsilon: 0.9543392200009913    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 649   score: 2.0   memory length: 123277   epsilon: 0.9539095600010006    steps: 217    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 650   score: 2.0   memory length: 123497   epsilon: 0.95347396000101    steps: 220    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 651   score: 0.0   memory length: 123621   epsilon: 0.9532284400010154    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 652   score: 3.0   memory length: 123849   epsilon: 0.9527770000010252    steps: 228    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 653   score: 3.0   memory length: 124096   epsilon: 0.9522879400010358    steps: 247    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 654   score: 0.0   memory length: 124220   epsilon: 0.9520424200010411    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 655   score: 0.0   memory length: 124344   epsilon: 0.9517969000010464    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 656   score: 3.0   memory length: 124593   epsilon: 0.9513038800010571    steps: 249    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 657   score: 3.0   memory length: 124802   epsilon: 0.9508900600010661    steps: 209    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 658   score: 2.0   memory length: 125000   epsilon: 0.9504980200010746    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 659   score: 1.0   memory length: 125151   epsilon: 0.9501990400010811    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 660   score: 2.0   memory length: 125350   epsilon: 0.9498050200010897    steps: 199    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 661   score: 3.0   memory length: 125621   epsilon: 0.9492684400011013    steps: 271    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 662   score: 5.0   memory length: 125960   epsilon: 0.9485972200011159    steps: 339    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 663   score: 2.0   memory length: 126182   epsilon: 0.9481576600011254    steps: 222    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 664   score: 0.0   memory length: 126306   epsilon: 0.9479121400011308    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 665   score: 1.0   memory length: 126476   epsilon: 0.9475755400011381    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 666   score: 0.0   memory length: 126600   epsilon: 0.9473300200011434    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 667   score: 0.0   memory length: 126724   epsilon: 0.9470845000011487    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 668   score: 0.0   memory length: 126848   epsilon: 0.9468389800011541    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 669   score: 5.0   memory length: 127189   epsilon: 0.9461638000011687    steps: 341    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 670   score: 4.0   memory length: 127466   epsilon: 0.9456153400011806    steps: 277    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 671   score: 4.0   memory length: 127762   epsilon: 0.9450292600011934    steps: 296    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 672   score: 3.0   memory length: 128011   epsilon: 0.9445362400012041    steps: 249    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 673   score: 1.0   memory length: 128163   epsilon: 0.9442352800012106    steps: 152    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 674   score: 2.0   memory length: 128383   epsilon: 0.94379968000122    steps: 220    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 675   score: 2.0   memory length: 128582   epsilon: 0.9434056600012286    steps: 199    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 676   score: 2.0   memory length: 128781   epsilon: 0.9430116400012372    steps: 199    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 677   score: 0.0   memory length: 128905   epsilon: 0.9427661200012425    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 678   score: 2.0   memory length: 129086   epsilon: 0.9424077400012503    steps: 181    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 679   score: 4.0   memory length: 129403   epsilon: 0.9417800800012639    steps: 317    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 680   score: 2.0   memory length: 129619   epsilon: 0.9413524000012732    steps: 216    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 681   score: 5.0   memory length: 129984   epsilon: 0.9406297000012889    steps: 365    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 682   score: 6.0   memory length: 130321   epsilon: 0.9399624400013034    steps: 337    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 683   score: 0.0   memory length: 130445   epsilon: 0.9397169200013087    steps: 124    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 684   score: 1.0   memory length: 130597   epsilon: 0.9394159600013152    steps: 152    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 685   score: 1.0   memory length: 130749   epsilon: 0.9391150000013218    steps: 152    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 686   score: 2.0   memory length: 130972   epsilon: 0.9386734600013313    steps: 223    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 687   score: 2.0   memory length: 131189   epsilon: 0.9382438000013407    steps: 217    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 688   score: 0.0   memory length: 131313   epsilon: 0.937998280001346    steps: 124    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 689   score: 0.0   memory length: 131436   epsilon: 0.9377547400013513    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 690   score: 4.0   memory length: 131707   epsilon: 0.9372181600013629    steps: 271    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 691   score: 2.0   memory length: 131889   epsilon: 0.9368578000013708    steps: 182    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 692   score: 0.0   memory length: 132013   epsilon: 0.9366122800013761    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 693   score: 0.0   memory length: 132137   epsilon: 0.9363667600013814    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 694   score: 0.0   memory length: 132261   epsilon: 0.9361212400013867    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 695   score: 0.0   memory length: 132385   epsilon: 0.9358757200013921    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 696   score: 2.0   memory length: 132586   epsilon: 0.9354777400014007    steps: 201    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 697   score: 2.0   memory length: 132788   epsilon: 0.9350777800014094    steps: 202    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 698   score: 0.0   memory length: 132912   epsilon: 0.9348322600014147    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 699   score: 2.0   memory length: 133094   epsilon: 0.9344719000014226    steps: 182    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 700   score: 2.0   memory length: 133293   epsilon: 0.9340778800014311    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 701   score: 1.0   memory length: 133445   epsilon: 0.9337769200014376    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 702   score: 3.0   memory length: 133692   epsilon: 0.9332878600014483    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 703   score: 2.0   memory length: 133891   epsilon: 0.9328938400014568    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 704   score: 3.0   memory length: 134157   epsilon: 0.9323671600014682    steps: 266    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 705   score: 0.0   memory length: 134280   epsilon: 0.9321236200014735    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 706   score: 0.0   memory length: 134403   epsilon: 0.9318800800014788    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 707   score: 4.0   memory length: 134678   epsilon: 0.9313355800014906    steps: 275    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 708   score: 5.0   memory length: 135021   epsilon: 0.9306564400015054    steps: 343    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 709   score: 1.0   memory length: 135173   epsilon: 0.9303554800015119    steps: 152    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 710   score: 3.0   memory length: 135400   epsilon: 0.9299060200015217    steps: 227    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 711   score: 1.0   memory length: 135552   epsilon: 0.9296050600015282    steps: 152    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 712   score: 2.0   memory length: 135770   epsilon: 0.9291734200015376    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 713   score: 0.0   memory length: 135894   epsilon: 0.9289279000015429    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 714   score: 0.0   memory length: 136017   epsilon: 0.9286843600015482    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 715   score: 1.0   memory length: 136169   epsilon: 0.9283834000015547    steps: 152    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 716   score: 4.0   memory length: 136451   epsilon: 0.9278250400015668    steps: 282    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 717   score: 0.0   memory length: 136575   epsilon: 0.9275795200015722    steps: 124    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 718   score: 2.0   memory length: 136795   epsilon: 0.9271439200015816    steps: 220    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 719   score: 0.0   memory length: 136919   epsilon: 0.926898400001587    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 720   score: 3.0   memory length: 137145   epsilon: 0.9264509200015967    steps: 226    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 721   score: 3.0   memory length: 137373   epsilon: 0.9259994800016065    steps: 228    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 722   score: 3.0   memory length: 137617   epsilon: 0.925516360001617    steps: 244    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 723   score: 1.0   memory length: 137769   epsilon: 0.9252154000016235    steps: 152    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 724   score: 3.0   memory length: 137996   epsilon: 0.9247659400016333    steps: 227    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 725   score: 1.0   memory length: 138166   epsilon: 0.9244293400016406    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 726   score: 4.0   memory length: 138426   epsilon: 0.9239145400016517    steps: 260    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 727   score: 2.0   memory length: 138609   epsilon: 0.9235522000016596    steps: 183    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 728   score: 3.0   memory length: 138836   epsilon: 0.9231027400016694    steps: 227    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 729   score: 2.0   memory length: 139035   epsilon: 0.9227087200016779    steps: 199    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 730   score: 2.0   memory length: 139254   epsilon: 0.9222751000016873    steps: 219    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 731   score: 1.0   memory length: 139406   epsilon: 0.9219741400016939    steps: 152    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 732   score: 0.0   memory length: 139529   epsilon: 0.9217306000016992    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 733   score: 0.0   memory length: 139653   epsilon: 0.9214850800017045    steps: 124    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 734   score: 0.0   memory length: 139777   epsilon: 0.9212395600017098    steps: 124    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 735   score: 3.0   memory length: 140007   epsilon: 0.9207841600017197    steps: 230    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 736   score: 3.0   memory length: 140236   epsilon: 0.9203307400017295    steps: 229    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 737   score: 0.0   memory length: 140360   epsilon: 0.9200852200017349    steps: 124    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 738   score: 0.0   memory length: 140483   epsilon: 0.9198416800017402    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 739   score: 2.0   memory length: 140682   epsilon: 0.9194476600017487    steps: 199    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 740   score: 1.0   memory length: 140854   epsilon: 0.9191071000017561    steps: 172    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 741   score: 1.0   memory length: 141024   epsilon: 0.9187705000017634    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 742   score: 1.0   memory length: 141197   epsilon: 0.9184279600017708    steps: 173    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 743   score: 1.0   memory length: 141370   epsilon: 0.9180854200017783    steps: 173    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 744   score: 0.0   memory length: 141494   epsilon: 0.9178399000017836    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 745   score: 3.0   memory length: 141746   epsilon: 0.9173409400017944    steps: 252    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 746   score: 3.0   memory length: 141993   epsilon: 0.9168518800018051    steps: 247    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 747   score: 6.0   memory length: 142349   epsilon: 0.9161470000018204    steps: 356    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 748   score: 0.0   memory length: 142473   epsilon: 0.9159014800018257    steps: 124    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 749   score: 3.0   memory length: 142740   epsilon: 0.9153728200018372    steps: 267    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 750   score: 3.0   memory length: 142970   epsilon: 0.9149174200018471    steps: 230    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 751   score: 2.0   memory length: 143169   epsilon: 0.9145234000018556    steps: 199    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 752   score: 2.0   memory length: 143352   epsilon: 0.9141610600018635    steps: 183    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 753   score: 3.0   memory length: 143618   epsilon: 0.9136343800018749    steps: 266    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 754   score: 4.0   memory length: 143895   epsilon: 0.9130859200018868    steps: 277    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 755   score: 2.0   memory length: 144094   epsilon: 0.9126919000018954    steps: 199    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 756   score: 0.0   memory length: 144218   epsilon: 0.9124463800019007    steps: 124    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 757   score: 1.0   memory length: 144391   epsilon: 0.9121038400019081    steps: 173    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 758   score: 2.0   memory length: 144593   epsilon: 0.9117038800019168    steps: 202    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 759   score: 2.0   memory length: 144810   epsilon: 0.9112742200019261    steps: 217    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 760   score: 1.0   memory length: 144962   epsilon: 0.9109732600019327    steps: 152    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 761   score: 4.0   memory length: 145281   epsilon: 0.9103416400019464    steps: 319    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 762   score: 3.0   memory length: 145510   epsilon: 0.9098882200019562    steps: 229    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 763   score: 3.0   memory length: 145759   epsilon: 0.9093952000019669    steps: 249    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 764   score: 5.0   memory length: 146126   epsilon: 0.9086685400019827    steps: 367    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 765   score: 3.0   memory length: 146354   epsilon: 0.9082171000019925    steps: 228    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 766   score: 3.0   memory length: 146580   epsilon: 0.9077696200020022    steps: 226    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 767   score: 3.0   memory length: 146830   epsilon: 0.907274620002013    steps: 250    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 768   score: 4.0   memory length: 147144   epsilon: 0.9066529000020265    steps: 314    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 769   score: 0.0   memory length: 147268   epsilon: 0.9064073800020318    steps: 124    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 770   score: 0.0   memory length: 147391   epsilon: 0.9061638400020371    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 771   score: 2.0   memory length: 147573   epsilon: 0.9058034800020449    steps: 182    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 772   score: 2.0   memory length: 147793   epsilon: 0.9053678800020544    steps: 220    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 773   score: 2.0   memory length: 147992   epsilon: 0.9049738600020629    steps: 199    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 774   score: 0.0   memory length: 148116   epsilon: 0.9047283400020683    steps: 124    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 775   score: 1.0   memory length: 148286   epsilon: 0.9043917400020756    steps: 170    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 776   score: 3.0   memory length: 148533   epsilon: 0.9039026800020862    steps: 247    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 777   score: 3.0   memory length: 148798   epsilon: 0.9033779800020976    steps: 265    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 778   score: 0.0   memory length: 148922   epsilon: 0.9031324600021029    steps: 124    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 779   score: 0.0   memory length: 149046   epsilon: 0.9028869400021082    steps: 124    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 780   score: 0.0   memory length: 149170   epsilon: 0.9026414200021136    steps: 124    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 781   score: 2.0   memory length: 149369   epsilon: 0.9022474000021221    steps: 199    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 782   score: 0.0   memory length: 149493   epsilon: 0.9020018800021274    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 783   score: 2.0   memory length: 149692   epsilon: 0.901607860002136    steps: 199    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 784   score: 0.0   memory length: 149815   epsilon: 0.9013643200021413    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 785   score: 2.0   memory length: 150017   epsilon: 0.90096436000215    steps: 202    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 786   score: 4.0   memory length: 150299   epsilon: 0.9004060000021621    steps: 282    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 787   score: 0.0   memory length: 150423   epsilon: 0.9001604800021674    steps: 124    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 788   score: 6.0   memory length: 150802   epsilon: 0.8994100600021837    steps: 379    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 789   score: 3.0   memory length: 151032   epsilon: 0.8989546600021936    steps: 230    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 790   score: 1.0   memory length: 151203   epsilon: 0.898616080002201    steps: 171    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 791   score: 0.0   memory length: 151327   epsilon: 0.8983705600022063    steps: 124    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 792   score: 2.0   memory length: 151526   epsilon: 0.8979765400022148    steps: 199    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 793   score: 3.0   memory length: 151772   epsilon: 0.8974894600022254    steps: 246    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 794   score: 3.0   memory length: 152037   epsilon: 0.8969647600022368    steps: 265    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 795   score: 0.0   memory length: 152161   epsilon: 0.8967192400022421    steps: 124    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 796   score: 0.0   memory length: 152285   epsilon: 0.8964737200022475    steps: 124    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 797   score: 5.0   memory length: 152593   epsilon: 0.8958638800022607    steps: 308    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 798   score: 2.0   memory length: 152791   epsilon: 0.8954718400022692    steps: 198    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 799   score: 0.0   memory length: 152915   epsilon: 0.8952263200022745    steps: 124    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 800   score: 5.0   memory length: 153242   epsilon: 0.8945788600022886    steps: 327    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 801   score: 4.0   memory length: 153516   epsilon: 0.8940363400023004    steps: 274    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 802   score: 0.0   memory length: 153640   epsilon: 0.8937908200023057    steps: 124    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 803   score: 1.0   memory length: 153792   epsilon: 0.8934898600023122    steps: 152    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 804   score: 2.0   memory length: 153991   epsilon: 0.8930958400023208    steps: 199    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 805   score: 2.0   memory length: 154174   epsilon: 0.8927335000023287    steps: 183    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 806   score: 1.0   memory length: 154345   epsilon: 0.892394920002336    steps: 171    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 807   score: 3.0   memory length: 154593   epsilon: 0.8919038800023467    steps: 248    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 808   score: 0.0   memory length: 154717   epsilon: 0.891658360002352    steps: 124    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 809   score: 2.0   memory length: 154898   epsilon: 0.8912999800023598    steps: 181    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 810   score: 3.0   memory length: 155167   epsilon: 0.8907673600023713    steps: 269    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 811   score: 3.0   memory length: 155421   epsilon: 0.8902644400023823    steps: 254    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 812   score: 2.0   memory length: 155620   epsilon: 0.8898704200023908    steps: 199    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 813   score: 0.0   memory length: 155744   epsilon: 0.8896249000023961    steps: 124    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 814   score: 0.0   memory length: 155868   epsilon: 0.8893793800024015    steps: 124    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 815   score: 1.0   memory length: 156039   epsilon: 0.8890408000024088    steps: 171    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 816   score: 4.0   memory length: 156316   epsilon: 0.8884923400024207    steps: 277    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 817   score: 1.0   memory length: 156470   epsilon: 0.8881874200024273    steps: 154    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 818   score: 1.0   memory length: 156622   epsilon: 0.8878864600024339    steps: 152    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 819   score: 1.0   memory length: 156773   epsilon: 0.8875874800024404    steps: 151    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 820   score: 1.0   memory length: 156924   epsilon: 0.8872885000024469    steps: 151    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 821   score: 3.0   memory length: 157190   epsilon: 0.8867618200024583    steps: 266    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 822   score: 2.0   memory length: 157388   epsilon: 0.8863697800024668    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 823   score: 2.0   memory length: 157586   epsilon: 0.8859777400024753    steps: 198    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 824   score: 7.0   memory length: 157868   epsilon: 0.8854193800024874    steps: 282    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 825   score: 3.0   memory length: 158118   epsilon: 0.8849243800024982    steps: 250    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 826   score: 3.0   memory length: 158345   epsilon: 0.8844749200025079    steps: 227    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 827   score: 3.0   memory length: 158616   epsilon: 0.8839383400025196    steps: 271    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 828   score: 1.0   memory length: 158785   epsilon: 0.8836037200025268    steps: 169    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 829   score: 0.0   memory length: 158909   epsilon: 0.8833582000025322    steps: 124    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 830   score: 1.0   memory length: 159061   epsilon: 0.8830572400025387    steps: 152    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 831   score: 0.0   memory length: 159185   epsilon: 0.882811720002544    steps: 124    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 832   score: 3.0   memory length: 159432   epsilon: 0.8823226600025547    steps: 247    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 833   score: 0.0   memory length: 159556   epsilon: 0.88207714000256    steps: 124    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 834   score: 6.0   memory length: 159908   epsilon: 0.8813801800025751    steps: 352    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 835   score: 1.0   memory length: 160081   epsilon: 0.8810376400025826    steps: 173    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 836   score: 7.0   memory length: 160363   epsilon: 0.8804792800025947    steps: 282    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 837   score: 7.0   memory length: 160661   epsilon: 0.8798892400026075    steps: 298    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 838   score: 0.0   memory length: 160785   epsilon: 0.8796437200026128    steps: 124    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 839   score: 1.0   memory length: 160956   epsilon: 0.8793051400026202    steps: 171    lr: 0.0001     evaluation reward: 2.04\n",
            "episode: 840   score: 2.0   memory length: 161155   epsilon: 0.8789111200026287    steps: 199    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 841   score: 3.0   memory length: 161381   epsilon: 0.8784636400026384    steps: 226    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 842   score: 1.0   memory length: 161552   epsilon: 0.8781250600026458    steps: 171    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 843   score: 4.0   memory length: 161870   epsilon: 0.8774954200026595    steps: 318    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 844   score: 2.0   memory length: 162089   epsilon: 0.8770618000026689    steps: 219    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 845   score: 2.0   memory length: 162308   epsilon: 0.8766281800026783    steps: 219    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 846   score: 0.0   memory length: 162432   epsilon: 0.8763826600026836    steps: 124    lr: 0.0001     evaluation reward: 2.08\n",
            "episode: 847   score: 4.0   memory length: 162690   epsilon: 0.8758718200026947    steps: 258    lr: 0.0001     evaluation reward: 2.06\n",
            "episode: 848   score: 2.0   memory length: 162889   epsilon: 0.8754778000027033    steps: 199    lr: 0.0001     evaluation reward: 2.08\n",
            "episode: 849   score: 5.0   memory length: 163234   epsilon: 0.8747947000027181    steps: 345    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 850   score: 3.0   memory length: 163445   epsilon: 0.8743769200027272    steps: 211    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 851   score: 3.0   memory length: 163712   epsilon: 0.8738482600027386    steps: 267    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 852   score: 4.0   memory length: 163973   epsilon: 0.8733314800027498    steps: 261    lr: 0.0001     evaluation reward: 2.13\n",
            "episode: 853   score: 6.0   memory length: 164317   epsilon: 0.8726503600027646    steps: 344    lr: 0.0001     evaluation reward: 2.16\n",
            "episode: 854   score: 2.0   memory length: 164515   epsilon: 0.8722583200027731    steps: 198    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 855   score: 0.0   memory length: 164639   epsilon: 0.8720128000027785    steps: 124    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 856   score: 0.0   memory length: 164763   epsilon: 0.8717672800027838    steps: 124    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 857   score: 0.0   memory length: 164887   epsilon: 0.8715217600027891    steps: 124    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 858   score: 2.0   memory length: 165104   epsilon: 0.8710921000027985    steps: 217    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 859   score: 2.0   memory length: 165285   epsilon: 0.8707337200028062    steps: 181    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 860   score: 6.0   memory length: 165663   epsilon: 0.8699852800028225    steps: 378    lr: 0.0001     evaluation reward: 2.16\n",
            "episode: 861   score: 2.0   memory length: 165879   epsilon: 0.8695576000028318    steps: 216    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 862   score: 1.0   memory length: 166049   epsilon: 0.8692210000028391    steps: 170    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 863   score: 0.0   memory length: 166173   epsilon: 0.8689754800028444    steps: 124    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 864   score: 3.0   memory length: 166422   epsilon: 0.8684824600028551    steps: 249    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 865   score: 6.0   memory length: 166677   epsilon: 0.8679775600028661    steps: 255    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 866   score: 0.0   memory length: 166800   epsilon: 0.8677340200028714    steps: 123    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 867   score: 6.0   memory length: 167142   epsilon: 0.8670568600028861    steps: 342    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 868   score: 3.0   memory length: 167410   epsilon: 0.8665262200028976    steps: 268    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 869   score: 1.0   memory length: 167562   epsilon: 0.8662252600029041    steps: 152    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 870   score: 0.0   memory length: 167686   epsilon: 0.8659797400029094    steps: 124    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 871   score: 1.0   memory length: 167859   epsilon: 0.8656372000029169    steps: 173    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 872   score: 2.0   memory length: 168061   epsilon: 0.8652372400029256    steps: 202    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 873   score: 5.0   memory length: 168386   epsilon: 0.8645937400029395    steps: 325    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 874   score: 3.0   memory length: 168634   epsilon: 0.8641027000029502    steps: 248    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 875   score: 0.0   memory length: 168757   epsilon: 0.8638591600029555    steps: 123    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 876   score: 3.0   memory length: 168985   epsilon: 0.8634077200029653    steps: 228    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 877   score: 3.0   memory length: 169251   epsilon: 0.8628810400029767    steps: 266    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 878   score: 3.0   memory length: 169478   epsilon: 0.8624315800029865    steps: 227    lr: 0.0001     evaluation reward: 2.17\n",
            "episode: 879   score: 4.0   memory length: 169779   epsilon: 0.8618356000029994    steps: 301    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 880   score: 1.0   memory length: 169948   epsilon: 0.8615009800030067    steps: 169    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 881   score: 2.0   memory length: 170147   epsilon: 0.8611069600030152    steps: 199    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 882   score: 2.0   memory length: 170345   epsilon: 0.8607149200030237    steps: 198    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 883   score: 3.0   memory length: 170596   epsilon: 0.8602179400030345    steps: 251    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 884   score: 4.0   memory length: 170879   epsilon: 0.8596576000030467    steps: 283    lr: 0.0001     evaluation reward: 2.29\n",
            "episode: 885   score: 0.0   memory length: 171002   epsilon: 0.859414060003052    steps: 123    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 886   score: 1.0   memory length: 171154   epsilon: 0.8591131000030585    steps: 152    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 887   score: 3.0   memory length: 171366   epsilon: 0.8586933400030676    steps: 212    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 888   score: 1.0   memory length: 171535   epsilon: 0.8583587200030749    steps: 169    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 889   score: 3.0   memory length: 171802   epsilon: 0.8578300600030864    steps: 267    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 890   score: 3.0   memory length: 172070   epsilon: 0.8572994200030979    steps: 268    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 891   score: 3.0   memory length: 172340   epsilon: 0.8567648200031095    steps: 270    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 892   score: 1.0   memory length: 172513   epsilon: 0.8564222800031169    steps: 173    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 893   score: 9.0   memory length: 173021   epsilon: 0.8554164400031388    steps: 508    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 894   score: 3.0   memory length: 173269   epsilon: 0.8549254000031494    steps: 248    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 895   score: 1.0   memory length: 173420   epsilon: 0.8546264200031559    steps: 151    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 896   score: 3.0   memory length: 173686   epsilon: 0.8540997400031674    steps: 266    lr: 0.0001     evaluation reward: 2.36\n",
            "episode: 897   score: 0.0   memory length: 173810   epsilon: 0.8538542200031727    steps: 124    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 898   score: 5.0   memory length: 174138   epsilon: 0.8532047800031868    steps: 328    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 899   score: 3.0   memory length: 174385   epsilon: 0.8527157200031974    steps: 247    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 900   score: 5.0   memory length: 174707   epsilon: 0.8520781600032112    steps: 322    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 901   score: 0.0   memory length: 174831   epsilon: 0.8518326400032166    steps: 124    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 902   score: 4.0   memory length: 175110   epsilon: 0.8512802200032286    steps: 279    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 903   score: 2.0   memory length: 175329   epsilon: 0.850846600003238    steps: 219    lr: 0.0001     evaluation reward: 2.38\n",
            "episode: 904   score: 0.0   memory length: 175453   epsilon: 0.8506010800032433    steps: 124    lr: 0.0001     evaluation reward: 2.36\n",
            "episode: 905   score: 1.0   memory length: 175625   epsilon: 0.8502605200032507    steps: 172    lr: 0.0001     evaluation reward: 2.35\n",
            "episode: 906   score: 3.0   memory length: 175888   epsilon: 0.849739780003262    steps: 263    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 907   score: 6.0   memory length: 176222   epsilon: 0.8490784600032764    steps: 334    lr: 0.0001     evaluation reward: 2.4\n",
            "episode: 908   score: 5.0   memory length: 176530   epsilon: 0.8484686200032896    steps: 308    lr: 0.0001     evaluation reward: 2.45\n",
            "episode: 909   score: 3.0   memory length: 176777   epsilon: 0.8479795600033002    steps: 247    lr: 0.0001     evaluation reward: 2.46\n",
            "episode: 910   score: 3.0   memory length: 177025   epsilon: 0.8474885200033109    steps: 248    lr: 0.0001     evaluation reward: 2.46\n",
            "episode: 911   score: 4.0   memory length: 177325   epsilon: 0.8468945200033238    steps: 300    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 912   score: 1.0   memory length: 177477   epsilon: 0.8465935600033303    steps: 152    lr: 0.0001     evaluation reward: 2.46\n",
            "episode: 913   score: 3.0   memory length: 177726   epsilon: 0.846100540003341    steps: 249    lr: 0.0001     evaluation reward: 2.49\n",
            "episode: 914   score: 3.0   memory length: 177976   epsilon: 0.8456055400033518    steps: 250    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 915   score: 6.0   memory length: 178334   epsilon: 0.8448967000033671    steps: 358    lr: 0.0001     evaluation reward: 2.57\n",
            "episode: 916   score: 2.0   memory length: 178535   epsilon: 0.8444987200033758    steps: 201    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 917   score: 1.0   memory length: 178687   epsilon: 0.8441977600033823    steps: 152    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 918   score: 3.0   memory length: 178914   epsilon: 0.8437483000033921    steps: 227    lr: 0.0001     evaluation reward: 2.57\n",
            "episode: 919   score: 4.0   memory length: 179210   epsilon: 0.8431622200034048    steps: 296    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 920   score: 3.0   memory length: 179458   epsilon: 0.8426711800034155    steps: 248    lr: 0.0001     evaluation reward: 2.62\n",
            "episode: 921   score: 3.0   memory length: 179670   epsilon: 0.8422514200034246    steps: 212    lr: 0.0001     evaluation reward: 2.62\n",
            "episode: 922   score: 4.0   memory length: 179946   epsilon: 0.8417049400034364    steps: 276    lr: 0.0001     evaluation reward: 2.64\n",
            "episode: 923   score: 0.0   memory length: 180070   epsilon: 0.8414594200034418    steps: 124    lr: 0.0001     evaluation reward: 2.62\n",
            "episode: 924   score: 3.0   memory length: 180297   epsilon: 0.8410099600034515    steps: 227    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 925   score: 3.0   memory length: 180544   epsilon: 0.8405209000034621    steps: 247    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 926   score: 2.0   memory length: 180746   epsilon: 0.8401209400034708    steps: 202    lr: 0.0001     evaluation reward: 2.57\n",
            "episode: 927   score: 2.0   memory length: 180947   epsilon: 0.8397229600034795    steps: 201    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 928   score: 1.0   memory length: 181116   epsilon: 0.8393883400034867    steps: 169    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 929   score: 3.0   memory length: 181345   epsilon: 0.8389349200034966    steps: 229    lr: 0.0001     evaluation reward: 2.59\n",
            "episode: 930   score: 0.0   memory length: 181468   epsilon: 0.8386913800035019    steps: 123    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 931   score: 4.0   memory length: 181762   epsilon: 0.8381092600035145    steps: 294    lr: 0.0001     evaluation reward: 2.62\n",
            "episode: 932   score: 4.0   memory length: 182079   epsilon: 0.8374816000035281    steps: 317    lr: 0.0001     evaluation reward: 2.63\n",
            "episode: 933   score: 2.0   memory length: 182282   epsilon: 0.8370796600035368    steps: 203    lr: 0.0001     evaluation reward: 2.65\n",
            "episode: 934   score: 5.0   memory length: 182605   epsilon: 0.8364401200035507    steps: 323    lr: 0.0001     evaluation reward: 2.64\n",
            "episode: 935   score: 4.0   memory length: 182900   epsilon: 0.8358560200035634    steps: 295    lr: 0.0001     evaluation reward: 2.67\n",
            "episode: 936   score: 3.0   memory length: 183151   epsilon: 0.8353590400035742    steps: 251    lr: 0.0001     evaluation reward: 2.63\n",
            "episode: 937   score: 3.0   memory length: 183404   epsilon: 0.8348581000035851    steps: 253    lr: 0.0001     evaluation reward: 2.59\n",
            "episode: 938   score: 1.0   memory length: 183576   epsilon: 0.8345175400035925    steps: 172    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 939   score: 1.0   memory length: 183727   epsilon: 0.834218560003599    steps: 151    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 940   score: 3.0   memory length: 183959   epsilon: 0.8337592000036089    steps: 232    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 941   score: 2.0   memory length: 184176   epsilon: 0.8333295400036183    steps: 217    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 942   score: 1.0   memory length: 184328   epsilon: 0.8330285800036248    steps: 152    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 943   score: 2.0   memory length: 184530   epsilon: 0.8326286200036335    steps: 202    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 944   score: 10.0   memory length: 184929   epsilon: 0.8318386000036506    steps: 399    lr: 0.0001     evaluation reward: 2.66\n",
            "episode: 945   score: 1.0   memory length: 185099   epsilon: 0.8315020000036579    steps: 170    lr: 0.0001     evaluation reward: 2.65\n",
            "episode: 946   score: 1.0   memory length: 185272   epsilon: 0.8311594600036654    steps: 173    lr: 0.0001     evaluation reward: 2.66\n",
            "episode: 947   score: 3.0   memory length: 185518   epsilon: 0.8306723800036759    steps: 246    lr: 0.0001     evaluation reward: 2.65\n",
            "episode: 948   score: 5.0   memory length: 185840   epsilon: 0.8300348200036898    steps: 322    lr: 0.0001     evaluation reward: 2.68\n",
            "episode: 949   score: 3.0   memory length: 186106   epsilon: 0.8295081400037012    steps: 266    lr: 0.0001     evaluation reward: 2.66\n",
            "episode: 950   score: 7.0   memory length: 186461   epsilon: 0.8288052400037165    steps: 355    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 951   score: 4.0   memory length: 186742   epsilon: 0.8282488600037285    steps: 281    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 952   score: 3.0   memory length: 186974   epsilon: 0.8277895000037385    steps: 232    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 953   score: 7.0   memory length: 187401   epsilon: 0.8269440400037569    steps: 427    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 954   score: 4.0   memory length: 187679   epsilon: 0.8263936000037688    steps: 278    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 955   score: 2.0   memory length: 187860   epsilon: 0.8260352200037766    steps: 181    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 956   score: 6.0   memory length: 188236   epsilon: 0.8252907400037928    steps: 376    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 957   score: 0.0   memory length: 188360   epsilon: 0.8250452200037981    steps: 124    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 958   score: 1.0   memory length: 188529   epsilon: 0.8247106000038054    steps: 169    lr: 0.0001     evaluation reward: 2.8\n",
            "episode: 959   score: 1.0   memory length: 188681   epsilon: 0.8244096400038119    steps: 152    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 960   score: 2.0   memory length: 188862   epsilon: 0.8240512600038197    steps: 181    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 961   score: 4.0   memory length: 189141   epsilon: 0.8234988400038317    steps: 279    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 962   score: 0.0   memory length: 189264   epsilon: 0.823255300003837    steps: 123    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 963   score: 5.0   memory length: 189609   epsilon: 0.8225722000038518    steps: 345    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 964   score: 2.0   memory length: 189812   epsilon: 0.8221702600038605    steps: 203    lr: 0.0001     evaluation reward: 2.8\n",
            "episode: 965   score: 3.0   memory length: 190082   epsilon: 0.8216356600038721    steps: 270    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 966   score: 3.0   memory length: 190330   epsilon: 0.8211446200038828    steps: 248    lr: 0.0001     evaluation reward: 2.8\n",
            "episode: 967   score: 5.0   memory length: 190624   epsilon: 0.8205625000038954    steps: 294    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 968   score: 2.0   memory length: 190843   epsilon: 0.8201288800039048    steps: 219    lr: 0.0001     evaluation reward: 2.78\n",
            "episode: 969   score: 2.0   memory length: 191042   epsilon: 0.8197348600039134    steps: 199    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 970   score: 6.0   memory length: 191385   epsilon: 0.8190557200039281    steps: 343    lr: 0.0001     evaluation reward: 2.85\n",
            "episode: 971   score: 5.0   memory length: 191709   epsilon: 0.818414200003942    steps: 324    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 972   score: 2.0   memory length: 191910   epsilon: 0.8180162200039507    steps: 201    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 973   score: 5.0   memory length: 192277   epsilon: 0.8172895600039665    steps: 367    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 974   score: 1.0   memory length: 192447   epsilon: 0.8169529600039738    steps: 170    lr: 0.0001     evaluation reward: 2.87\n",
            "episode: 975   score: 3.0   memory length: 192674   epsilon: 0.8165035000039835    steps: 227    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 976   score: 4.0   memory length: 192938   epsilon: 0.8159807800039949    steps: 264    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 977   score: 2.0   memory length: 193138   epsilon: 0.8155847800040035    steps: 200    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 978   score: 2.0   memory length: 193359   epsilon: 0.815147200004013    steps: 221    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 979   score: 0.0   memory length: 193483   epsilon: 0.8149016800040183    steps: 124    lr: 0.0001     evaluation reward: 2.85\n",
            "episode: 980   score: 4.0   memory length: 193759   epsilon: 0.8143552000040302    steps: 276    lr: 0.0001     evaluation reward: 2.88\n",
            "episode: 981   score: 2.0   memory length: 193981   epsilon: 0.8139156400040397    steps: 222    lr: 0.0001     evaluation reward: 2.88\n",
            "episode: 982   score: 3.0   memory length: 194213   epsilon: 0.8134562800040497    steps: 232    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 983   score: 3.0   memory length: 194461   epsilon: 0.8129652400040603    steps: 248    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 984   score: 3.0   memory length: 194708   epsilon: 0.812476180004071    steps: 247    lr: 0.0001     evaluation reward: 2.88\n",
            "episode: 985   score: 4.0   memory length: 194986   epsilon: 0.8119257400040829    steps: 278    lr: 0.0001     evaluation reward: 2.92\n",
            "episode: 986   score: 1.0   memory length: 195159   epsilon: 0.8115832000040903    steps: 173    lr: 0.0001     evaluation reward: 2.92\n",
            "episode: 987   score: 2.0   memory length: 195376   epsilon: 0.8111535400040997    steps: 217    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 988   score: 5.0   memory length: 195683   epsilon: 0.8105456800041129    steps: 307    lr: 0.0001     evaluation reward: 2.95\n",
            "episode: 989   score: 4.0   memory length: 195959   epsilon: 0.8099992000041247    steps: 276    lr: 0.0001     evaluation reward: 2.96\n",
            "episode: 990   score: 1.0   memory length: 196111   epsilon: 0.8096982400041313    steps: 152    lr: 0.0001     evaluation reward: 2.94\n",
            "episode: 991   score: 4.0   memory length: 196386   epsilon: 0.8091537400041431    steps: 275    lr: 0.0001     evaluation reward: 2.95\n",
            "episode: 992   score: 3.0   memory length: 196634   epsilon: 0.8086627000041537    steps: 248    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 993   score: 3.0   memory length: 196881   epsilon: 0.8081736400041644    steps: 247    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 994   score: 3.0   memory length: 197110   epsilon: 0.8077202200041742    steps: 229    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 995   score: 5.0   memory length: 197427   epsilon: 0.8070925600041878    steps: 317    lr: 0.0001     evaluation reward: 2.95\n",
            "episode: 996   score: 2.0   memory length: 197627   epsilon: 0.8066965600041964    steps: 200    lr: 0.0001     evaluation reward: 2.94\n",
            "episode: 997   score: 1.0   memory length: 197798   epsilon: 0.8063579800042038    steps: 171    lr: 0.0001     evaluation reward: 2.95\n",
            "episode: 998   score: 4.0   memory length: 198079   epsilon: 0.8058016000042159    steps: 281    lr: 0.0001     evaluation reward: 2.94\n",
            "episode: 999   score: 2.0   memory length: 198278   epsilon: 0.8054075800042244    steps: 199    lr: 0.0001     evaluation reward: 2.93\n",
            "episode: 1000   score: 0.0   memory length: 198402   epsilon: 0.8051620600042297    steps: 124    lr: 0.0001     evaluation reward: 2.88\n",
            "episode: 1001   score: 3.0   memory length: 198650   epsilon: 0.8046710200042404    steps: 248    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 1002   score: 2.0   memory length: 198849   epsilon: 0.804277000004249    steps: 199    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 1003   score: 4.0   memory length: 199125   epsilon: 0.8037305200042608    steps: 276    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 1004   score: 1.0   memory length: 199277   epsilon: 0.8034295600042674    steps: 152    lr: 0.0001     evaluation reward: 2.92\n",
            "episode: 1005   score: 4.0   memory length: 199573   epsilon: 0.8028434800042801    steps: 296    lr: 0.0001     evaluation reward: 2.95\n",
            "episode: 1006   score: 3.0   memory length: 199817   epsilon: 0.8023603600042906    steps: 244    lr: 0.0001     evaluation reward: 2.95\n",
            "episode: 1007   score: 0.0   memory length: 199940   epsilon: 0.8021168200042958    steps: 123    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 1008   score: 1.0   memory length: 200092   epsilon: 0.8018158600043024    steps: 152    lr: 4e-05     evaluation reward: 2.85\n",
            "episode: 1009   score: 1.0   memory length: 200244   epsilon: 0.8015149000043089    steps: 152    lr: 4e-05     evaluation reward: 2.83\n",
            "episode: 1010   score: 0.0   memory length: 200368   epsilon: 0.8012693800043142    steps: 124    lr: 4e-05     evaluation reward: 2.8\n",
            "episode: 1011   score: 0.0   memory length: 200492   epsilon: 0.8010238600043196    steps: 124    lr: 4e-05     evaluation reward: 2.76\n",
            "episode: 1012   score: 3.0   memory length: 200719   epsilon: 0.8005744000043293    steps: 227    lr: 4e-05     evaluation reward: 2.78\n",
            "episode: 1013   score: 0.0   memory length: 200843   epsilon: 0.8003288800043347    steps: 124    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1014   score: 3.0   memory length: 201074   epsilon: 0.7998715000043446    steps: 231    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1015   score: 2.0   memory length: 201274   epsilon: 0.7994755000043532    steps: 200    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1016   score: 2.0   memory length: 201472   epsilon: 0.7990834600043617    steps: 198    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1017   score: 1.0   memory length: 201644   epsilon: 0.7987429000043691    steps: 172    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1018   score: 2.0   memory length: 201825   epsilon: 0.7983845200043769    steps: 181    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1019   score: 3.0   memory length: 202092   epsilon: 0.7978558600043884    steps: 267    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1020   score: 3.0   memory length: 202345   epsilon: 0.7973549200043992    steps: 253    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1021   score: 2.0   memory length: 202543   epsilon: 0.7969628800044077    steps: 198    lr: 4e-05     evaluation reward: 2.68\n",
            "episode: 1022   score: 3.0   memory length: 202811   epsilon: 0.7964322400044193    steps: 268    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1023   score: 4.0   memory length: 203068   epsilon: 0.7959233800044303    steps: 257    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1024   score: 5.0   memory length: 203436   epsilon: 0.7951947400044461    steps: 368    lr: 4e-05     evaluation reward: 2.73\n",
            "episode: 1025   score: 0.0   memory length: 203560   epsilon: 0.7949492200044515    steps: 124    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1026   score: 6.0   memory length: 203954   epsilon: 0.7941691000044684    steps: 394    lr: 4e-05     evaluation reward: 2.74\n",
            "episode: 1027   score: 3.0   memory length: 204199   epsilon: 0.7936840000044789    steps: 245    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1028   score: 4.0   memory length: 204514   epsilon: 0.7930603000044925    steps: 315    lr: 4e-05     evaluation reward: 2.78\n",
            "episode: 1029   score: 3.0   memory length: 204765   epsilon: 0.7925633200045032    steps: 251    lr: 4e-05     evaluation reward: 2.78\n",
            "episode: 1030   score: 2.0   memory length: 204964   epsilon: 0.7921693000045118    steps: 199    lr: 4e-05     evaluation reward: 2.8\n",
            "episode: 1031   score: 8.0   memory length: 205394   epsilon: 0.7913179000045303    steps: 430    lr: 4e-05     evaluation reward: 2.84\n",
            "episode: 1032   score: 3.0   memory length: 205662   epsilon: 0.7907872600045418    steps: 268    lr: 4e-05     evaluation reward: 2.83\n",
            "episode: 1033   score: 3.0   memory length: 205912   epsilon: 0.7902922600045525    steps: 250    lr: 4e-05     evaluation reward: 2.84\n",
            "episode: 1034   score: 4.0   memory length: 206191   epsilon: 0.7897398400045645    steps: 279    lr: 4e-05     evaluation reward: 2.83\n",
            "episode: 1035   score: 3.0   memory length: 206436   epsilon: 0.7892547400045751    steps: 245    lr: 4e-05     evaluation reward: 2.82\n",
            "episode: 1036   score: 3.0   memory length: 206667   epsilon: 0.788797360004585    steps: 231    lr: 4e-05     evaluation reward: 2.82\n",
            "episode: 1037   score: 3.0   memory length: 206915   epsilon: 0.7883063200045957    steps: 248    lr: 4e-05     evaluation reward: 2.82\n",
            "episode: 1038   score: 3.0   memory length: 207186   epsilon: 0.7877697400046073    steps: 271    lr: 4e-05     evaluation reward: 2.84\n",
            "episode: 1039   score: 6.0   memory length: 207544   epsilon: 0.7870609000046227    steps: 358    lr: 4e-05     evaluation reward: 2.89\n",
            "episode: 1040   score: 4.0   memory length: 207822   epsilon: 0.7865104600046346    steps: 278    lr: 4e-05     evaluation reward: 2.9\n",
            "episode: 1041   score: 2.0   memory length: 208022   epsilon: 0.7861144600046432    steps: 200    lr: 4e-05     evaluation reward: 2.9\n",
            "episode: 1042   score: 6.0   memory length: 208420   epsilon: 0.7853264200046604    steps: 398    lr: 4e-05     evaluation reward: 2.95\n",
            "episode: 1043   score: 2.0   memory length: 208640   epsilon: 0.7848908200046698    steps: 220    lr: 4e-05     evaluation reward: 2.95\n",
            "episode: 1044   score: 1.0   memory length: 208810   epsilon: 0.7845542200046771    steps: 170    lr: 4e-05     evaluation reward: 2.86\n",
            "episode: 1045   score: 0.0   memory length: 208934   epsilon: 0.7843087000046824    steps: 124    lr: 4e-05     evaluation reward: 2.85\n",
            "episode: 1046   score: 5.0   memory length: 209303   epsilon: 0.7835780800046983    steps: 369    lr: 4e-05     evaluation reward: 2.89\n",
            "episode: 1047   score: 2.0   memory length: 209521   epsilon: 0.7831464400047077    steps: 218    lr: 4e-05     evaluation reward: 2.88\n",
            "episode: 1048   score: 2.0   memory length: 209722   epsilon: 0.7827484600047163    steps: 201    lr: 4e-05     evaluation reward: 2.85\n",
            "episode: 1049   score: 4.0   memory length: 210001   epsilon: 0.7821960400047283    steps: 279    lr: 4e-05     evaluation reward: 2.86\n",
            "episode: 1050   score: 1.0   memory length: 210171   epsilon: 0.7818594400047356    steps: 170    lr: 4e-05     evaluation reward: 2.8\n",
            "episode: 1051   score: 2.0   memory length: 210354   epsilon: 0.7814971000047435    steps: 183    lr: 4e-05     evaluation reward: 2.78\n",
            "episode: 1052   score: 4.0   memory length: 210650   epsilon: 0.7809110200047562    steps: 296    lr: 4e-05     evaluation reward: 2.79\n",
            "episode: 1053   score: 3.0   memory length: 210879   epsilon: 0.780457600004766    steps: 229    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1054   score: 0.0   memory length: 211003   epsilon: 0.7802120800047714    steps: 124    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1055   score: 2.0   memory length: 211186   epsilon: 0.7798497400047792    steps: 183    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1056   score: 4.0   memory length: 211486   epsilon: 0.7792557400047921    steps: 300    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1057   score: 1.0   memory length: 211658   epsilon: 0.7789151800047995    steps: 172    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1058   score: 2.0   memory length: 211880   epsilon: 0.7784756200048091    steps: 222    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1059   score: 3.0   memory length: 212107   epsilon: 0.7780261600048188    steps: 227    lr: 4e-05     evaluation reward: 2.73\n",
            "episode: 1060   score: 1.0   memory length: 212277   epsilon: 0.7776895600048261    steps: 170    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1061   score: 1.0   memory length: 212429   epsilon: 0.7773886000048327    steps: 152    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1062   score: 1.0   memory length: 212582   epsilon: 0.7770856600048393    steps: 153    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1063   score: 0.0   memory length: 212706   epsilon: 0.7768401400048446    steps: 124    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1064   score: 0.0   memory length: 212830   epsilon: 0.7765946200048499    steps: 124    lr: 4e-05     evaluation reward: 2.63\n",
            "episode: 1065   score: 3.0   memory length: 213100   epsilon: 0.7760600200048615    steps: 270    lr: 4e-05     evaluation reward: 2.63\n",
            "episode: 1066   score: 2.0   memory length: 213300   epsilon: 0.7756640200048701    steps: 200    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1067   score: 3.0   memory length: 213550   epsilon: 0.7751690200048809    steps: 250    lr: 4e-05     evaluation reward: 2.6\n",
            "episode: 1068   score: 0.0   memory length: 213674   epsilon: 0.7749235000048862    steps: 124    lr: 4e-05     evaluation reward: 2.58\n",
            "episode: 1069   score: 2.0   memory length: 213875   epsilon: 0.7745255200048948    steps: 201    lr: 4e-05     evaluation reward: 2.58\n",
            "episode: 1070   score: 5.0   memory length: 214164   epsilon: 0.7739533000049073    steps: 289    lr: 4e-05     evaluation reward: 2.57\n",
            "episode: 1071   score: 1.0   memory length: 214316   epsilon: 0.7736523400049138    steps: 152    lr: 4e-05     evaluation reward: 2.53\n",
            "episode: 1072   score: 6.0   memory length: 214638   epsilon: 0.7730147800049276    steps: 322    lr: 4e-05     evaluation reward: 2.57\n",
            "episode: 1073   score: 2.0   memory length: 214837   epsilon: 0.7726207600049362    steps: 199    lr: 4e-05     evaluation reward: 2.54\n",
            "episode: 1074   score: 4.0   memory length: 215135   epsilon: 0.772030720004949    steps: 298    lr: 4e-05     evaluation reward: 2.57\n",
            "episode: 1075   score: 5.0   memory length: 215505   epsilon: 0.7712981200049649    steps: 370    lr: 4e-05     evaluation reward: 2.59\n",
            "episode: 1076   score: 3.0   memory length: 215770   epsilon: 0.7707734200049763    steps: 265    lr: 4e-05     evaluation reward: 2.58\n",
            "episode: 1077   score: 2.0   memory length: 215988   epsilon: 0.7703417800049857    steps: 218    lr: 4e-05     evaluation reward: 2.58\n",
            "episode: 1078   score: 7.0   memory length: 216373   epsilon: 0.7695794800050022    steps: 385    lr: 4e-05     evaluation reward: 2.63\n",
            "episode: 1079   score: 4.0   memory length: 216649   epsilon: 0.7690330000050141    steps: 276    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1080   score: 4.0   memory length: 216926   epsilon: 0.768484540005026    steps: 277    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1081   score: 5.0   memory length: 217256   epsilon: 0.7678311400050402    steps: 330    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1082   score: 3.0   memory length: 217482   epsilon: 0.7673836600050499    steps: 226    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1083   score: 0.0   memory length: 217606   epsilon: 0.7671381400050552    steps: 124    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1084   score: 3.0   memory length: 217818   epsilon: 0.7667183800050643    steps: 212    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1085   score: 4.0   memory length: 218131   epsilon: 0.7660986400050778    steps: 313    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1086   score: 0.0   memory length: 218254   epsilon: 0.7658551000050831    steps: 123    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1087   score: 1.0   memory length: 218424   epsilon: 0.7655185000050904    steps: 170    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1088   score: 2.0   memory length: 218626   epsilon: 0.765118540005099    steps: 202    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1089   score: 6.0   memory length: 218964   epsilon: 0.7644493000051136    steps: 338    lr: 4e-05     evaluation reward: 2.64\n",
            "episode: 1090   score: 6.0   memory length: 219302   epsilon: 0.7637800600051281    steps: 338    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1091   score: 3.0   memory length: 219552   epsilon: 0.7632850600051388    steps: 250    lr: 4e-05     evaluation reward: 2.68\n",
            "episode: 1092   score: 5.0   memory length: 219883   epsilon: 0.7626296800051531    steps: 331    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1093   score: 4.0   memory length: 220157   epsilon: 0.7620871600051649    steps: 274    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1094   score: 5.0   memory length: 220505   epsilon: 0.7613981200051798    steps: 348    lr: 4e-05     evaluation reward: 2.73\n",
            "episode: 1095   score: 4.0   memory length: 220823   epsilon: 0.7607684800051935    steps: 318    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1096   score: 2.0   memory length: 221024   epsilon: 0.7603705000052021    steps: 201    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1097   score: 1.0   memory length: 221175   epsilon: 0.7600715200052086    steps: 151    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1098   score: 4.0   memory length: 221454   epsilon: 0.7595191000052206    steps: 279    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1099   score: 5.0   memory length: 221742   epsilon: 0.758948860005233    steps: 288    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1100   score: 6.0   memory length: 222076   epsilon: 0.7582875400052473    steps: 334    lr: 4e-05     evaluation reward: 2.81\n",
            "episode: 1101   score: 4.0   memory length: 222354   epsilon: 0.7577371000052593    steps: 278    lr: 4e-05     evaluation reward: 2.82\n",
            "episode: 1102   score: 2.0   memory length: 222574   epsilon: 0.7573015000052687    steps: 220    lr: 4e-05     evaluation reward: 2.82\n",
            "episode: 1103   score: 3.0   memory length: 222823   epsilon: 0.7568084800052794    steps: 249    lr: 4e-05     evaluation reward: 2.81\n",
            "episode: 1104   score: 7.0   memory length: 223193   epsilon: 0.7560758800052954    steps: 370    lr: 4e-05     evaluation reward: 2.87\n",
            "episode: 1105   score: 3.0   memory length: 223423   epsilon: 0.7556204800053052    steps: 230    lr: 4e-05     evaluation reward: 2.86\n",
            "episode: 1106   score: 3.0   memory length: 223671   epsilon: 0.7551294400053159    steps: 248    lr: 4e-05     evaluation reward: 2.86\n",
            "episode: 1107   score: 3.0   memory length: 223898   epsilon: 0.7546799800053257    steps: 227    lr: 4e-05     evaluation reward: 2.89\n",
            "episode: 1108   score: 8.0   memory length: 224322   epsilon: 0.7538404600053439    steps: 424    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1109   score: 5.0   memory length: 224637   epsilon: 0.7532167600053574    steps: 315    lr: 4e-05     evaluation reward: 3.0\n",
            "episode: 1110   score: 4.0   memory length: 224893   epsilon: 0.7527098800053684    steps: 256    lr: 4e-05     evaluation reward: 3.04\n",
            "episode: 1111   score: 7.0   memory length: 225293   epsilon: 0.7519178800053856    steps: 400    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1112   score: 0.0   memory length: 225417   epsilon: 0.751672360005391    steps: 124    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1113   score: 3.0   memory length: 225662   epsilon: 0.7511872600054015    steps: 245    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1114   score: 8.0   memory length: 226145   epsilon: 0.7502309200054222    steps: 483    lr: 4e-05     evaluation reward: 3.16\n",
            "episode: 1115   score: 6.0   memory length: 226500   epsilon: 0.7495280200054375    steps: 355    lr: 4e-05     evaluation reward: 3.2\n",
            "episode: 1116   score: 5.0   memory length: 226803   epsilon: 0.7489280800054505    steps: 303    lr: 4e-05     evaluation reward: 3.23\n",
            "episode: 1117   score: 5.0   memory length: 227127   epsilon: 0.7482865600054645    steps: 324    lr: 4e-05     evaluation reward: 3.27\n",
            "episode: 1118   score: 2.0   memory length: 227310   epsilon: 0.7479242200054723    steps: 183    lr: 4e-05     evaluation reward: 3.27\n",
            "episode: 1119   score: 2.0   memory length: 227491   epsilon: 0.7475658400054801    steps: 181    lr: 4e-05     evaluation reward: 3.26\n",
            "episode: 1120   score: 6.0   memory length: 227863   epsilon: 0.7468292800054961    steps: 372    lr: 4e-05     evaluation reward: 3.29\n",
            "episode: 1121   score: 4.0   memory length: 228139   epsilon: 0.746282800005508    steps: 276    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1122   score: 4.0   memory length: 228428   epsilon: 0.7457105800055204    steps: 289    lr: 4e-05     evaluation reward: 3.32\n",
            "episode: 1123   score: 6.0   memory length: 228770   epsilon: 0.7450334200055351    steps: 342    lr: 4e-05     evaluation reward: 3.34\n",
            "episode: 1124   score: 4.0   memory length: 229045   epsilon: 0.7444889200055469    steps: 275    lr: 4e-05     evaluation reward: 3.33\n",
            "episode: 1125   score: 3.0   memory length: 229275   epsilon: 0.7440335200055568    steps: 230    lr: 4e-05     evaluation reward: 3.36\n",
            "episode: 1126   score: 1.0   memory length: 229426   epsilon: 0.7437345400055633    steps: 151    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1127   score: 6.0   memory length: 229821   epsilon: 0.7429524400055803    steps: 395    lr: 4e-05     evaluation reward: 3.34\n",
            "episode: 1128   score: 1.0   memory length: 229991   epsilon: 0.7426158400055876    steps: 170    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1129   score: 3.0   memory length: 230217   epsilon: 0.7421683600055973    steps: 226    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1130   score: 6.0   memory length: 230573   epsilon: 0.7414634800056126    steps: 356    lr: 4e-05     evaluation reward: 3.35\n",
            "episode: 1131   score: 3.0   memory length: 230803   epsilon: 0.7410080800056225    steps: 230    lr: 4e-05     evaluation reward: 3.3\n",
            "episode: 1132   score: 2.0   memory length: 231003   epsilon: 0.7406120800056311    steps: 200    lr: 4e-05     evaluation reward: 3.29\n",
            "episode: 1133   score: 3.0   memory length: 231255   epsilon: 0.7401131200056419    steps: 252    lr: 4e-05     evaluation reward: 3.29\n",
            "episode: 1134   score: 5.0   memory length: 231601   epsilon: 0.7394280400056568    steps: 346    lr: 4e-05     evaluation reward: 3.3\n",
            "episode: 1135   score: 4.0   memory length: 231859   epsilon: 0.7389172000056679    steps: 258    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1136   score: 3.0   memory length: 232126   epsilon: 0.7383885400056793    steps: 267    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1137   score: 4.0   memory length: 232423   epsilon: 0.7378004800056921    steps: 297    lr: 4e-05     evaluation reward: 3.32\n",
            "episode: 1138   score: 2.0   memory length: 232626   epsilon: 0.7373985400057008    steps: 203    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1139   score: 3.0   memory length: 232871   epsilon: 0.7369134400057114    steps: 245    lr: 4e-05     evaluation reward: 3.28\n",
            "episode: 1140   score: 5.0   memory length: 233182   epsilon: 0.7362976600057247    steps: 311    lr: 4e-05     evaluation reward: 3.29\n",
            "episode: 1141   score: 4.0   memory length: 233444   epsilon: 0.735778900005736    steps: 262    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1142   score: 6.0   memory length: 233799   epsilon: 0.7350760000057512    steps: 355    lr: 4e-05     evaluation reward: 3.31\n",
            "episode: 1143   score: 4.0   memory length: 234096   epsilon: 0.734487940005764    steps: 297    lr: 4e-05     evaluation reward: 3.33\n",
            "episode: 1144   score: 7.0   memory length: 234503   epsilon: 0.7336820800057815    steps: 407    lr: 4e-05     evaluation reward: 3.39\n",
            "episode: 1145   score: 5.0   memory length: 234814   epsilon: 0.7330663000057949    steps: 311    lr: 4e-05     evaluation reward: 3.44\n",
            "episode: 1146   score: 2.0   memory length: 235034   epsilon: 0.7326307000058043    steps: 220    lr: 4e-05     evaluation reward: 3.41\n",
            "episode: 1147   score: 6.0   memory length: 235370   epsilon: 0.7319654200058188    steps: 336    lr: 4e-05     evaluation reward: 3.45\n",
            "episode: 1148   score: 1.0   memory length: 235522   epsilon: 0.7316644600058253    steps: 152    lr: 4e-05     evaluation reward: 3.44\n",
            "episode: 1149   score: 3.0   memory length: 235790   epsilon: 0.7311338200058368    steps: 268    lr: 4e-05     evaluation reward: 3.43\n",
            "episode: 1150   score: 6.0   memory length: 236188   epsilon: 0.7303457800058539    steps: 398    lr: 4e-05     evaluation reward: 3.48\n",
            "episode: 1151   score: 5.0   memory length: 236480   epsilon: 0.7297676200058665    steps: 292    lr: 4e-05     evaluation reward: 3.51\n",
            "episode: 1152   score: 6.0   memory length: 236818   epsilon: 0.729098380005881    steps: 338    lr: 4e-05     evaluation reward: 3.53\n",
            "episode: 1153   score: 6.0   memory length: 237192   epsilon: 0.7283578600058971    steps: 374    lr: 4e-05     evaluation reward: 3.56\n",
            "episode: 1154   score: 10.0   memory length: 237682   epsilon: 0.7273876600059181    steps: 490    lr: 4e-05     evaluation reward: 3.66\n",
            "episode: 1155   score: 3.0   memory length: 237933   epsilon: 0.7268906800059289    steps: 251    lr: 4e-05     evaluation reward: 3.67\n",
            "episode: 1156   score: 2.0   memory length: 238154   epsilon: 0.7264531000059384    steps: 221    lr: 4e-05     evaluation reward: 3.65\n",
            "episode: 1157   score: 1.0   memory length: 238324   epsilon: 0.7261165000059457    steps: 170    lr: 4e-05     evaluation reward: 3.65\n",
            "episode: 1158   score: 7.0   memory length: 238710   epsilon: 0.7253522200059623    steps: 386    lr: 4e-05     evaluation reward: 3.7\n",
            "episode: 1159   score: 4.0   memory length: 239007   epsilon: 0.7247641600059751    steps: 297    lr: 4e-05     evaluation reward: 3.71\n",
            "episode: 1160   score: 2.0   memory length: 239207   epsilon: 0.7243681600059837    steps: 200    lr: 4e-05     evaluation reward: 3.72\n",
            "episode: 1161   score: 8.0   memory length: 239661   epsilon: 0.7234692400060032    steps: 454    lr: 4e-05     evaluation reward: 3.79\n",
            "episode: 1162   score: 4.0   memory length: 239928   epsilon: 0.7229405800060147    steps: 267    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1163   score: 5.0   memory length: 240274   epsilon: 0.7222555000060296    steps: 346    lr: 4e-05     evaluation reward: 3.87\n",
            "episode: 1164   score: 7.0   memory length: 240724   epsilon: 0.7213645000060489    steps: 450    lr: 4e-05     evaluation reward: 3.94\n",
            "episode: 1165   score: 1.0   memory length: 240876   epsilon: 0.7210635400060554    steps: 152    lr: 4e-05     evaluation reward: 3.92\n",
            "episode: 1166   score: 3.0   memory length: 241123   epsilon: 0.720574480006066    steps: 247    lr: 4e-05     evaluation reward: 3.93\n",
            "episode: 1167   score: 5.0   memory length: 241413   epsilon: 0.7200002800060785    steps: 290    lr: 4e-05     evaluation reward: 3.95\n",
            "episode: 1168   score: 5.0   memory length: 241738   epsilon: 0.7193567800060925    steps: 325    lr: 4e-05     evaluation reward: 4.0\n",
            "episode: 1169   score: 6.0   memory length: 242059   epsilon: 0.7187212000061063    steps: 321    lr: 4e-05     evaluation reward: 4.04\n",
            "episode: 1170   score: 10.0   memory length: 242473   epsilon: 0.7179014800061241    steps: 414    lr: 4e-05     evaluation reward: 4.09\n",
            "episode: 1171   score: 9.0   memory length: 242969   epsilon: 0.7169194000061454    steps: 496    lr: 4e-05     evaluation reward: 4.17\n",
            "episode: 1172   score: 6.0   memory length: 243360   epsilon: 0.7161452200061622    steps: 391    lr: 4e-05     evaluation reward: 4.17\n",
            "episode: 1173   score: 3.0   memory length: 243590   epsilon: 0.7156898200061721    steps: 230    lr: 4e-05     evaluation reward: 4.18\n",
            "episode: 1174   score: 3.0   memory length: 243837   epsilon: 0.7152007600061827    steps: 247    lr: 4e-05     evaluation reward: 4.17\n",
            "episode: 1175   score: 3.0   memory length: 244084   epsilon: 0.7147117000061933    steps: 247    lr: 4e-05     evaluation reward: 4.15\n",
            "episode: 1176   score: 1.0   memory length: 244236   epsilon: 0.7144107400061999    steps: 152    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1177   score: 2.0   memory length: 244438   epsilon: 0.7140107800062085    steps: 202    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1178   score: 8.0   memory length: 244840   epsilon: 0.7132148200062258    steps: 402    lr: 4e-05     evaluation reward: 4.14\n",
            "episode: 1179   score: 6.0   memory length: 245235   epsilon: 0.7124327200062428    steps: 395    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1180   score: 0.0   memory length: 245359   epsilon: 0.7121872000062481    steps: 124    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1181   score: 3.0   memory length: 245591   epsilon: 0.7117278400062581    steps: 232    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1182   score: 3.0   memory length: 245825   epsilon: 0.7112645200062682    steps: 234    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1183   score: 3.0   memory length: 246053   epsilon: 0.710813080006278    steps: 228    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1184   score: 4.0   memory length: 246332   epsilon: 0.71026066000629    steps: 279    lr: 4e-05     evaluation reward: 4.14\n",
            "episode: 1185   score: 3.0   memory length: 246558   epsilon: 0.7098131800062997    steps: 226    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1186   score: 3.0   memory length: 246789   epsilon: 0.7093558000063096    steps: 231    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1187   score: 5.0   memory length: 247111   epsilon: 0.7087182400063234    steps: 322    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1188   score: 4.0   memory length: 247408   epsilon: 0.7081301800063362    steps: 297    lr: 4e-05     evaluation reward: 4.22\n",
            "episode: 1189   score: 3.0   memory length: 247655   epsilon: 0.7076411200063468    steps: 247    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1190   score: 7.0   memory length: 248085   epsilon: 0.7067897200063653    steps: 430    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1191   score: 3.0   memory length: 248331   epsilon: 0.7063026400063759    steps: 246    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1192   score: 6.0   memory length: 248667   epsilon: 0.7056373600063903    steps: 336    lr: 4e-05     evaluation reward: 4.21\n",
            "episode: 1193   score: 2.0   memory length: 248866   epsilon: 0.7052433400063989    steps: 199    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1194   score: 5.0   memory length: 249175   epsilon: 0.7046315200064122    steps: 309    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1195   score: 5.0   memory length: 249519   epsilon: 0.703950400006427    steps: 344    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1196   score: 4.0   memory length: 249797   epsilon: 0.7033999600064389    steps: 278    lr: 4e-05     evaluation reward: 4.22\n",
            "episode: 1197   score: 6.0   memory length: 250171   epsilon: 0.702659440006455    steps: 374    lr: 4e-05     evaluation reward: 4.27\n",
            "episode: 1198   score: 5.0   memory length: 250497   epsilon: 0.702013960006469    steps: 326    lr: 4e-05     evaluation reward: 4.28\n",
            "episode: 1199   score: 2.0   memory length: 250717   epsilon: 0.7015783600064784    steps: 220    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1200   score: 0.0   memory length: 250841   epsilon: 0.7013328400064838    steps: 124    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1201   score: 3.0   memory length: 251087   epsilon: 0.7008457600064943    steps: 246    lr: 4e-05     evaluation reward: 4.18\n",
            "episode: 1202   score: 9.0   memory length: 251536   epsilon: 0.6999567400065136    steps: 449    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1203   score: 3.0   memory length: 251766   epsilon: 0.6995013400065235    steps: 230    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1204   score: 4.0   memory length: 252062   epsilon: 0.6989152600065363    steps: 296    lr: 4e-05     evaluation reward: 4.22\n",
            "episode: 1205   score: 8.0   memory length: 252514   epsilon: 0.6980203000065557    steps: 452    lr: 4e-05     evaluation reward: 4.27\n",
            "episode: 1206   score: 3.0   memory length: 252741   epsilon: 0.6975708400065654    steps: 227    lr: 4e-05     evaluation reward: 4.27\n",
            "episode: 1207   score: 6.0   memory length: 253098   epsilon: 0.6968639800065808    steps: 357    lr: 4e-05     evaluation reward: 4.3\n",
            "episode: 1208   score: 3.0   memory length: 253345   epsilon: 0.6963749200065914    steps: 247    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1209   score: 5.0   memory length: 253672   epsilon: 0.6957274600066055    steps: 327    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1210   score: 3.0   memory length: 253900   epsilon: 0.6952760200066153    steps: 228    lr: 4e-05     evaluation reward: 4.24\n",
            "episode: 1211   score: 2.0   memory length: 254118   epsilon: 0.6948443800066246    steps: 218    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1212   score: 3.0   memory length: 254344   epsilon: 0.6943969000066343    steps: 226    lr: 4e-05     evaluation reward: 4.22\n",
            "episode: 1213   score: 6.0   memory length: 254673   epsilon: 0.6937454800066485    steps: 329    lr: 4e-05     evaluation reward: 4.25\n",
            "episode: 1214   score: 7.0   memory length: 255079   epsilon: 0.6929416000066659    steps: 406    lr: 4e-05     evaluation reward: 4.24\n",
            "episode: 1215   score: 11.0   memory length: 255513   epsilon: 0.6920822800066846    steps: 434    lr: 4e-05     evaluation reward: 4.29\n",
            "episode: 1216   score: 6.0   memory length: 255859   epsilon: 0.6913972000066995    steps: 346    lr: 4e-05     evaluation reward: 4.3\n",
            "episode: 1217   score: 4.0   memory length: 256117   epsilon: 0.6908863600067106    steps: 258    lr: 4e-05     evaluation reward: 4.29\n",
            "episode: 1218   score: 10.0   memory length: 256667   epsilon: 0.6897973600067342    steps: 550    lr: 4e-05     evaluation reward: 4.37\n",
            "episode: 1219   score: 6.0   memory length: 257021   epsilon: 0.6890964400067494    steps: 354    lr: 4e-05     evaluation reward: 4.41\n",
            "episode: 1220   score: 4.0   memory length: 257281   epsilon: 0.6885816400067606    steps: 260    lr: 4e-05     evaluation reward: 4.39\n",
            "episode: 1221   score: 7.0   memory length: 257683   epsilon: 0.6877856800067779    steps: 402    lr: 4e-05     evaluation reward: 4.42\n",
            "episode: 1222   score: 4.0   memory length: 257961   epsilon: 0.6872352400067898    steps: 278    lr: 4e-05     evaluation reward: 4.42\n",
            "episode: 1223   score: 4.0   memory length: 258240   epsilon: 0.6866828200068018    steps: 279    lr: 4e-05     evaluation reward: 4.4\n",
            "episode: 1224   score: 4.0   memory length: 258498   epsilon: 0.6861719800068129    steps: 258    lr: 4e-05     evaluation reward: 4.4\n",
            "episode: 1225   score: 3.0   memory length: 258747   epsilon: 0.6856789600068236    steps: 249    lr: 4e-05     evaluation reward: 4.4\n",
            "episode: 1226   score: 9.0   memory length: 259238   epsilon: 0.6847067800068447    steps: 491    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1227   score: 2.0   memory length: 259436   epsilon: 0.6843147400068532    steps: 198    lr: 4e-05     evaluation reward: 4.44\n",
            "episode: 1228   score: 2.0   memory length: 259619   epsilon: 0.6839524000068611    steps: 183    lr: 4e-05     evaluation reward: 4.45\n",
            "episode: 1229   score: 6.0   memory length: 259964   epsilon: 0.6832693000068759    steps: 345    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1230   score: 4.0   memory length: 260223   epsilon: 0.682756480006887    steps: 259    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1231   score: 3.0   memory length: 260474   epsilon: 0.6822595000068978    steps: 251    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1232   score: 4.0   memory length: 260750   epsilon: 0.6817130200069097    steps: 276    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1233   score: 2.0   memory length: 260948   epsilon: 0.6813209800069182    steps: 198    lr: 4e-05     evaluation reward: 4.47\n",
            "episode: 1234   score: 4.0   memory length: 261224   epsilon: 0.6807745000069301    steps: 276    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1235   score: 7.0   memory length: 261613   epsilon: 0.6800042800069468    steps: 389    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1236   score: 4.0   memory length: 261913   epsilon: 0.6794102800069597    steps: 300    lr: 4e-05     evaluation reward: 4.5\n",
            "episode: 1237   score: 3.0   memory length: 262145   epsilon: 0.6789509200069697    steps: 232    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1238   score: 4.0   memory length: 262419   epsilon: 0.6784084000069814    steps: 274    lr: 4e-05     evaluation reward: 4.51\n",
            "episode: 1239   score: 2.0   memory length: 262618   epsilon: 0.67801438000699    steps: 199    lr: 4e-05     evaluation reward: 4.5\n",
            "episode: 1240   score: 4.0   memory length: 262896   epsilon: 0.6774639400070019    steps: 278    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1241   score: 4.0   memory length: 263141   epsilon: 0.6769788400070125    steps: 245    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1242   score: 4.0   memory length: 263437   epsilon: 0.6763927600070252    steps: 296    lr: 4e-05     evaluation reward: 4.47\n",
            "episode: 1243   score: 5.0   memory length: 263744   epsilon: 0.6757849000070384    steps: 307    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1244   score: 5.0   memory length: 264071   epsilon: 0.6751374400070524    steps: 327    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1245   score: 4.0   memory length: 264329   epsilon: 0.6746266000070635    steps: 258    lr: 4e-05     evaluation reward: 4.45\n",
            "episode: 1246   score: 3.0   memory length: 264543   epsilon: 0.6742028800070727    steps: 214    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1247   score: 6.0   memory length: 264914   epsilon: 0.6734683000070887    steps: 371    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1248   score: 4.0   memory length: 265192   epsilon: 0.6729178600071006    steps: 278    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1249   score: 5.0   memory length: 265496   epsilon: 0.6723159400071137    steps: 304    lr: 4e-05     evaluation reward: 4.51\n",
            "episode: 1250   score: 7.0   memory length: 265921   epsilon: 0.671474440007132    steps: 425    lr: 4e-05     evaluation reward: 4.52\n",
            "episode: 1251   score: 2.0   memory length: 266138   epsilon: 0.6710447800071413    steps: 217    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1252   score: 4.0   memory length: 266394   epsilon: 0.6705379000071523    steps: 256    lr: 4e-05     evaluation reward: 4.47\n",
            "episode: 1253   score: 4.0   memory length: 266669   epsilon: 0.6699934000071641    steps: 275    lr: 4e-05     evaluation reward: 4.45\n",
            "episode: 1254   score: 4.0   memory length: 266947   epsilon: 0.6694429600071761    steps: 278    lr: 4e-05     evaluation reward: 4.39\n",
            "episode: 1255   score: 4.0   memory length: 267224   epsilon: 0.668894500007188    steps: 277    lr: 4e-05     evaluation reward: 4.4\n",
            "episode: 1256   score: 3.0   memory length: 267494   epsilon: 0.6683599000071996    steps: 270    lr: 4e-05     evaluation reward: 4.41\n",
            "episode: 1257   score: 2.0   memory length: 267693   epsilon: 0.6679658800072081    steps: 199    lr: 4e-05     evaluation reward: 4.42\n",
            "episode: 1258   score: 5.0   memory length: 268000   epsilon: 0.6673580200072213    steps: 307    lr: 4e-05     evaluation reward: 4.4\n",
            "episode: 1259   score: 4.0   memory length: 268276   epsilon: 0.6668115400072332    steps: 276    lr: 4e-05     evaluation reward: 4.4\n",
            "episode: 1260   score: 12.0   memory length: 268845   epsilon: 0.6656849200072577    steps: 569    lr: 4e-05     evaluation reward: 4.5\n",
            "episode: 1261   score: 6.0   memory length: 269200   epsilon: 0.6649820200072729    steps: 355    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1262   score: 4.0   memory length: 269463   epsilon: 0.6644612800072842    steps: 263    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1263   score: 2.0   memory length: 269661   epsilon: 0.6640692400072927    steps: 198    lr: 4e-05     evaluation reward: 4.45\n",
            "episode: 1264   score: 3.0   memory length: 269871   epsilon: 0.6636534400073018    steps: 210    lr: 4e-05     evaluation reward: 4.41\n",
            "episode: 1265   score: 8.0   memory length: 270347   epsilon: 0.6627109600073222    steps: 476    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1266   score: 8.0   memory length: 270644   epsilon: 0.662122900007335    steps: 297    lr: 4e-05     evaluation reward: 4.53\n",
            "episode: 1267   score: 3.0   memory length: 270873   epsilon: 0.6616694800073448    steps: 229    lr: 4e-05     evaluation reward: 4.51\n",
            "episode: 1268   score: 6.0   memory length: 271230   epsilon: 0.6609626200073602    steps: 357    lr: 4e-05     evaluation reward: 4.52\n",
            "episode: 1269   score: 2.0   memory length: 271411   epsilon: 0.660604240007368    steps: 181    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1270   score: 4.0   memory length: 271708   epsilon: 0.6600161800073807    steps: 297    lr: 4e-05     evaluation reward: 4.42\n",
            "episode: 1271   score: 5.0   memory length: 272024   epsilon: 0.6593905000073943    steps: 316    lr: 4e-05     evaluation reward: 4.38\n",
            "episode: 1272   score: 3.0   memory length: 272272   epsilon: 0.658899460007405    steps: 248    lr: 4e-05     evaluation reward: 4.35\n",
            "episode: 1273   score: 5.0   memory length: 272580   epsilon: 0.6582896200074182    steps: 308    lr: 4e-05     evaluation reward: 4.37\n",
            "episode: 1274   score: 6.0   memory length: 272946   epsilon: 0.6575649400074339    steps: 366    lr: 4e-05     evaluation reward: 4.4\n",
            "episode: 1275   score: 6.0   memory length: 273345   epsilon: 0.6567749200074511    steps: 399    lr: 4e-05     evaluation reward: 4.43\n",
            "episode: 1276   score: 3.0   memory length: 273590   epsilon: 0.6562898200074616    steps: 245    lr: 4e-05     evaluation reward: 4.45\n",
            "episode: 1277   score: 6.0   memory length: 273924   epsilon: 0.655628500007476    steps: 334    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1278   score: 5.0   memory length: 274266   epsilon: 0.6549513400074907    steps: 342    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1279   score: 6.0   memory length: 274648   epsilon: 0.6541949800075071    steps: 382    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1280   score: 5.0   memory length: 274977   epsilon: 0.6535435600075212    steps: 329    lr: 4e-05     evaluation reward: 4.51\n",
            "episode: 1281   score: 6.0   memory length: 275330   epsilon: 0.6528446200075364    steps: 353    lr: 4e-05     evaluation reward: 4.54\n",
            "episode: 1282   score: 5.0   memory length: 275659   epsilon: 0.6521932000075505    steps: 329    lr: 4e-05     evaluation reward: 4.56\n",
            "episode: 1283   score: 6.0   memory length: 275980   epsilon: 0.6515576200075643    steps: 321    lr: 4e-05     evaluation reward: 4.59\n",
            "episode: 1284   score: 4.0   memory length: 276277   epsilon: 0.6509695600075771    steps: 297    lr: 4e-05     evaluation reward: 4.59\n",
            "episode: 1285   score: 3.0   memory length: 276524   epsilon: 0.6504805000075877    steps: 247    lr: 4e-05     evaluation reward: 4.59\n",
            "episode: 1286   score: 5.0   memory length: 276831   epsilon: 0.6498726400076009    steps: 307    lr: 4e-05     evaluation reward: 4.61\n",
            "episode: 1287   score: 2.0   memory length: 277012   epsilon: 0.6495142600076087    steps: 181    lr: 4e-05     evaluation reward: 4.58\n",
            "episode: 1288   score: 2.0   memory length: 277195   epsilon: 0.6491519200076166    steps: 183    lr: 4e-05     evaluation reward: 4.56\n",
            "episode: 1289   score: 3.0   memory length: 277427   epsilon: 0.6486925600076265    steps: 232    lr: 4e-05     evaluation reward: 4.56\n",
            "episode: 1290   score: 6.0   memory length: 277800   epsilon: 0.6479540200076426    steps: 373    lr: 4e-05     evaluation reward: 4.55\n",
            "episode: 1291   score: 3.0   memory length: 278048   epsilon: 0.6474629800076532    steps: 248    lr: 4e-05     evaluation reward: 4.55\n",
            "episode: 1292   score: 4.0   memory length: 278306   epsilon: 0.6469521400076643    steps: 258    lr: 4e-05     evaluation reward: 4.53\n",
            "episode: 1293   score: 5.0   memory length: 278648   epsilon: 0.646274980007679    steps: 342    lr: 4e-05     evaluation reward: 4.56\n",
            "episode: 1294   score: 6.0   memory length: 278992   epsilon: 0.6455938600076938    steps: 344    lr: 4e-05     evaluation reward: 4.57\n",
            "episode: 1295   score: 6.0   memory length: 279368   epsilon: 0.64484938000771    steps: 376    lr: 4e-05     evaluation reward: 4.58\n",
            "episode: 1296   score: 3.0   memory length: 279582   epsilon: 0.6444256600077192    steps: 214    lr: 4e-05     evaluation reward: 4.57\n",
            "episode: 1297   score: 2.0   memory length: 279781   epsilon: 0.6440316400077277    steps: 199    lr: 4e-05     evaluation reward: 4.53\n",
            "episode: 1298   score: 6.0   memory length: 280132   epsilon: 0.6433366600077428    steps: 351    lr: 4e-05     evaluation reward: 4.54\n",
            "episode: 1299   score: 9.0   memory length: 280587   epsilon: 0.6424357600077624    steps: 455    lr: 4e-05     evaluation reward: 4.61\n",
            "episode: 1300   score: 4.0   memory length: 280863   epsilon: 0.6418892800077742    steps: 276    lr: 4e-05     evaluation reward: 4.65\n",
            "episode: 1301   score: 3.0   memory length: 281110   epsilon: 0.6414002200077848    steps: 247    lr: 4e-05     evaluation reward: 4.65\n",
            "episode: 1302   score: 3.0   memory length: 281324   epsilon: 0.640976500007794    steps: 214    lr: 4e-05     evaluation reward: 4.59\n",
            "episode: 1303   score: 5.0   memory length: 281650   epsilon: 0.6403310200078081    steps: 326    lr: 4e-05     evaluation reward: 4.61\n",
            "episode: 1304   score: 8.0   memory length: 282047   epsilon: 0.6395449600078251    steps: 397    lr: 4e-05     evaluation reward: 4.65\n",
            "episode: 1305   score: 5.0   memory length: 282371   epsilon: 0.638903440007839    steps: 324    lr: 4e-05     evaluation reward: 4.62\n",
            "episode: 1306   score: 4.0   memory length: 282650   epsilon: 0.638351020007851    steps: 279    lr: 4e-05     evaluation reward: 4.63\n",
            "episode: 1307   score: 3.0   memory length: 282879   epsilon: 0.6378976000078609    steps: 229    lr: 4e-05     evaluation reward: 4.6\n",
            "episode: 1308   score: 7.0   memory length: 283276   epsilon: 0.637111540007878    steps: 397    lr: 4e-05     evaluation reward: 4.64\n",
            "episode: 1309   score: 5.0   memory length: 283618   epsilon: 0.6364343800078927    steps: 342    lr: 4e-05     evaluation reward: 4.64\n",
            "episode: 1310   score: 8.0   memory length: 284035   epsilon: 0.6356087200079106    steps: 417    lr: 4e-05     evaluation reward: 4.69\n",
            "episode: 1311   score: 6.0   memory length: 284409   epsilon: 0.6348682000079267    steps: 374    lr: 4e-05     evaluation reward: 4.73\n",
            "episode: 1312   score: 3.0   memory length: 284623   epsilon: 0.6344444800079359    steps: 214    lr: 4e-05     evaluation reward: 4.73\n",
            "episode: 1313   score: 3.0   memory length: 284890   epsilon: 0.6339158200079473    steps: 267    lr: 4e-05     evaluation reward: 4.7\n",
            "episode: 1314   score: 5.0   memory length: 285253   epsilon: 0.6331970800079629    steps: 363    lr: 4e-05     evaluation reward: 4.68\n",
            "episode: 1315   score: 4.0   memory length: 285550   epsilon: 0.6326090200079757    steps: 297    lr: 4e-05     evaluation reward: 4.61\n",
            "episode: 1316   score: 4.0   memory length: 285833   epsilon: 0.6320486800079879    steps: 283    lr: 4e-05     evaluation reward: 4.59\n",
            "episode: 1317   score: 6.0   memory length: 286196   epsilon: 0.6313299400080035    steps: 363    lr: 4e-05     evaluation reward: 4.61\n",
            "episode: 1318   score: 3.0   memory length: 286427   epsilon: 0.6308725600080134    steps: 231    lr: 4e-05     evaluation reward: 4.54\n",
            "episode: 1319   score: 9.0   memory length: 286946   epsilon: 0.6298449400080357    steps: 519    lr: 4e-05     evaluation reward: 4.57\n",
            "episode: 1320   score: 5.0   memory length: 287255   epsilon: 0.629233120008049    steps: 309    lr: 4e-05     evaluation reward: 4.58\n",
            "episode: 1321   score: 4.0   memory length: 287534   epsilon: 0.628680700008061    steps: 279    lr: 4e-05     evaluation reward: 4.55\n",
            "episode: 1322   score: 2.0   memory length: 287715   epsilon: 0.6283223200080688    steps: 181    lr: 4e-05     evaluation reward: 4.53\n",
            "episode: 1323   score: 7.0   memory length: 288106   epsilon: 0.6275481400080856    steps: 391    lr: 4e-05     evaluation reward: 4.56\n",
            "episode: 1324   score: 4.0   memory length: 288346   epsilon: 0.6270729400080959    steps: 240    lr: 4e-05     evaluation reward: 4.56\n",
            "episode: 1325   score: 3.0   memory length: 288573   epsilon: 0.6266234800081056    steps: 227    lr: 4e-05     evaluation reward: 4.56\n",
            "episode: 1326   score: 2.0   memory length: 288792   epsilon: 0.626189860008115    steps: 219    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1327   score: 4.0   memory length: 289110   epsilon: 0.6255602200081287    steps: 318    lr: 4e-05     evaluation reward: 4.51\n",
            "episode: 1328   score: 4.0   memory length: 289369   epsilon: 0.6250474000081399    steps: 259    lr: 4e-05     evaluation reward: 4.53\n",
            "episode: 1329   score: 4.0   memory length: 289632   epsilon: 0.6245266600081512    steps: 263    lr: 4e-05     evaluation reward: 4.51\n",
            "episode: 1330   score: 3.0   memory length: 289878   epsilon: 0.6240395800081617    steps: 246    lr: 4e-05     evaluation reward: 4.5\n",
            "episode: 1331   score: 5.0   memory length: 290222   epsilon: 0.6233584600081765    steps: 344    lr: 4e-05     evaluation reward: 4.52\n",
            "episode: 1332   score: 3.0   memory length: 290449   epsilon: 0.6229090000081863    steps: 227    lr: 4e-05     evaluation reward: 4.51\n",
            "episode: 1333   score: 3.0   memory length: 290681   epsilon: 0.6224496400081962    steps: 232    lr: 4e-05     evaluation reward: 4.52\n",
            "episode: 1334   score: 1.0   memory length: 290832   epsilon: 0.6221506600082027    steps: 151    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1335   score: 6.0   memory length: 291199   epsilon: 0.6214240000082185    steps: 367    lr: 4e-05     evaluation reward: 4.48\n",
            "episode: 1336   score: 5.0   memory length: 291524   epsilon: 0.6207805000082325    steps: 325    lr: 4e-05     evaluation reward: 4.49\n",
            "episode: 1337   score: 6.0   memory length: 291858   epsilon: 0.6201191800082468    steps: 334    lr: 4e-05     evaluation reward: 4.52\n",
            "episode: 1338   score: 5.0   memory length: 292162   epsilon: 0.6195172600082599    steps: 304    lr: 4e-05     evaluation reward: 4.53\n",
            "episode: 1339   score: 4.0   memory length: 292436   epsilon: 0.6189747400082717    steps: 274    lr: 4e-05     evaluation reward: 4.55\n",
            "episode: 1340   score: 5.0   memory length: 292724   epsilon: 0.6184045000082841    steps: 288    lr: 4e-05     evaluation reward: 4.56\n",
            "episode: 1341   score: 6.0   memory length: 293099   epsilon: 0.6176620000083002    steps: 375    lr: 4e-05     evaluation reward: 4.58\n",
            "episode: 1342   score: 5.0   memory length: 293391   epsilon: 0.6170838400083127    steps: 292    lr: 4e-05     evaluation reward: 4.59\n",
            "episode: 1343   score: 5.0   memory length: 293717   epsilon: 0.6164383600083267    steps: 326    lr: 4e-05     evaluation reward: 4.59\n",
            "episode: 1344   score: 7.0   memory length: 294089   epsilon: 0.6157018000083427    steps: 372    lr: 4e-05     evaluation reward: 4.61\n",
            "episode: 1345   score: 4.0   memory length: 294367   epsilon: 0.6151513600083547    steps: 278    lr: 4e-05     evaluation reward: 4.61\n",
            "episode: 1346   score: 7.0   memory length: 294788   epsilon: 0.6143177800083728    steps: 421    lr: 4e-05     evaluation reward: 4.65\n",
            "episode: 1347   score: 5.0   memory length: 295085   epsilon: 0.6137297200083855    steps: 297    lr: 4e-05     evaluation reward: 4.64\n",
            "episode: 1348   score: 3.0   memory length: 295296   epsilon: 0.6133119400083946    steps: 211    lr: 4e-05     evaluation reward: 4.63\n",
            "episode: 1349   score: 0.0   memory length: 295420   epsilon: 0.6130664200084    steps: 124    lr: 4e-05     evaluation reward: 4.58\n",
            "episode: 1350   score: 4.0   memory length: 295724   epsilon: 0.612464500008413    steps: 304    lr: 4e-05     evaluation reward: 4.55\n",
            "episode: 1351   score: 7.0   memory length: 296080   epsilon: 0.6117596200084283    steps: 356    lr: 4e-05     evaluation reward: 4.6\n",
            "episode: 1352   score: 4.0   memory length: 296342   epsilon: 0.6112408600084396    steps: 262    lr: 4e-05     evaluation reward: 4.6\n",
            "episode: 1353   score: 8.0   memory length: 296752   epsilon: 0.6104290600084572    steps: 410    lr: 4e-05     evaluation reward: 4.64\n",
            "episode: 1354   score: 8.0   memory length: 297248   epsilon: 0.6094469800084785    steps: 496    lr: 4e-05     evaluation reward: 4.68\n",
            "episode: 1355   score: 3.0   memory length: 297477   epsilon: 0.6089935600084884    steps: 229    lr: 4e-05     evaluation reward: 4.67\n",
            "episode: 1356   score: 8.0   memory length: 297914   epsilon: 0.6081283000085071    steps: 437    lr: 4e-05     evaluation reward: 4.72\n",
            "episode: 1357   score: 11.0   memory length: 298408   epsilon: 0.6071501800085284    steps: 494    lr: 4e-05     evaluation reward: 4.81\n",
            "episode: 1358   score: 5.0   memory length: 298701   epsilon: 0.606570040008541    steps: 293    lr: 4e-05     evaluation reward: 4.81\n",
            "episode: 1359   score: 4.0   memory length: 298979   epsilon: 0.6060196000085529    steps: 278    lr: 4e-05     evaluation reward: 4.81\n",
            "episode: 1360   score: 6.0   memory length: 299317   epsilon: 0.6053503600085675    steps: 338    lr: 4e-05     evaluation reward: 4.75\n",
            "episode: 1361   score: 5.0   memory length: 299624   epsilon: 0.6047425000085807    steps: 307    lr: 4e-05     evaluation reward: 4.74\n",
            "episode: 1362   score: 5.0   memory length: 299951   epsilon: 0.6040950400085947    steps: 327    lr: 4e-05     evaluation reward: 4.75\n",
            "episode: 1363   score: 7.0   memory length: 300299   epsilon: 0.6034060000086097    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 4.8\n",
            "episode: 1364   score: 7.0   memory length: 300687   epsilon: 0.6026377600086263    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 4.84\n",
            "episode: 1365   score: 6.0   memory length: 301030   epsilon: 0.6019586200086411    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.82\n",
            "episode: 1366   score: 9.0   memory length: 301498   epsilon: 0.6010319800086612    steps: 468    lr: 1.6000000000000003e-05     evaluation reward: 4.83\n",
            "episode: 1367   score: 7.0   memory length: 301925   epsilon: 0.6001865200086796    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 4.87\n",
            "episode: 1368   score: 8.0   memory length: 302334   epsilon: 0.5993767000086971    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
            "episode: 1369   score: 4.0   memory length: 302578   epsilon: 0.5988935800087076    steps: 244    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
            "episode: 1370   score: 10.0   memory length: 303048   epsilon: 0.5979629800087278    steps: 470    lr: 1.6000000000000003e-05     evaluation reward: 4.97\n",
            "episode: 1371   score: 4.0   memory length: 303308   epsilon: 0.597448180008739    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 4.96\n",
            "episode: 1372   score: 2.0   memory length: 303507   epsilon: 0.5970541600087476    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
            "episode: 1373   score: 10.0   memory length: 303870   epsilon: 0.5963354200087632    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 5.0\n",
            "episode: 1374   score: 13.0   memory length: 304425   epsilon: 0.595236520008787    steps: 555    lr: 1.6000000000000003e-05     evaluation reward: 5.07\n",
            "episode: 1375   score: 8.0   memory length: 304841   epsilon: 0.5944128400088049    steps: 416    lr: 1.6000000000000003e-05     evaluation reward: 5.09\n",
            "episode: 1376   score: 8.0   memory length: 305279   epsilon: 0.5935456000088237    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
            "episode: 1377   score: 8.0   memory length: 305723   epsilon: 0.5926664800088428    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
            "episode: 1378   score: 3.0   memory length: 305974   epsilon: 0.5921695000088536    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
            "episode: 1379   score: 10.0   memory length: 306481   epsilon: 0.5911656400088754    steps: 507    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
            "episode: 1380   score: 4.0   memory length: 306737   epsilon: 0.5906587600088864    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 5.17\n",
            "episode: 1381   score: 7.0   memory length: 307143   epsilon: 0.5898548800089038    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
            "episode: 1382   score: 11.0   memory length: 307685   epsilon: 0.5887817200089271    steps: 542    lr: 1.6000000000000003e-05     evaluation reward: 5.24\n",
            "episode: 1383   score: 2.0   memory length: 307883   epsilon: 0.5883896800089357    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
            "episode: 1384   score: 12.0   memory length: 308484   epsilon: 0.5871997000089615    steps: 601    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
            "episode: 1385   score: 4.0   memory length: 308763   epsilon: 0.5866472800089735    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1386   score: 5.0   memory length: 309072   epsilon: 0.5860354600089868    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1387   score: 7.0   memory length: 309469   epsilon: 0.5852494000090038    steps: 397    lr: 1.6000000000000003e-05     evaluation reward: 5.34\n",
            "episode: 1388   score: 5.0   memory length: 309795   epsilon: 0.5846039200090178    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
            "episode: 1389   score: 5.0   memory length: 310124   epsilon: 0.583952500009032    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
            "episode: 1390   score: 2.0   memory length: 310307   epsilon: 0.5835901600090398    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
            "episode: 1391   score: 8.0   memory length: 310711   epsilon: 0.5827902400090572    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
            "episode: 1392   score: 7.0   memory length: 311097   epsilon: 0.5820259600090738    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
            "episode: 1393   score: 4.0   memory length: 311358   epsilon: 0.581509180009085    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
            "episode: 1394   score: 5.0   memory length: 311687   epsilon: 0.5808577600090992    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
            "episode: 1395   score: 10.0   memory length: 312213   epsilon: 0.5798162800091218    steps: 526    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
            "episode: 1396   score: 11.0   memory length: 312794   epsilon: 0.5786659000091467    steps: 581    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
            "episode: 1397   score: 5.0   memory length: 313087   epsilon: 0.5780857600091593    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
            "episode: 1398   score: 3.0   memory length: 313318   epsilon: 0.5776283800091693    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
            "episode: 1399   score: 5.0   memory length: 313619   epsilon: 0.5770324000091822    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
            "episode: 1400   score: 8.0   memory length: 314091   epsilon: 0.5760978400092025    steps: 472    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
            "episode: 1401   score: 6.0   memory length: 314422   epsilon: 0.5754424600092167    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
            "episode: 1402   score: 9.0   memory length: 314899   epsilon: 0.5744980000092372    steps: 477    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
            "episode: 1403   score: 3.0   memory length: 315147   epsilon: 0.5740069600092479    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
            "episode: 1404   score: 9.0   memory length: 315607   epsilon: 0.5730961600092677    steps: 460    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
            "episode: 1405   score: 9.0   memory length: 316060   epsilon: 0.5721992200092871    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1406   score: 6.0   memory length: 316387   epsilon: 0.5715517600093012    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
            "episode: 1407   score: 8.0   memory length: 316807   epsilon: 0.5707201600093192    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 5.72\n",
            "episode: 1408   score: 7.0   memory length: 317231   epsilon: 0.5698806400093375    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 5.72\n",
            "episode: 1409   score: 8.0   memory length: 317690   epsilon: 0.5689718200093572    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 5.75\n",
            "episode: 1410   score: 5.0   memory length: 318013   epsilon: 0.5683322800093711    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 5.72\n",
            "episode: 1411   score: 7.0   memory length: 318449   epsilon: 0.5674690000093898    steps: 436    lr: 1.6000000000000003e-05     evaluation reward: 5.73\n",
            "episode: 1412   score: 6.0   memory length: 318750   epsilon: 0.5668730200094028    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 5.76\n",
            "episode: 1413   score: 6.0   memory length: 319105   epsilon: 0.566170120009418    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
            "episode: 1414   score: 7.0   memory length: 319457   epsilon: 0.5654731600094332    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
            "episode: 1415   score: 9.0   memory length: 319933   epsilon: 0.5645306800094536    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
            "episode: 1416   score: 8.0   memory length: 320307   epsilon: 0.5637901600094697    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 5.9\n",
            "episode: 1417   score: 5.0   memory length: 320589   epsilon: 0.5632318000094818    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
            "episode: 1418   score: 12.0   memory length: 321064   epsilon: 0.5622913000095022    steps: 475    lr: 1.6000000000000003e-05     evaluation reward: 5.98\n",
            "episode: 1419   score: 10.0   memory length: 321550   epsilon: 0.5613290200095231    steps: 486    lr: 1.6000000000000003e-05     evaluation reward: 5.99\n",
            "episode: 1420   score: 8.0   memory length: 321952   epsilon: 0.5605330600095404    steps: 402    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
            "episode: 1421   score: 5.0   memory length: 322303   epsilon: 0.5598380800095555    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 6.03\n",
            "episode: 1422   score: 12.0   memory length: 322871   epsilon: 0.5587134400095799    steps: 568    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
            "episode: 1423   score: 7.0   memory length: 323230   epsilon: 0.5580026200095953    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
            "episode: 1424   score: 7.0   memory length: 323604   epsilon: 0.5572621000096114    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 6.16\n",
            "episode: 1425   score: 5.0   memory length: 323930   epsilon: 0.5566166200096254    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 6.18\n",
            "episode: 1426   score: 6.0   memory length: 324302   epsilon: 0.5558800600096414    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 6.22\n",
            "episode: 1427   score: 1.0   memory length: 324454   epsilon: 0.5555791000096479    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 6.19\n",
            "episode: 1428   score: 6.0   memory length: 324831   epsilon: 0.5548326400096641    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 6.21\n",
            "episode: 1429   score: 11.0   memory length: 325419   epsilon: 0.5536684000096894    steps: 588    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
            "episode: 1430   score: 6.0   memory length: 325762   epsilon: 0.5529892600097042    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
            "episode: 1431   score: 7.0   memory length: 326149   epsilon: 0.5522230000097208    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 6.33\n",
            "episode: 1432   score: 7.0   memory length: 326547   epsilon: 0.5514349600097379    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 6.37\n",
            "episode: 1433   score: 7.0   memory length: 326924   epsilon: 0.5506885000097541    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 6.41\n",
            "episode: 1434   score: 7.0   memory length: 327362   epsilon: 0.5498212600097729    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 6.47\n",
            "episode: 1435   score: 6.0   memory length: 327691   epsilon: 0.5491698400097871    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 6.47\n",
            "episode: 1436   score: 10.0   memory length: 328228   epsilon: 0.5481065800098102    steps: 537    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
            "episode: 1437   score: 8.0   memory length: 328704   epsilon: 0.5471641000098306    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
            "episode: 1438   score: 4.0   memory length: 328982   epsilon: 0.5466136600098426    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
            "episode: 1439   score: 7.0   memory length: 329369   epsilon: 0.5458474000098592    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
            "episode: 1440   score: 6.0   memory length: 329768   epsilon: 0.5450573800098764    steps: 399    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
            "episode: 1441   score: 10.0   memory length: 330123   epsilon: 0.5443544800098916    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1442   score: 2.0   memory length: 330306   epsilon: 0.5439921400098995    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
            "episode: 1443   score: 5.0   memory length: 330594   epsilon: 0.5434219000099119    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
            "episode: 1444   score: 5.0   memory length: 330887   epsilon: 0.5428417600099245    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
            "episode: 1445   score: 7.0   memory length: 331309   epsilon: 0.5420062000099426    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 6.59\n",
            "episode: 1446   score: 4.0   memory length: 331572   epsilon: 0.5414854600099539    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
            "episode: 1447   score: 5.0   memory length: 331898   epsilon: 0.5408399800099679    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
            "episode: 1448   score: 8.0   memory length: 332312   epsilon: 0.5400202600099857    steps: 414    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1449   score: 8.0   memory length: 332740   epsilon: 0.5391728200100041    steps: 428    lr: 1.6000000000000003e-05     evaluation reward: 6.69\n",
            "episode: 1450   score: 10.0   memory length: 333293   epsilon: 0.5380778800100279    steps: 553    lr: 1.6000000000000003e-05     evaluation reward: 6.75\n",
            "episode: 1451   score: 10.0   memory length: 333795   epsilon: 0.5370839200100495    steps: 502    lr: 1.6000000000000003e-05     evaluation reward: 6.78\n",
            "episode: 1452   score: 7.0   memory length: 334163   epsilon: 0.5363552800100653    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 6.81\n",
            "episode: 1453   score: 6.0   memory length: 334515   epsilon: 0.5356583200100804    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 6.79\n",
            "episode: 1454   score: 5.0   memory length: 334807   epsilon: 0.535080160010093    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 6.76\n",
            "episode: 1455   score: 7.0   memory length: 335177   epsilon: 0.5343475600101089    steps: 370    lr: 1.6000000000000003e-05     evaluation reward: 6.8\n",
            "episode: 1456   score: 8.0   memory length: 335472   epsilon: 0.5337634600101215    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 6.8\n",
            "episode: 1457   score: 6.0   memory length: 335872   epsilon: 0.5329714600101387    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 6.75\n",
            "episode: 1458   score: 4.0   memory length: 336130   epsilon: 0.5324606200101498    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 6.74\n",
            "episode: 1459   score: 4.0   memory length: 336389   epsilon: 0.531947800010161    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 6.74\n",
            "episode: 1460   score: 8.0   memory length: 336838   epsilon: 0.5310587800101803    steps: 449    lr: 1.6000000000000003e-05     evaluation reward: 6.76\n",
            "episode: 1461   score: 14.0   memory length: 337353   epsilon: 0.5300390800102024    steps: 515    lr: 1.6000000000000003e-05     evaluation reward: 6.85\n",
            "episode: 1462   score: 3.0   memory length: 337567   epsilon: 0.5296153600102116    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 6.83\n",
            "episode: 1463   score: 6.0   memory length: 337890   epsilon: 0.5289758200102255    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 6.82\n",
            "episode: 1464   score: 6.0   memory length: 338246   epsilon: 0.5282709400102408    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 6.81\n",
            "episode: 1465   score: 10.0   memory length: 338764   epsilon: 0.527245300010263    steps: 518    lr: 1.6000000000000003e-05     evaluation reward: 6.85\n",
            "episode: 1466   score: 17.0   memory length: 339306   epsilon: 0.5261721400102863    steps: 542    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
            "episode: 1467   score: 9.0   memory length: 339665   epsilon: 0.5254613200103018    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 6.95\n",
            "episode: 1468   score: 11.0   memory length: 340197   epsilon: 0.5244079600103246    steps: 532    lr: 1.6000000000000003e-05     evaluation reward: 6.98\n",
            "episode: 1469   score: 10.0   memory length: 340718   epsilon: 0.523376380010347    steps: 521    lr: 1.6000000000000003e-05     evaluation reward: 7.04\n",
            "episode: 1470   score: 8.0   memory length: 341153   epsilon: 0.5225150800103657    steps: 435    lr: 1.6000000000000003e-05     evaluation reward: 7.02\n",
            "episode: 1471   score: 6.0   memory length: 341496   epsilon: 0.5218359400103805    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 7.04\n",
            "episode: 1472   score: 6.0   memory length: 341808   epsilon: 0.5212181800103939    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 7.08\n",
            "episode: 1473   score: 7.0   memory length: 342158   epsilon: 0.5205251800104089    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 7.05\n",
            "episode: 1474   score: 4.0   memory length: 342398   epsilon: 0.5200499800104192    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 6.96\n",
            "episode: 1475   score: 10.0   memory length: 342923   epsilon: 0.5190104800104418    steps: 525    lr: 1.6000000000000003e-05     evaluation reward: 6.98\n",
            "episode: 1476   score: 6.0   memory length: 343283   epsilon: 0.5182976800104573    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 6.96\n",
            "episode: 1477   score: 5.0   memory length: 343612   epsilon: 0.5176462600104714    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
            "episode: 1478   score: 6.0   memory length: 343965   epsilon: 0.5169473200104866    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 6.96\n",
            "episode: 1479   score: 12.0   memory length: 344473   epsilon: 0.5159414800105084    steps: 508    lr: 1.6000000000000003e-05     evaluation reward: 6.98\n",
            "episode: 1480   score: 8.0   memory length: 344970   epsilon: 0.5149574200105298    steps: 497    lr: 1.6000000000000003e-05     evaluation reward: 7.02\n",
            "episode: 1481   score: 4.0   memory length: 345269   epsilon: 0.5143654000105427    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 6.99\n",
            "episode: 1482   score: 7.0   memory length: 345674   epsilon: 0.5135635000105601    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 6.95\n",
            "episode: 1483   score: 7.0   memory length: 346031   epsilon: 0.5128566400105754    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 7.0\n",
            "episode: 1484   score: 5.0   memory length: 346340   epsilon: 0.5122448200105887    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
            "episode: 1485   score: 8.0   memory length: 346771   epsilon: 0.5113914400106072    steps: 431    lr: 1.6000000000000003e-05     evaluation reward: 6.97\n",
            "episode: 1486   score: 3.0   memory length: 346985   epsilon: 0.5109677200106164    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 6.95\n",
            "episode: 1487   score: 10.0   memory length: 347484   epsilon: 0.5099797000106379    steps: 499    lr: 1.6000000000000003e-05     evaluation reward: 6.98\n",
            "episode: 1488   score: 4.0   memory length: 347760   epsilon: 0.5094332200106497    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 6.97\n",
            "episode: 1489   score: 18.0   memory length: 348377   epsilon: 0.5082115600106762    steps: 617    lr: 1.6000000000000003e-05     evaluation reward: 7.1\n",
            "episode: 1490   score: 7.0   memory length: 348774   epsilon: 0.5074255000106933    steps: 397    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
            "episode: 1491   score: 12.0   memory length: 349201   epsilon: 0.5065800400107117    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
            "episode: 1492   score: 1.0   memory length: 349353   epsilon: 0.5062790800107182    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 7.13\n",
            "episode: 1493   score: 6.0   memory length: 349678   epsilon: 0.5056355800107322    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
            "episode: 1494   score: 2.0   memory length: 349879   epsilon: 0.5052376000107408    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 7.12\n",
            "episode: 1495   score: 8.0   memory length: 350318   epsilon: 0.5043683800107597    steps: 439    lr: 1.6000000000000003e-05     evaluation reward: 7.1\n",
            "episode: 1496   score: 7.0   memory length: 350722   epsilon: 0.503568460010777    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 7.06\n",
            "episode: 1497   score: 9.0   memory length: 351181   epsilon: 0.5026596400107968    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 7.1\n",
            "episode: 1498   score: 7.0   memory length: 351587   epsilon: 0.5018557600108142    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
            "episode: 1499   score: 5.0   memory length: 351876   epsilon: 0.5012835400108266    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
            "episode: 1500   score: 12.0   memory length: 352513   epsilon: 0.500022280010854    steps: 637    lr: 1.6000000000000003e-05     evaluation reward: 7.18\n",
            "episode: 1501   score: 10.0   memory length: 352887   epsilon: 0.49928176001084995    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
            "episode: 1502   score: 10.0   memory length: 353401   epsilon: 0.4982640400108435    steps: 514    lr: 1.6000000000000003e-05     evaluation reward: 7.23\n",
            "episode: 1503   score: 8.0   memory length: 353853   epsilon: 0.49736908001083785    steps: 452    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
            "episode: 1504   score: 7.0   memory length: 354277   epsilon: 0.49652956001083254    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 7.26\n",
            "episode: 1505   score: 8.0   memory length: 354722   epsilon: 0.49564846001082696    steps: 445    lr: 1.6000000000000003e-05     evaluation reward: 7.25\n",
            "episode: 1506   score: 4.0   memory length: 354963   epsilon: 0.49517128001082394    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 7.23\n",
            "episode: 1507   score: 7.0   memory length: 355338   epsilon: 0.49442878001081925    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
            "episode: 1508   score: 7.0   memory length: 355741   epsilon: 0.4936308400108142    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
            "episode: 1509   score: 5.0   memory length: 356054   epsilon: 0.4930111000108103    steps: 313    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
            "episode: 1510   score: 5.0   memory length: 356341   epsilon: 0.4924428400108067    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
            "episode: 1511   score: 8.0   memory length: 356799   epsilon: 0.49153600001080094    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 7.2\n",
            "episode: 1512   score: 4.0   memory length: 357078   epsilon: 0.49098358001079745    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 7.18\n",
            "episode: 1513   score: 4.0   memory length: 357346   epsilon: 0.4904529400107941    steps: 268    lr: 1.6000000000000003e-05     evaluation reward: 7.16\n",
            "episode: 1514   score: 9.0   memory length: 357813   epsilon: 0.48952828001078824    steps: 467    lr: 1.6000000000000003e-05     evaluation reward: 7.18\n",
            "episode: 1515   score: 8.0   memory length: 358258   epsilon: 0.48864718001078267    steps: 445    lr: 1.6000000000000003e-05     evaluation reward: 7.17\n",
            "episode: 1516   score: 12.0   memory length: 358743   epsilon: 0.4876868800107766    steps: 485    lr: 1.6000000000000003e-05     evaluation reward: 7.21\n",
            "episode: 1517   score: 6.0   memory length: 359117   epsilon: 0.4869463600107719    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
            "episode: 1518   score: 2.0   memory length: 359300   epsilon: 0.4865840200107696    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 7.12\n",
            "episode: 1519   score: 7.0   memory length: 359728   epsilon: 0.48573658001076425    steps: 428    lr: 1.6000000000000003e-05     evaluation reward: 7.09\n",
            "episode: 1520   score: 3.0   memory length: 359939   epsilon: 0.4853188000107616    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 7.04\n",
            "episode: 1521   score: 5.0   memory length: 360265   epsilon: 0.4846733200107575    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 7.04\n",
            "episode: 1522   score: 10.0   memory length: 360715   epsilon: 0.4837823200107519    steps: 450    lr: 1.6000000000000003e-05     evaluation reward: 7.02\n",
            "episode: 1523   score: 10.0   memory length: 361209   epsilon: 0.4828042000107457    steps: 494    lr: 1.6000000000000003e-05     evaluation reward: 7.05\n",
            "episode: 1524   score: 3.0   memory length: 361478   epsilon: 0.48227158001074233    steps: 269    lr: 1.6000000000000003e-05     evaluation reward: 7.01\n",
            "episode: 1525   score: 6.0   memory length: 361835   epsilon: 0.48156472001073786    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 7.02\n",
            "episode: 1526   score: 11.0   memory length: 362369   epsilon: 0.48050740001073117    steps: 534    lr: 1.6000000000000003e-05     evaluation reward: 7.07\n",
            "episode: 1527   score: 7.0   memory length: 362763   epsilon: 0.47972728001072623    steps: 394    lr: 1.6000000000000003e-05     evaluation reward: 7.13\n",
            "episode: 1528   score: 8.0   memory length: 363207   epsilon: 0.47884816001072067    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
            "episode: 1529   score: 11.0   memory length: 363745   epsilon: 0.47778292001071393    steps: 538    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
            "episode: 1530   score: 6.0   memory length: 364103   epsilon: 0.47707408001070944    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
            "episode: 1531   score: 4.0   memory length: 364379   epsilon: 0.476527600010706    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 7.12\n",
            "episode: 1532   score: 9.0   memory length: 364886   epsilon: 0.47552374001069964    steps: 507    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
            "episode: 1533   score: 5.0   memory length: 365197   epsilon: 0.47490796001069574    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 7.12\n",
            "episode: 1534   score: 9.0   memory length: 365683   epsilon: 0.47394568001068965    steps: 486    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
            "episode: 1535   score: 3.0   memory length: 365897   epsilon: 0.47352196001068697    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 7.11\n",
            "episode: 1536   score: 7.0   memory length: 366271   epsilon: 0.4727814400106823    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 7.08\n",
            "episode: 1537   score: 5.0   memory length: 366596   epsilon: 0.4721379400106782    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 7.05\n",
            "episode: 1538   score: 6.0   memory length: 366954   epsilon: 0.47142910001067373    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 7.07\n",
            "episode: 1539   score: 7.0   memory length: 367315   epsilon: 0.4707143200106692    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 7.07\n",
            "episode: 1540   score: 10.0   memory length: 367864   epsilon: 0.46962730001066233    steps: 549    lr: 1.6000000000000003e-05     evaluation reward: 7.11\n",
            "episode: 1541   score: 10.0   memory length: 368391   epsilon: 0.46858384001065573    steps: 527    lr: 1.6000000000000003e-05     evaluation reward: 7.11\n",
            "episode: 1542   score: 9.0   memory length: 368900   epsilon: 0.46757602001064935    steps: 509    lr: 1.6000000000000003e-05     evaluation reward: 7.18\n",
            "episode: 1543   score: 8.0   memory length: 369356   epsilon: 0.46667314001064364    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 7.21\n",
            "episode: 1544   score: 10.0   memory length: 369858   epsilon: 0.46567918001063735    steps: 502    lr: 1.6000000000000003e-05     evaluation reward: 7.26\n",
            "episode: 1545   score: 11.0   memory length: 370365   epsilon: 0.464675320010631    steps: 507    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
            "episode: 1546   score: 4.0   memory length: 370625   epsilon: 0.46416052001062774    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
            "episode: 1547   score: 10.0   memory length: 371120   epsilon: 0.46318042001062154    steps: 495    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
            "episode: 1548   score: 9.0   memory length: 371618   epsilon: 0.4621943800106153    steps: 498    lr: 1.6000000000000003e-05     evaluation reward: 7.36\n",
            "episode: 1549   score: 9.0   memory length: 372062   epsilon: 0.46131526001060974    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 7.37\n",
            "episode: 1550   score: 8.0   memory length: 372454   epsilon: 0.46053910001060483    steps: 392    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
            "episode: 1551   score: 7.0   memory length: 372823   epsilon: 0.4598084800106002    steps: 369    lr: 1.6000000000000003e-05     evaluation reward: 7.32\n",
            "episode: 1552   score: 5.0   memory length: 373117   epsilon: 0.4592263600105965    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
            "episode: 1553   score: 5.0   memory length: 373429   epsilon: 0.4586086000105926    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 7.29\n",
            "episode: 1554   score: 7.0   memory length: 373829   epsilon: 0.4578166000105876    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 7.31\n",
            "episode: 1555   score: 8.0   memory length: 374269   epsilon: 0.4569454000105821    steps: 440    lr: 1.6000000000000003e-05     evaluation reward: 7.32\n",
            "episode: 1556   score: 7.0   memory length: 374664   epsilon: 0.45616330001057714    steps: 395    lr: 1.6000000000000003e-05     evaluation reward: 7.31\n",
            "episode: 1557   score: 5.0   memory length: 374990   epsilon: 0.45551782001057306    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
            "episode: 1558   score: 7.0   memory length: 375377   epsilon: 0.4547515600105682    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 7.33\n",
            "episode: 1559   score: 11.0   memory length: 375989   epsilon: 0.45353980001056055    steps: 612    lr: 1.6000000000000003e-05     evaluation reward: 7.4\n",
            "episode: 1560   score: 7.0   memory length: 376378   epsilon: 0.4527695800105557    steps: 389    lr: 1.6000000000000003e-05     evaluation reward: 7.39\n",
            "episode: 1561   score: 10.0   memory length: 376858   epsilon: 0.45181918001054966    steps: 480    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
            "episode: 1562   score: 12.0   memory length: 377334   epsilon: 0.4508767000105437    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 7.44\n",
            "episode: 1563   score: 5.0   memory length: 377662   epsilon: 0.4502272600105396    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 7.43\n",
            "episode: 1564   score: 13.0   memory length: 378115   epsilon: 0.4493303200105339    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 7.5\n",
            "episode: 1565   score: 10.0   memory length: 378653   epsilon: 0.4482650800105272    steps: 538    lr: 1.6000000000000003e-05     evaluation reward: 7.5\n",
            "episode: 1566   score: 7.0   memory length: 379041   epsilon: 0.4474968400105223    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 7.4\n",
            "episode: 1567   score: 26.0   memory length: 379912   epsilon: 0.4457722600105114    steps: 871    lr: 1.6000000000000003e-05     evaluation reward: 7.57\n",
            "episode: 1568   score: 6.0   memory length: 380284   epsilon: 0.44503570001050674    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 7.52\n",
            "episode: 1569   score: 5.0   memory length: 380614   epsilon: 0.4443823000105026    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 7.47\n",
            "episode: 1570   score: 7.0   memory length: 380982   epsilon: 0.443653660010498    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 7.46\n",
            "episode: 1571   score: 14.0   memory length: 381630   epsilon: 0.4423706200104899    steps: 648    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
            "episode: 1572   score: 6.0   memory length: 381984   epsilon: 0.44166970001048544    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
            "episode: 1573   score: 7.0   memory length: 382351   epsilon: 0.44094304001048085    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
            "episode: 1574   score: 9.0   memory length: 382821   epsilon: 0.44001244001047496    steps: 470    lr: 1.6000000000000003e-05     evaluation reward: 7.59\n",
            "episode: 1575   score: 5.0   memory length: 383131   epsilon: 0.4393986400104711    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
            "episode: 1576   score: 9.0   memory length: 383588   epsilon: 0.43849378001046535    steps: 457    lr: 1.6000000000000003e-05     evaluation reward: 7.57\n",
            "episode: 1577   score: 6.0   memory length: 383944   epsilon: 0.4377889000104609    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 7.58\n",
            "episode: 1578   score: 6.0   memory length: 384287   epsilon: 0.4371097600104566    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 7.58\n",
            "episode: 1579   score: 8.0   memory length: 384721   epsilon: 0.43625044001045116    steps: 434    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
            "episode: 1580   score: 8.0   memory length: 385139   epsilon: 0.4354228000104459    steps: 418    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
            "episode: 1581   score: 12.0   memory length: 385736   epsilon: 0.43424074001043844    steps: 597    lr: 1.6000000000000003e-05     evaluation reward: 7.62\n",
            "episode: 1582   score: 8.0   memory length: 386196   epsilon: 0.4333299400104327    steps: 460    lr: 1.6000000000000003e-05     evaluation reward: 7.63\n",
            "episode: 1583   score: 8.0   memory length: 386630   epsilon: 0.43247062001042724    steps: 434    lr: 1.6000000000000003e-05     evaluation reward: 7.64\n",
            "episode: 1584   score: 7.0   memory length: 387059   epsilon: 0.43162120001042187    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 7.66\n",
            "episode: 1585   score: 10.0   memory length: 387545   epsilon: 0.4306589200104158    steps: 486    lr: 1.6000000000000003e-05     evaluation reward: 7.68\n",
            "episode: 1586   score: 9.0   memory length: 388001   epsilon: 0.42975604001041007    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 7.74\n",
            "episode: 1587   score: 10.0   memory length: 388540   epsilon: 0.4286888200104033    steps: 539    lr: 1.6000000000000003e-05     evaluation reward: 7.74\n",
            "episode: 1588   score: 9.0   memory length: 389024   epsilon: 0.42773050001039725    steps: 484    lr: 1.6000000000000003e-05     evaluation reward: 7.79\n",
            "episode: 1589   score: 7.0   memory length: 389400   epsilon: 0.42698602001039254    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 7.68\n",
            "episode: 1590   score: 16.0   memory length: 389982   epsilon: 0.42583366001038525    steps: 582    lr: 1.6000000000000003e-05     evaluation reward: 7.77\n",
            "episode: 1591   score: 8.0   memory length: 390426   epsilon: 0.4249545400103797    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 7.73\n",
            "episode: 1592   score: 9.0   memory length: 390904   epsilon: 0.4240081000103737    steps: 478    lr: 1.6000000000000003e-05     evaluation reward: 7.81\n",
            "episode: 1593   score: 4.0   memory length: 391202   epsilon: 0.42341806001036997    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 7.79\n",
            "episode: 1594   score: 7.0   memory length: 391595   epsilon: 0.42263992001036504    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 7.84\n",
            "episode: 1595   score: 5.0   memory length: 391921   epsilon: 0.42199444001036096    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 7.81\n",
            "episode: 1596   score: 7.0   memory length: 392286   epsilon: 0.4212717400103564    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 7.81\n",
            "episode: 1597   score: 9.0   memory length: 392778   epsilon: 0.4202975800103502    steps: 492    lr: 1.6000000000000003e-05     evaluation reward: 7.81\n",
            "episode: 1598   score: 6.0   memory length: 393135   epsilon: 0.41959072001034575    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 7.8\n",
            "episode: 1599   score: 7.0   memory length: 393522   epsilon: 0.4188244600103409    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 7.82\n",
            "episode: 1600   score: 7.0   memory length: 393892   epsilon: 0.41809186001033627    steps: 370    lr: 1.6000000000000003e-05     evaluation reward: 7.77\n",
            "episode: 1601   score: 8.0   memory length: 394328   epsilon: 0.4172285800103308    steps: 436    lr: 1.6000000000000003e-05     evaluation reward: 7.75\n",
            "episode: 1602   score: 9.0   memory length: 394812   epsilon: 0.41627026001032474    steps: 484    lr: 1.6000000000000003e-05     evaluation reward: 7.74\n",
            "episode: 1603   score: 7.0   memory length: 395158   epsilon: 0.4155851800103204    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 7.73\n",
            "episode: 1604   score: 5.0   memory length: 395448   epsilon: 0.4150109800103168    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 7.71\n",
            "episode: 1605   score: 5.0   memory length: 395738   epsilon: 0.41443678001031314    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 7.68\n",
            "episode: 1606   score: 7.0   memory length: 396162   epsilon: 0.41359726001030783    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 7.71\n",
            "episode: 1607   score: 6.0   memory length: 396507   epsilon: 0.4129141600103035    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 7.7\n",
            "episode: 1608   score: 9.0   memory length: 397014   epsilon: 0.41191030001029716    steps: 507    lr: 1.6000000000000003e-05     evaluation reward: 7.72\n",
            "episode: 1609   score: 8.0   memory length: 397405   epsilon: 0.41113612001029226    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 7.75\n",
            "episode: 1610   score: 6.0   memory length: 397758   epsilon: 0.41043718001028784    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 7.76\n",
            "episode: 1611   score: 10.0   memory length: 398310   epsilon: 0.4093442200102809    steps: 552    lr: 1.6000000000000003e-05     evaluation reward: 7.78\n",
            "episode: 1612   score: 6.0   memory length: 398641   epsilon: 0.4086888400102768    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 7.8\n",
            "episode: 1613   score: 21.0   memory length: 399313   epsilon: 0.40735828001026836    steps: 672    lr: 1.6000000000000003e-05     evaluation reward: 7.97\n",
            "episode: 1614   score: 9.0   memory length: 399783   epsilon: 0.40642768001026247    steps: 470    lr: 1.6000000000000003e-05     evaluation reward: 7.97\n",
            "episode: 1615   score: 12.0   memory length: 400225   epsilon: 0.40555252001025693    steps: 442    lr: 6.400000000000001e-06     evaluation reward: 8.01\n",
            "episode: 1616   score: 5.0   memory length: 400571   epsilon: 0.4048674400102526    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 7.94\n",
            "episode: 1617   score: 9.0   memory length: 401088   epsilon: 0.4038437800102461    steps: 517    lr: 6.400000000000001e-06     evaluation reward: 7.97\n",
            "episode: 1618   score: 7.0   memory length: 401477   epsilon: 0.40307356001024125    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 8.02\n",
            "episode: 1619   score: 6.0   memory length: 401870   epsilon: 0.4022954200102363    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 8.01\n",
            "episode: 1620   score: 5.0   memory length: 402158   epsilon: 0.4017251800102327    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 8.03\n",
            "episode: 1621   score: 15.0   memory length: 402694   epsilon: 0.400663900010226    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 8.13\n",
            "episode: 1622   score: 8.0   memory length: 403179   epsilon: 0.39970360001021993    steps: 485    lr: 6.400000000000001e-06     evaluation reward: 8.11\n",
            "episode: 1623   score: 7.0   memory length: 403540   epsilon: 0.3989888200102154    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 8.08\n",
            "episode: 1624   score: 13.0   memory length: 404140   epsilon: 0.3978008200102079    steps: 600    lr: 6.400000000000001e-06     evaluation reward: 8.18\n",
            "episode: 1625   score: 8.0   memory length: 404593   epsilon: 0.3969038800102022    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 8.2\n",
            "episode: 1626   score: 7.0   memory length: 404988   epsilon: 0.39612178001019727    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 8.16\n",
            "episode: 1627   score: 8.0   memory length: 405397   epsilon: 0.39531196001019214    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
            "episode: 1628   score: 8.0   memory length: 405861   epsilon: 0.39439324001018633    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
            "episode: 1629   score: 7.0   memory length: 406223   epsilon: 0.3936764800101818    steps: 362    lr: 6.400000000000001e-06     evaluation reward: 8.13\n",
            "episode: 1630   score: 9.0   memory length: 406680   epsilon: 0.39277162001017607    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 8.16\n",
            "episode: 1631   score: 7.0   memory length: 407058   epsilon: 0.39202318001017133    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
            "episode: 1632   score: 6.0   memory length: 407392   epsilon: 0.39136186001016715    steps: 334    lr: 6.400000000000001e-06     evaluation reward: 8.16\n",
            "episode: 1633   score: 13.0   memory length: 407969   epsilon: 0.3902194000101599    steps: 577    lr: 6.400000000000001e-06     evaluation reward: 8.24\n",
            "episode: 1634   score: 9.0   memory length: 408462   epsilon: 0.38924326001015375    steps: 493    lr: 6.400000000000001e-06     evaluation reward: 8.24\n",
            "episode: 1635   score: 11.0   memory length: 409002   epsilon: 0.388174060010147    steps: 540    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
            "episode: 1636   score: 6.0   memory length: 409340   epsilon: 0.38750482001014275    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 8.31\n",
            "episode: 1637   score: 9.0   memory length: 409789   epsilon: 0.3866158000101371    steps: 449    lr: 6.400000000000001e-06     evaluation reward: 8.35\n",
            "episode: 1638   score: 6.0   memory length: 410110   epsilon: 0.3859802200101331    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 8.35\n",
            "episode: 1639   score: 15.0   memory length: 410714   epsilon: 0.38478430001012554    steps: 604    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
            "episode: 1640   score: 10.0   memory length: 411236   epsilon: 0.383750740010119    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
            "episode: 1641   score: 4.0   memory length: 411517   epsilon: 0.3831943600101155    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 8.37\n",
            "episode: 1642   score: 5.0   memory length: 411791   epsilon: 0.38265184001011204    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 8.33\n",
            "episode: 1643   score: 13.0   memory length: 412275   epsilon: 0.381693520010106    steps: 484    lr: 6.400000000000001e-06     evaluation reward: 8.38\n",
            "episode: 1644   score: 8.0   memory length: 412687   epsilon: 0.3808777600101008    steps: 412    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
            "episode: 1645   score: 8.0   memory length: 413075   epsilon: 0.38010952001009596    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 8.33\n",
            "episode: 1646   score: 9.0   memory length: 413530   epsilon: 0.37920862001009026    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 8.38\n",
            "episode: 1647   score: 13.0   memory length: 414054   epsilon: 0.3781711000100837    steps: 524    lr: 6.400000000000001e-06     evaluation reward: 8.41\n",
            "episode: 1648   score: 10.0   memory length: 414532   epsilon: 0.3772246600100777    steps: 478    lr: 6.400000000000001e-06     evaluation reward: 8.42\n",
            "episode: 1649   score: 11.0   memory length: 415052   epsilon: 0.3761950600100712    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 8.44\n",
            "episode: 1650   score: 7.0   memory length: 415440   epsilon: 0.37542682001006633    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
            "episode: 1651   score: 4.0   memory length: 415683   epsilon: 0.3749456800100633    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 8.4\n",
            "episode: 1652   score: 10.0   memory length: 416183   epsilon: 0.373955680010057    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 8.45\n",
            "episode: 1653   score: 8.0   memory length: 416637   epsilon: 0.37305676001005134    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 8.48\n",
            "episode: 1654   score: 11.0   memory length: 417146   epsilon: 0.37204894001004496    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 8.52\n",
            "episode: 1655   score: 10.0   memory length: 417644   epsilon: 0.3710629000100387    steps: 498    lr: 6.400000000000001e-06     evaluation reward: 8.54\n",
            "episode: 1656   score: 16.0   memory length: 418350   epsilon: 0.3696650200100299    steps: 706    lr: 6.400000000000001e-06     evaluation reward: 8.63\n",
            "episode: 1657   score: 10.0   memory length: 418710   epsilon: 0.36895222001002537    steps: 360    lr: 6.400000000000001e-06     evaluation reward: 8.68\n",
            "episode: 1658   score: 8.0   memory length: 419138   epsilon: 0.36810478001002    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 8.69\n",
            "episode: 1659   score: 8.0   memory length: 419535   epsilon: 0.36731872001001503    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 8.66\n",
            "episode: 1660   score: 14.0   memory length: 420024   epsilon: 0.3663505000100089    steps: 489    lr: 6.400000000000001e-06     evaluation reward: 8.73\n",
            "episode: 1661   score: 11.0   memory length: 420562   epsilon: 0.36528526001000217    steps: 538    lr: 6.400000000000001e-06     evaluation reward: 8.74\n",
            "episode: 1662   score: 7.0   memory length: 420970   epsilon: 0.36447742000999706    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 8.69\n",
            "episode: 1663   score: 7.0   memory length: 421394   epsilon: 0.36363790000999174    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 8.71\n",
            "episode: 1664   score: 8.0   memory length: 421821   epsilon: 0.3627924400099864    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 8.66\n",
            "episode: 1665   score: 9.0   memory length: 422297   epsilon: 0.36184996000998043    steps: 476    lr: 6.400000000000001e-06     evaluation reward: 8.65\n",
            "episode: 1666   score: 13.0   memory length: 422902   epsilon: 0.36065206000997285    steps: 605    lr: 6.400000000000001e-06     evaluation reward: 8.71\n",
            "episode: 1667   score: 10.0   memory length: 423295   epsilon: 0.35987392000996793    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 8.55\n",
            "episode: 1668   score: 10.0   memory length: 423792   epsilon: 0.3588898600099617    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 8.59\n",
            "episode: 1669   score: 12.0   memory length: 424265   epsilon: 0.3579533200099558    steps: 473    lr: 6.400000000000001e-06     evaluation reward: 8.66\n",
            "episode: 1670   score: 10.0   memory length: 424754   epsilon: 0.35698510000994965    steps: 489    lr: 6.400000000000001e-06     evaluation reward: 8.69\n",
            "episode: 1671   score: 15.0   memory length: 425335   epsilon: 0.3558347200099424    steps: 581    lr: 6.400000000000001e-06     evaluation reward: 8.7\n",
            "episode: 1672   score: 10.0   memory length: 425819   epsilon: 0.3548764000099363    steps: 484    lr: 6.400000000000001e-06     evaluation reward: 8.74\n",
            "episode: 1673   score: 8.0   memory length: 426248   epsilon: 0.35402698000993094    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 8.75\n",
            "episode: 1674   score: 5.0   memory length: 426519   epsilon: 0.35349040000992754    steps: 271    lr: 6.400000000000001e-06     evaluation reward: 8.71\n",
            "episode: 1675   score: 10.0   memory length: 426980   epsilon: 0.35257762000992177    steps: 461    lr: 6.400000000000001e-06     evaluation reward: 8.76\n",
            "episode: 1676   score: 9.0   memory length: 427437   epsilon: 0.35167276000991604    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 8.76\n",
            "episode: 1677   score: 4.0   memory length: 427715   epsilon: 0.35112232000991256    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 8.74\n",
            "episode: 1678   score: 7.0   memory length: 428123   epsilon: 0.35031448000990745    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 8.75\n",
            "episode: 1679   score: 12.0   memory length: 428684   epsilon: 0.3492037000099004    steps: 561    lr: 6.400000000000001e-06     evaluation reward: 8.79\n",
            "episode: 1680   score: 8.0   memory length: 429083   epsilon: 0.3484136800098954    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 8.79\n",
            "episode: 1681   score: 9.0   memory length: 429554   epsilon: 0.3474811000098895    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 8.76\n",
            "episode: 1682   score: 11.0   memory length: 430088   epsilon: 0.34642378000988283    steps: 534    lr: 6.400000000000001e-06     evaluation reward: 8.79\n",
            "episode: 1683   score: 7.0   memory length: 430434   epsilon: 0.3457387000098785    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 8.78\n",
            "episode: 1684   score: 8.0   memory length: 430862   epsilon: 0.34489126000987314    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 8.79\n",
            "episode: 1685   score: 12.0   memory length: 431410   epsilon: 0.34380622000986627    steps: 548    lr: 6.400000000000001e-06     evaluation reward: 8.81\n",
            "episode: 1686   score: 10.0   memory length: 431937   epsilon: 0.34276276000985967    steps: 527    lr: 6.400000000000001e-06     evaluation reward: 8.82\n",
            "episode: 1687   score: 13.0   memory length: 432502   epsilon: 0.3416440600098526    steps: 565    lr: 6.400000000000001e-06     evaluation reward: 8.85\n",
            "episode: 1688   score: 11.0   memory length: 433010   epsilon: 0.3406382200098462    steps: 508    lr: 6.400000000000001e-06     evaluation reward: 8.87\n",
            "episode: 1689   score: 13.0   memory length: 433611   epsilon: 0.3394482400098387    steps: 601    lr: 6.400000000000001e-06     evaluation reward: 8.93\n",
            "episode: 1690   score: 13.0   memory length: 434242   epsilon: 0.3381988600098308    steps: 631    lr: 6.400000000000001e-06     evaluation reward: 8.9\n",
            "episode: 1691   score: 16.0   memory length: 434860   epsilon: 0.33697522000982305    steps: 618    lr: 6.400000000000001e-06     evaluation reward: 8.98\n",
            "episode: 1692   score: 10.0   memory length: 435413   epsilon: 0.3358802800098161    steps: 553    lr: 6.400000000000001e-06     evaluation reward: 8.99\n",
            "episode: 1693   score: 9.0   memory length: 435870   epsilon: 0.3349754200098104    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 9.04\n",
            "episode: 1694   score: 9.0   memory length: 436300   epsilon: 0.334124020009805    steps: 430    lr: 6.400000000000001e-06     evaluation reward: 9.06\n",
            "episode: 1695   score: 8.0   memory length: 436726   epsilon: 0.3332805400097997    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 9.09\n",
            "episode: 1696   score: 6.0   memory length: 437087   epsilon: 0.33256576000979515    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 9.08\n",
            "episode: 1697   score: 10.0   memory length: 437595   epsilon: 0.3315599200097888    steps: 508    lr: 6.400000000000001e-06     evaluation reward: 9.09\n",
            "episode: 1698   score: 10.0   memory length: 438088   epsilon: 0.3305837800097826    steps: 493    lr: 6.400000000000001e-06     evaluation reward: 9.13\n",
            "episode: 1699   score: 15.0   memory length: 438659   epsilon: 0.32945320000977546    steps: 571    lr: 6.400000000000001e-06     evaluation reward: 9.21\n",
            "episode: 1700   score: 15.0   memory length: 439335   epsilon: 0.328114720009767    steps: 676    lr: 6.400000000000001e-06     evaluation reward: 9.29\n",
            "episode: 1701   score: 7.0   memory length: 439739   epsilon: 0.32731480000976193    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 9.28\n",
            "episode: 1702   score: 11.0   memory length: 440248   epsilon: 0.32630698000975555    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 9.3\n",
            "episode: 1703   score: 8.0   memory length: 440682   epsilon: 0.3254476600097501    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 9.31\n",
            "episode: 1704   score: 12.0   memory length: 441207   epsilon: 0.32440816000974354    steps: 525    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1705   score: 9.0   memory length: 441636   epsilon: 0.32355874000973817    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 9.42\n",
            "episode: 1706   score: 12.0   memory length: 442141   epsilon: 0.32255884000973184    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 9.47\n",
            "episode: 1707   score: 15.0   memory length: 442543   epsilon: 0.3217628800097268    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 9.56\n",
            "episode: 1708   score: 5.0   memory length: 442856   epsilon: 0.3211431400097229    steps: 313    lr: 6.400000000000001e-06     evaluation reward: 9.52\n",
            "episode: 1709   score: 16.0   memory length: 443538   epsilon: 0.31979278000971434    steps: 682    lr: 6.400000000000001e-06     evaluation reward: 9.6\n",
            "episode: 1710   score: 9.0   memory length: 444009   epsilon: 0.31886020000970844    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 9.63\n",
            "episode: 1711   score: 11.0   memory length: 444521   epsilon: 0.317846440009702    steps: 512    lr: 6.400000000000001e-06     evaluation reward: 9.64\n",
            "episode: 1712   score: 10.0   memory length: 445001   epsilon: 0.316896040009696    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 9.68\n",
            "episode: 1713   score: 11.0   memory length: 445423   epsilon: 0.3160604800096907    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 9.58\n",
            "episode: 1714   score: 9.0   memory length: 445874   epsilon: 0.3151675000096851    steps: 451    lr: 6.400000000000001e-06     evaluation reward: 9.58\n",
            "episode: 1715   score: 10.0   memory length: 446396   epsilon: 0.31413394000967854    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 9.56\n",
            "episode: 1716   score: 16.0   memory length: 447148   epsilon: 0.3126449800096691    steps: 752    lr: 6.400000000000001e-06     evaluation reward: 9.67\n",
            "episode: 1717   score: 9.0   memory length: 447617   epsilon: 0.31171636000966324    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 9.67\n",
            "episode: 1718   score: 8.0   memory length: 448059   epsilon: 0.3108412000096577    steps: 442    lr: 6.400000000000001e-06     evaluation reward: 9.68\n",
            "episode: 1719   score: 10.0   memory length: 448600   epsilon: 0.3097700200096509    steps: 541    lr: 6.400000000000001e-06     evaluation reward: 9.72\n",
            "episode: 1720   score: 13.0   memory length: 449097   epsilon: 0.3087859600096447    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 9.8\n",
            "episode: 1721   score: 18.0   memory length: 449644   epsilon: 0.30770290000963785    steps: 547    lr: 6.400000000000001e-06     evaluation reward: 9.83\n",
            "episode: 1722   score: 9.0   memory length: 450141   epsilon: 0.3067188400096316    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 9.84\n",
            "episode: 1723   score: 8.0   memory length: 450574   epsilon: 0.3058615000096262    steps: 433    lr: 6.400000000000001e-06     evaluation reward: 9.85\n",
            "episode: 1724   score: 15.0   memory length: 451138   epsilon: 0.30474478000961913    steps: 564    lr: 6.400000000000001e-06     evaluation reward: 9.87\n",
            "episode: 1725   score: 17.0   memory length: 451783   epsilon: 0.30346768000961105    steps: 645    lr: 6.400000000000001e-06     evaluation reward: 9.96\n",
            "episode: 1726   score: 4.0   memory length: 452042   epsilon: 0.3029548600096078    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 9.93\n",
            "episode: 1727   score: 13.0   memory length: 452649   epsilon: 0.3017530000096002    steps: 607    lr: 6.400000000000001e-06     evaluation reward: 9.98\n",
            "episode: 1728   score: 8.0   memory length: 453091   epsilon: 0.30087784000959467    steps: 442    lr: 6.400000000000001e-06     evaluation reward: 9.98\n",
            "episode: 1729   score: 8.0   memory length: 453462   epsilon: 0.30014326000959    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 9.99\n",
            "episode: 1730   score: 7.0   memory length: 453834   epsilon: 0.29940670000958536    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 9.97\n",
            "episode: 1731   score: 10.0   memory length: 454333   epsilon: 0.2984186800095791    steps: 499    lr: 6.400000000000001e-06     evaluation reward: 10.0\n",
            "episode: 1732   score: 16.0   memory length: 454998   epsilon: 0.2971019800095708    steps: 665    lr: 6.400000000000001e-06     evaluation reward: 10.1\n",
            "episode: 1733   score: 16.0   memory length: 455655   epsilon: 0.29580112000956255    steps: 657    lr: 6.400000000000001e-06     evaluation reward: 10.13\n",
            "episode: 1734   score: 15.0   memory length: 456192   epsilon: 0.2947378600095558    steps: 537    lr: 6.400000000000001e-06     evaluation reward: 10.19\n",
            "episode: 1735   score: 12.0   memory length: 456638   epsilon: 0.29385478000955023    steps: 446    lr: 6.400000000000001e-06     evaluation reward: 10.2\n",
            "episode: 1736   score: 8.0   memory length: 457058   epsilon: 0.29302318000954497    steps: 420    lr: 6.400000000000001e-06     evaluation reward: 10.22\n",
            "episode: 1737   score: 12.0   memory length: 457677   epsilon: 0.2917975600095372    steps: 619    lr: 6.400000000000001e-06     evaluation reward: 10.25\n",
            "episode: 1738   score: 9.0   memory length: 458175   epsilon: 0.290811520009531    steps: 498    lr: 6.400000000000001e-06     evaluation reward: 10.28\n",
            "episode: 1739   score: 17.0   memory length: 458763   epsilon: 0.2896472800095236    steps: 588    lr: 6.400000000000001e-06     evaluation reward: 10.3\n",
            "episode: 1740   score: 9.0   memory length: 459270   epsilon: 0.28864342000951726    steps: 507    lr: 6.400000000000001e-06     evaluation reward: 10.29\n",
            "episode: 1741   score: 9.0   memory length: 459752   epsilon: 0.2876890600095112    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 10.34\n",
            "episode: 1742   score: 8.0   memory length: 460165   epsilon: 0.28687132000950605    steps: 413    lr: 6.400000000000001e-06     evaluation reward: 10.37\n",
            "episode: 1743   score: 11.0   memory length: 460686   epsilon: 0.2858397400094995    steps: 521    lr: 6.400000000000001e-06     evaluation reward: 10.35\n",
            "episode: 1744   score: 7.0   memory length: 461055   epsilon: 0.2851091200094949    steps: 369    lr: 6.400000000000001e-06     evaluation reward: 10.34\n",
            "episode: 1745   score: 10.0   memory length: 461545   epsilon: 0.28413892000948876    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 10.36\n",
            "episode: 1746   score: 10.0   memory length: 462042   epsilon: 0.28315486000948253    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 10.37\n",
            "episode: 1747   score: 16.0   memory length: 462788   epsilon: 0.2816777800094732    steps: 746    lr: 6.400000000000001e-06     evaluation reward: 10.4\n",
            "episode: 1748   score: 8.0   memory length: 463147   epsilon: 0.2809669600094687    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 10.38\n",
            "episode: 1749   score: 12.0   memory length: 463718   epsilon: 0.27983638000946154    steps: 571    lr: 6.400000000000001e-06     evaluation reward: 10.39\n",
            "episode: 1750   score: 11.0   memory length: 464208   epsilon: 0.2788661800094554    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 10.43\n",
            "episode: 1751   score: 19.0   memory length: 464772   epsilon: 0.27774946000944833    steps: 564    lr: 6.400000000000001e-06     evaluation reward: 10.58\n",
            "episode: 1752   score: 9.0   memory length: 465203   epsilon: 0.27689608000944294    steps: 431    lr: 6.400000000000001e-06     evaluation reward: 10.57\n",
            "episode: 1753   score: 7.0   memory length: 465590   epsilon: 0.2761298200094381    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 10.56\n",
            "episode: 1754   score: 12.0   memory length: 466172   epsilon: 0.2749774600094308    steps: 582    lr: 6.400000000000001e-06     evaluation reward: 10.57\n",
            "episode: 1755   score: 14.0   memory length: 466804   epsilon: 0.2737261000094229    steps: 632    lr: 6.400000000000001e-06     evaluation reward: 10.61\n",
            "episode: 1756   score: 5.0   memory length: 467130   epsilon: 0.2730806200094188    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 10.5\n",
            "episode: 1757   score: 5.0   memory length: 467423   epsilon: 0.2725004800094151    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 10.45\n",
            "episode: 1758   score: 11.0   memory length: 467916   epsilon: 0.27152434000940895    steps: 493    lr: 6.400000000000001e-06     evaluation reward: 10.48\n",
            "episode: 1759   score: 8.0   memory length: 468357   epsilon: 0.2706511600094034    steps: 441    lr: 6.400000000000001e-06     evaluation reward: 10.48\n",
            "episode: 1760   score: 8.0   memory length: 468836   epsilon: 0.2697027400093974    steps: 479    lr: 6.400000000000001e-06     evaluation reward: 10.42\n",
            "episode: 1761   score: 14.0   memory length: 469434   epsilon: 0.26851870000938993    steps: 598    lr: 6.400000000000001e-06     evaluation reward: 10.45\n",
            "episode: 1762   score: 10.0   memory length: 469923   epsilon: 0.2675504800093838    steps: 489    lr: 6.400000000000001e-06     evaluation reward: 10.48\n",
            "episode: 1763   score: 13.0   memory length: 470557   epsilon: 0.26629516000937586    steps: 634    lr: 6.400000000000001e-06     evaluation reward: 10.54\n",
            "episode: 1764   score: 9.0   memory length: 471015   epsilon: 0.2653883200093701    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 10.55\n",
            "episode: 1765   score: 9.0   memory length: 471469   epsilon: 0.26448940000936444    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 10.55\n",
            "episode: 1766   score: 6.0   memory length: 471844   epsilon: 0.26374690000935974    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 10.48\n",
            "episode: 1767   score: 6.0   memory length: 472190   epsilon: 0.2630618200093554    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 10.44\n",
            "episode: 1768   score: 9.0   memory length: 472683   epsilon: 0.26208568000934923    steps: 493    lr: 6.400000000000001e-06     evaluation reward: 10.43\n",
            "episode: 1769   score: 11.0   memory length: 473276   epsilon: 0.2609115400093418    steps: 593    lr: 6.400000000000001e-06     evaluation reward: 10.42\n",
            "episode: 1770   score: 13.0   memory length: 473759   epsilon: 0.25995520000933575    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 10.45\n",
            "episode: 1771   score: 14.0   memory length: 474291   epsilon: 0.2589018400093291    steps: 532    lr: 6.400000000000001e-06     evaluation reward: 10.44\n",
            "episode: 1772   score: 10.0   memory length: 474781   epsilon: 0.25793164000932295    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 10.44\n",
            "episode: 1773   score: 13.0   memory length: 475414   epsilon: 0.256678300009315    steps: 633    lr: 6.400000000000001e-06     evaluation reward: 10.49\n",
            "episode: 1774   score: 7.0   memory length: 475777   epsilon: 0.25595956000931047    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 10.51\n",
            "episode: 1775   score: 9.0   memory length: 476235   epsilon: 0.25505272000930473    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 10.5\n",
            "episode: 1776   score: 18.0   memory length: 477005   epsilon: 0.2535281200092951    steps: 770    lr: 6.400000000000001e-06     evaluation reward: 10.59\n",
            "episode: 1777   score: 14.0   memory length: 477722   epsilon: 0.2521084600092861    steps: 717    lr: 6.400000000000001e-06     evaluation reward: 10.69\n",
            "episode: 1778   score: 13.0   memory length: 478321   epsilon: 0.2509224400092786    steps: 599    lr: 6.400000000000001e-06     evaluation reward: 10.75\n",
            "episode: 1779   score: 9.0   memory length: 478846   epsilon: 0.24988294000927203    steps: 525    lr: 6.400000000000001e-06     evaluation reward: 10.72\n",
            "episode: 1780   score: 10.0   memory length: 479355   epsilon: 0.24887512000926565    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 10.74\n",
            "episode: 1781   score: 11.0   memory length: 479840   epsilon: 0.24791482000925957    steps: 485    lr: 6.400000000000001e-06     evaluation reward: 10.76\n",
            "episode: 1782   score: 11.0   memory length: 480265   epsilon: 0.24707332000925425    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 10.76\n",
            "episode: 1783   score: 8.0   memory length: 480668   epsilon: 0.2462753800092492    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 10.77\n",
            "episode: 1784   score: 13.0   memory length: 481118   epsilon: 0.24538438000924356    steps: 450    lr: 6.400000000000001e-06     evaluation reward: 10.82\n",
            "episode: 1785   score: 7.0   memory length: 481495   epsilon: 0.24463792000923884    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 10.77\n",
            "episode: 1786   score: 14.0   memory length: 482112   epsilon: 0.2434162600092311    steps: 617    lr: 6.400000000000001e-06     evaluation reward: 10.81\n",
            "episode: 1787   score: 9.0   memory length: 482534   epsilon: 0.24258070000922582    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 10.77\n",
            "episode: 1788   score: 11.0   memory length: 483032   epsilon: 0.2415946600092196    steps: 498    lr: 6.400000000000001e-06     evaluation reward: 10.77\n",
            "episode: 1789   score: 14.0   memory length: 483651   epsilon: 0.24036904000921183    steps: 619    lr: 6.400000000000001e-06     evaluation reward: 10.78\n",
            "episode: 1790   score: 9.0   memory length: 484128   epsilon: 0.23942458000920586    steps: 477    lr: 6.400000000000001e-06     evaluation reward: 10.74\n",
            "episode: 1791   score: 9.0   memory length: 484562   epsilon: 0.23856526000920042    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 10.67\n",
            "episode: 1792   score: 9.0   memory length: 485026   epsilon: 0.2376465400091946    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 10.66\n",
            "episode: 1793   score: 12.0   memory length: 485515   epsilon: 0.23667832000918848    steps: 489    lr: 6.400000000000001e-06     evaluation reward: 10.69\n",
            "episode: 1794   score: 14.0   memory length: 486210   epsilon: 0.23530222000917977    steps: 695    lr: 6.400000000000001e-06     evaluation reward: 10.74\n",
            "episode: 1795   score: 9.0   memory length: 486711   epsilon: 0.2343102400091735    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 10.75\n",
            "episode: 1796   score: 11.0   memory length: 487203   epsilon: 0.23333608000916733    steps: 492    lr: 6.400000000000001e-06     evaluation reward: 10.8\n",
            "episode: 1797   score: 13.0   memory length: 487794   epsilon: 0.23216590000915993    steps: 591    lr: 6.400000000000001e-06     evaluation reward: 10.83\n",
            "episode: 1798   score: 9.0   memory length: 488268   epsilon: 0.231227380009154    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 10.82\n",
            "episode: 1799   score: 11.0   memory length: 488707   epsilon: 0.2303581600091485    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 10.78\n",
            "episode: 1800   score: 12.0   memory length: 489264   epsilon: 0.22925530000914152    steps: 557    lr: 6.400000000000001e-06     evaluation reward: 10.75\n",
            "episode: 1801   score: 9.0   memory length: 489789   epsilon: 0.22821580000913494    steps: 525    lr: 6.400000000000001e-06     evaluation reward: 10.77\n",
            "episode: 1802   score: 10.0   memory length: 490332   epsilon: 0.22714066000912814    steps: 543    lr: 6.400000000000001e-06     evaluation reward: 10.76\n",
            "episode: 1803   score: 11.0   memory length: 490851   epsilon: 0.22611304000912164    steps: 519    lr: 6.400000000000001e-06     evaluation reward: 10.79\n",
            "episode: 1804   score: 12.0   memory length: 491417   epsilon: 0.22499236000911454    steps: 566    lr: 6.400000000000001e-06     evaluation reward: 10.79\n",
            "episode: 1805   score: 7.0   memory length: 491828   epsilon: 0.2241785800091094    steps: 411    lr: 6.400000000000001e-06     evaluation reward: 10.77\n",
            "episode: 1806   score: 5.0   memory length: 492138   epsilon: 0.2235647800091055    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 10.7\n",
            "episode: 1807   score: 11.0   memory length: 492707   epsilon: 0.22243816000909838    steps: 569    lr: 6.400000000000001e-06     evaluation reward: 10.66\n",
            "episode: 1808   score: 10.0   memory length: 493208   epsilon: 0.2214461800090921    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 10.71\n",
            "episode: 1809   score: 10.0   memory length: 493713   epsilon: 0.22044628000908578    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 10.65\n",
            "episode: 1810   score: 10.0   memory length: 494218   epsilon: 0.21944638000907946    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 10.66\n",
            "episode: 1811   score: 12.0   memory length: 494816   epsilon: 0.21826234000907196    steps: 598    lr: 6.400000000000001e-06     evaluation reward: 10.67\n",
            "episode: 1812   score: 9.0   memory length: 495321   epsilon: 0.21726244000906564    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 10.66\n",
            "episode: 1813   score: 17.0   memory length: 495956   epsilon: 0.21600514000905768    steps: 635    lr: 6.400000000000001e-06     evaluation reward: 10.72\n",
            "episode: 1814   score: 10.0   memory length: 496482   epsilon: 0.2149636600090511    steps: 526    lr: 6.400000000000001e-06     evaluation reward: 10.73\n",
            "episode: 1815   score: 11.0   memory length: 496955   epsilon: 0.21402712000904517    steps: 473    lr: 6.400000000000001e-06     evaluation reward: 10.74\n",
            "episode: 1816   score: 15.0   memory length: 497536   epsilon: 0.2128767400090379    steps: 581    lr: 6.400000000000001e-06     evaluation reward: 10.73\n",
            "episode: 1817   score: 6.0   memory length: 497937   epsilon: 0.21208276000903287    steps: 401    lr: 6.400000000000001e-06     evaluation reward: 10.7\n",
            "episode: 1818   score: 18.0   memory length: 498706   epsilon: 0.21056014000902323    steps: 769    lr: 6.400000000000001e-06     evaluation reward: 10.8\n",
            "episode: 1819   score: 9.0   memory length: 499177   epsilon: 0.20962756000901733    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 10.79\n",
            "episode: 1820   score: 12.0   memory length: 499819   epsilon: 0.2083564000090093    steps: 642    lr: 6.400000000000001e-06     evaluation reward: 10.78\n",
            "episode: 1821   score: 11.0   memory length: 500266   epsilon: 0.2074713400090037    steps: 447    lr: 2.560000000000001e-06     evaluation reward: 10.71\n",
            "episode: 1822   score: 16.0   memory length: 500867   epsilon: 0.20628136000899616    steps: 601    lr: 2.560000000000001e-06     evaluation reward: 10.78\n",
            "episode: 1823   score: 18.0   memory length: 501485   epsilon: 0.20505772000898842    steps: 618    lr: 2.560000000000001e-06     evaluation reward: 10.88\n",
            "episode: 1824   score: 14.0   memory length: 502126   epsilon: 0.2037885400089804    steps: 641    lr: 2.560000000000001e-06     evaluation reward: 10.87\n",
            "episode: 1825   score: 12.0   memory length: 502686   epsilon: 0.20267974000897337    steps: 560    lr: 2.560000000000001e-06     evaluation reward: 10.82\n",
            "episode: 1826   score: 9.0   memory length: 503134   epsilon: 0.20179270000896776    steps: 448    lr: 2.560000000000001e-06     evaluation reward: 10.87\n",
            "episode: 1827   score: 13.0   memory length: 503729   epsilon: 0.2006146000089603    steps: 595    lr: 2.560000000000001e-06     evaluation reward: 10.87\n",
            "episode: 1828   score: 11.0   memory length: 504321   epsilon: 0.1994424400089529    steps: 592    lr: 2.560000000000001e-06     evaluation reward: 10.9\n",
            "episode: 1829   score: 15.0   memory length: 504866   epsilon: 0.19836334000894607    steps: 545    lr: 2.560000000000001e-06     evaluation reward: 10.97\n",
            "episode: 1830   score: 12.0   memory length: 505426   epsilon: 0.19725454000893905    steps: 560    lr: 2.560000000000001e-06     evaluation reward: 11.02\n",
            "episode: 1831   score: 18.0   memory length: 505973   epsilon: 0.1961714800089322    steps: 547    lr: 2.560000000000001e-06     evaluation reward: 11.1\n",
            "episode: 1832   score: 14.0   memory length: 506606   epsilon: 0.19491814000892427    steps: 633    lr: 2.560000000000001e-06     evaluation reward: 11.08\n",
            "episode: 1833   score: 6.0   memory length: 506964   epsilon: 0.19420930000891978    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 10.98\n",
            "episode: 1834   score: 11.0   memory length: 507427   epsilon: 0.19329256000891398    steps: 463    lr: 2.560000000000001e-06     evaluation reward: 10.94\n",
            "episode: 1835   score: 13.0   memory length: 508093   epsilon: 0.19197388000890564    steps: 666    lr: 2.560000000000001e-06     evaluation reward: 10.95\n",
            "episode: 1836   score: 7.0   memory length: 508439   epsilon: 0.1912888000089013    steps: 346    lr: 2.560000000000001e-06     evaluation reward: 10.94\n",
            "episode: 1837   score: 13.0   memory length: 509045   epsilon: 0.1900889200088937    steps: 606    lr: 2.560000000000001e-06     evaluation reward: 10.95\n",
            "episode: 1838   score: 7.0   memory length: 509454   epsilon: 0.1892791000088886    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 10.93\n",
            "episode: 1839   score: 14.0   memory length: 510130   epsilon: 0.18794062000888012    steps: 676    lr: 2.560000000000001e-06     evaluation reward: 10.9\n",
            "episode: 1840   score: 12.0   memory length: 510663   epsilon: 0.18688528000887344    steps: 533    lr: 2.560000000000001e-06     evaluation reward: 10.93\n",
            "episode: 1841   score: 18.0   memory length: 511214   epsilon: 0.18579430000886654    steps: 551    lr: 2.560000000000001e-06     evaluation reward: 11.02\n",
            "episode: 1842   score: 8.0   memory length: 511626   epsilon: 0.18497854000886138    steps: 412    lr: 2.560000000000001e-06     evaluation reward: 11.02\n",
            "episode: 1843   score: 16.0   memory length: 512327   epsilon: 0.1835905600088526    steps: 701    lr: 2.560000000000001e-06     evaluation reward: 11.07\n",
            "episode: 1844   score: 7.0   memory length: 512670   epsilon: 0.1829114200088483    steps: 343    lr: 2.560000000000001e-06     evaluation reward: 11.07\n",
            "episode: 1845   score: 8.0   memory length: 513088   epsilon: 0.18208378000884307    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 11.05\n",
            "episode: 1846   score: 13.0   memory length: 513739   epsilon: 0.1807948000088349    steps: 651    lr: 2.560000000000001e-06     evaluation reward: 11.08\n",
            "episode: 1847   score: 16.0   memory length: 514383   epsilon: 0.17951968000882684    steps: 644    lr: 2.560000000000001e-06     evaluation reward: 11.08\n",
            "episode: 1848   score: 15.0   memory length: 514929   epsilon: 0.17843860000882    steps: 546    lr: 2.560000000000001e-06     evaluation reward: 11.15\n",
            "episode: 1849   score: 16.0   memory length: 515526   epsilon: 0.17725654000881252    steps: 597    lr: 2.560000000000001e-06     evaluation reward: 11.19\n",
            "episode: 1850   score: 10.0   memory length: 515998   epsilon: 0.1763219800088066    steps: 472    lr: 2.560000000000001e-06     evaluation reward: 11.18\n",
            "episode: 1851   score: 16.0   memory length: 516724   epsilon: 0.17488450000879752    steps: 726    lr: 2.560000000000001e-06     evaluation reward: 11.15\n",
            "episode: 1852   score: 10.0   memory length: 517080   epsilon: 0.17417962000879306    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 11.16\n",
            "episode: 1853   score: 9.0   memory length: 517544   epsilon: 0.17326090000878724    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 11.18\n",
            "episode: 1854   score: 15.0   memory length: 518176   epsilon: 0.17200954000877933    steps: 632    lr: 2.560000000000001e-06     evaluation reward: 11.21\n",
            "episode: 1855   score: 17.0   memory length: 518748   epsilon: 0.17087698000877216    steps: 572    lr: 2.560000000000001e-06     evaluation reward: 11.24\n",
            "episode: 1856   score: 8.0   memory length: 519210   epsilon: 0.16996222000876637    steps: 462    lr: 2.560000000000001e-06     evaluation reward: 11.27\n",
            "episode: 1857   score: 12.0   memory length: 519730   epsilon: 0.16893262000875986    steps: 520    lr: 2.560000000000001e-06     evaluation reward: 11.34\n",
            "episode: 1858   score: 12.0   memory length: 520290   epsilon: 0.16782382000875284    steps: 560    lr: 2.560000000000001e-06     evaluation reward: 11.35\n",
            "episode: 1859   score: 11.0   memory length: 520848   epsilon: 0.16671898000874585    steps: 558    lr: 2.560000000000001e-06     evaluation reward: 11.38\n",
            "episode: 1860   score: 12.0   memory length: 521423   epsilon: 0.16558048000873865    steps: 575    lr: 2.560000000000001e-06     evaluation reward: 11.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-25-5ee39bec7abc>\", line 53, in <cell line: 3>\n",
            "    pylab.savefig(\"./save_graph/breakout_ddqn.png\") # save graph for training visualization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\", line 1023, in savefig\n",
            "    res = fig.savefig(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\", line 3343, in savefig\n",
            "    self.canvas.print_figure(fname, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\", line 2366, in print_figure\n",
            "    result = print_method(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\", line 2232, in <lambda>\n",
            "    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\", line 509, in print_png\n",
            "    self._print_pil(filename_or_obj, \"png\", pil_kwargs, metadata)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\", line 458, in _print_pil\n",
            "    mpl.image.imsave(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\", line 1689, in imsave\n",
            "    image.save(fname, **pil_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2237, in save\n",
            "    fp = builtins.open(filename, \"w+b\")\n",
            "OSError: [Errno 107] Transport endpoint is not connected: './save_graph/breakout_ddqn.png'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-25-5ee39bec7abc>\", line 53, in <cell line: 3>\n",
            "    pylab.savefig(\"./save_graph/breakout_ddqn.png\") # save graph for training visualization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\", line 1023, in savefig\n",
            "    res = fig.savefig(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\", line 3343, in savefig\n",
            "    self.canvas.print_figure(fname, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\", line 2366, in print_figure\n",
            "    result = print_method(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\", line 2232, in <lambda>\n",
            "    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\", line 509, in print_png\n",
            "    self._print_pil(filename_or_obj, \"png\", pil_kwargs, metadata)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\", line 458, in _print_pil\n",
            "    mpl.image.imsave(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\", line 1689, in imsave\n",
            "    image.save(fname, **pil_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2237, in save\n",
            "    fp = builtins.open(filename, \"w+b\")\n",
            "OSError: [Errno 107] Transport endpoint is not connected: './save_graph/breakout_ddqn.png'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-25-5ee39bec7abc>\", line 53, in <cell line: 3>\n",
            "    pylab.savefig(\"./save_graph/breakout_ddqn.png\") # save graph for training visualization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\", line 1023, in savefig\n",
            "    res = fig.savefig(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\", line 3343, in savefig\n",
            "    self.canvas.print_figure(fname, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\", line 2366, in print_figure\n",
            "    result = print_method(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\", line 2232, in <lambda>\n",
            "    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\", line 509, in print_png\n",
            "    self._print_pil(filename_or_obj, \"png\", pil_kwargs, metadata)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\", line 458, in _print_pil\n",
            "    mpl.image.imsave(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\", line 1689, in imsave\n",
            "    image.save(fname, **pil_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2237, in save\n",
            "    fp = builtins.open(filename, \"w+b\")\n",
            "OSError: [Errno 107] Transport endpoint is not connected: './save_graph/breakout_ddqn.png'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Agent Performance"
      ],
      "metadata": {
        "id": "EL27avkt-1hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_ddqn_latest.pth\")"
      ],
      "metadata": {
        "id": "cjcqNfg0-3Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers import Monitor # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "metadata": {
        "id": "WxhHfnK0-5Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['ale.lives'])\n",
        "        \n",
        "    life = info['ale.lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ],
      "metadata": {
        "id": "RPCxdZnA-5oB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WVqODT6EJFa7",
        "8n9dP21dJFa9",
        "EL27avkt-1hj"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
