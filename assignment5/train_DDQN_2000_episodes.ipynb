{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will be prompted with a window asking to grant permissions\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the path in your Google Drive in the string below. Note: do not escape slashes or spaces\n",
    "import os\n",
    "datadir = \"/content/drive/My Drive/assignment5/\"\n",
    "if not os.path.exists(datadir):\n",
    "  !ln -s \"/content/drive/My Drive/assignment5/\" $datadir\n",
    "os.chdir(datadir)\n",
    "!pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DDQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 0.0   memory length: 123   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.0\n",
      "episode: 1   score: 3.0   memory length: 370   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 2   score: 3.0   memory length: 599   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 3   score: 1.0   memory length: 751   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 4   score: 5.0   memory length: 1063   epsilon: 1.0    steps: 312    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 5   score: 0.0   memory length: 1187   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 6   score: 2.0   memory length: 1406   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 7   score: 0.0   memory length: 1530   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 8   score: 0.0   memory length: 1653   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 9   score: 3.0   memory length: 1902   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 10   score: 1.0   memory length: 2075   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 11   score: 3.0   memory length: 2339   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 12   score: 2.0   memory length: 2537   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7692307692307692\n",
      "episode: 13   score: 0.0   memory length: 2660   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 14   score: 0.0   memory length: 2784   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5333333333333334\n",
      "episode: 15   score: 3.0   memory length: 3031   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 16   score: 3.0   memory length: 3282   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.7058823529411764\n",
      "episode: 17   score: 2.0   memory length: 3500   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.7222222222222223\n",
      "episode: 18   score: 2.0   memory length: 3699   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.736842105263158\n",
      "episode: 19   score: 1.0   memory length: 3869   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 20   score: 0.0   memory length: 3992   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.619047619047619\n",
      "episode: 21   score: 2.0   memory length: 4212   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 22   score: 2.0   memory length: 4429   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6521739130434783\n",
      "episode: 23   score: 2.0   memory length: 4648   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 24   score: 2.0   memory length: 4846   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 25   score: 2.0   memory length: 5045   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6923076923076923\n",
      "episode: 26   score: 2.0   memory length: 5264   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.7037037037037037\n",
      "episode: 27   score: 0.0   memory length: 5387   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 28   score: 2.0   memory length: 5607   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.6551724137931034\n",
      "episode: 29   score: 2.0   memory length: 5827   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 30   score: 0.0   memory length: 5951   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6129032258064515\n",
      "episode: 31   score: 3.0   memory length: 6219   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.65625\n",
      "episode: 32   score: 1.0   memory length: 6390   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 33   score: 2.0   memory length: 6589   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6470588235294117\n",
      "episode: 34   score: 1.0   memory length: 6742   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.6285714285714286\n",
      "episode: 35   score: 0.0   memory length: 6866   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 36   score: 4.0   memory length: 7163   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.6486486486486487\n",
      "episode: 37   score: 2.0   memory length: 7362   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6578947368421053\n",
      "episode: 38   score: 3.0   memory length: 7588   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.6923076923076923\n",
      "episode: 39   score: 2.0   memory length: 7786   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 40   score: 0.0   memory length: 7909   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6585365853658536\n",
      "episode: 41   score: 2.0   memory length: 8127   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 42   score: 4.0   memory length: 8404   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.7209302325581395\n",
      "episode: 43   score: 2.0   memory length: 8603   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7272727272727273\n",
      "episode: 44   score: 2.0   memory length: 8802   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7333333333333334\n",
      "episode: 45   score: 2.0   memory length: 9001   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7391304347826086\n",
      "episode: 46   score: 0.0   memory length: 9125   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.702127659574468\n",
      "episode: 47   score: 0.0   memory length: 9249   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 48   score: 2.0   memory length: 9448   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6734693877551021\n",
      "episode: 49   score: 3.0   memory length: 9675   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 50   score: 2.0   memory length: 9892   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.7058823529411764\n",
      "episode: 51   score: 2.0   memory length: 10092   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.7115384615384615\n",
      "episode: 52   score: 2.0   memory length: 10291   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7169811320754718\n",
      "episode: 53   score: 2.0   memory length: 10489   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7222222222222223\n",
      "episode: 54   score: 0.0   memory length: 10612   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.690909090909091\n",
      "episode: 55   score: 2.0   memory length: 10793   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.6964285714285714\n",
      "episode: 56   score: 0.0   memory length: 10916   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 57   score: 0.0   memory length: 11040   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6379310344827587\n",
      "episode: 58   score: 2.0   memory length: 11258   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6440677966101696\n",
      "episode: 59   score: 1.0   memory length: 11428   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6333333333333333\n",
      "episode: 60   score: 3.0   memory length: 11675   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6557377049180328\n",
      "episode: 61   score: 0.0   memory length: 11799   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6290322580645162\n",
      "episode: 62   score: 4.0   memory length: 12095   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 63   score: 1.0   memory length: 12265   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 64   score: 0.0   memory length: 12388   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6307692307692307\n",
      "episode: 65   score: 0.0   memory length: 12512   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.606060606060606\n",
      "episode: 66   score: 0.0   memory length: 12636   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5820895522388059\n",
      "episode: 67   score: 3.0   memory length: 12901   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.6029411764705883\n",
      "episode: 68   score: 2.0   memory length: 13102   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.608695652173913\n",
      "episode: 69   score: 1.0   memory length: 13272   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 70   score: 2.0   memory length: 13489   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6056338028169015\n",
      "episode: 71   score: 0.0   memory length: 13613   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 72   score: 3.0   memory length: 13862   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.6027397260273972\n",
      "episode: 73   score: 2.0   memory length: 14061   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6081081081081081\n",
      "episode: 74   score: 3.0   memory length: 14311   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.6266666666666667\n",
      "episode: 75   score: 0.0   memory length: 14435   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.605263157894737\n",
      "episode: 76   score: 1.0   memory length: 14586   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5974025974025974\n",
      "episode: 77   score: 0.0   memory length: 14710   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5769230769230769\n",
      "episode: 78   score: 3.0   memory length: 14959   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.5949367088607596\n",
      "episode: 79   score: 3.0   memory length: 15186   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.6125\n",
      "episode: 80   score: 0.0   memory length: 15310   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5925925925925926\n",
      "episode: 81   score: 0.0   memory length: 15433   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5731707317073171\n",
      "episode: 82   score: 5.0   memory length: 15742   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.6144578313253013\n",
      "episode: 83   score: 1.0   memory length: 15912   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6071428571428572\n",
      "episode: 84   score: 3.0   memory length: 16178   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.6235294117647059\n",
      "episode: 85   score: 0.0   memory length: 16301   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6046511627906976\n",
      "episode: 86   score: 3.0   memory length: 16528   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.6206896551724137\n",
      "episode: 87   score: 0.0   memory length: 16652   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6022727272727273\n",
      "episode: 88   score: 2.0   memory length: 16853   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.6067415730337078\n",
      "episode: 89   score: 1.0   memory length: 17004   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 90   score: 3.0   memory length: 17250   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
      "episode: 91   score: 2.0   memory length: 17451   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.6195652173913044\n",
      "episode: 92   score: 1.0   memory length: 17602   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6129032258064515\n",
      "episode: 93   score: 0.0   memory length: 17726   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5957446808510638\n",
      "episode: 94   score: 2.0   memory length: 17945   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 95   score: 0.0   memory length: 18069   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 96   score: 0.0   memory length: 18193   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5670103092783505\n",
      "episode: 97   score: 2.0   memory length: 18409   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 98   score: 0.0   memory length: 18532   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 99   score: 2.0   memory length: 18733   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 100   score: 1.0   memory length: 18885   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 101   score: 0.0   memory length: 19009   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 102   score: 0.0   memory length: 19133   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 103   score: 3.0   memory length: 19363   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 104   score: 2.0   memory length: 19562   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 105   score: 1.0   memory length: 19714   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 106   score: 2.0   memory length: 19913   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 107   score: 2.0   memory length: 20115   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 108   score: 1.0   memory length: 20285   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 109   score: 2.0   memory length: 20504   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 110   score: 3.0   memory length: 20753   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 111   score: 1.0   memory length: 20923   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 112   score: 3.0   memory length: 21169   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 113   score: 0.0   memory length: 21293   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 114   score: 1.0   memory length: 21463   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 115   score: 3.0   memory length: 21729   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 116   score: 3.0   memory length: 21976   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 117   score: 2.0   memory length: 22195   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 118   score: 1.0   memory length: 22347   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 119   score: 2.0   memory length: 22545   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 120   score: 1.0   memory length: 22714   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 121   score: 4.0   memory length: 22990   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 122   score: 0.0   memory length: 23114   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 123   score: 1.0   memory length: 23286   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 124   score: 4.0   memory length: 23562   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 125   score: 2.0   memory length: 23784   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 126   score: 0.0   memory length: 23908   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 127   score: 2.0   memory length: 24106   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 128   score: 5.0   memory length: 24419   epsilon: 1.0    steps: 313    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 129   score: 0.0   memory length: 24543   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 130   score: 2.0   memory length: 24762   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 131   score: 0.0   memory length: 24885   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 132   score: 1.0   memory length: 25036   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 133   score: 0.0   memory length: 25160   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 134   score: 2.0   memory length: 25381   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 135   score: 0.0   memory length: 25505   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 136   score: 2.0   memory length: 25723   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 137   score: 0.0   memory length: 25846   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 138   score: 0.0   memory length: 25970   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 139   score: 3.0   memory length: 26216   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 140   score: 3.0   memory length: 26486   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 141   score: 2.0   memory length: 26687   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 142   score: 0.0   memory length: 26811   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 143   score: 2.0   memory length: 27030   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 144   score: 0.0   memory length: 27153   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 145   score: 2.0   memory length: 27352   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 146   score: 3.0   memory length: 27599   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 147   score: 1.0   memory length: 27750   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 148   score: 1.0   memory length: 27920   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 149   score: 0.0   memory length: 28044   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 150   score: 0.0   memory length: 28167   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 151   score: 4.0   memory length: 28485   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 152   score: 0.0   memory length: 28609   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 153   score: 2.0   memory length: 28811   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 154   score: 1.0   memory length: 28980   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 155   score: 2.0   memory length: 29196   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 156   score: 2.0   memory length: 29415   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 157   score: 2.0   memory length: 29633   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 158   score: 0.0   memory length: 29757   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 159   score: 2.0   memory length: 29955   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 160   score: 1.0   memory length: 30106   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 161   score: 1.0   memory length: 30279   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 162   score: 7.0   memory length: 30600   epsilon: 1.0    steps: 321    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 163   score: 1.0   memory length: 30770   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 164   score: 3.0   memory length: 31018   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 165   score: 1.0   memory length: 31188   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 166   score: 2.0   memory length: 31410   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 167   score: 0.0   memory length: 31533   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 168   score: 0.0   memory length: 31656   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 169   score: 3.0   memory length: 31900   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 170   score: 4.0   memory length: 32217   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 171   score: 2.0   memory length: 32416   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 172   score: 1.0   memory length: 32587   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 173   score: 2.0   memory length: 32804   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 174   score: 2.0   memory length: 33021   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 175   score: 2.0   memory length: 33220   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 176   score: 2.0   memory length: 33402   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 177   score: 2.0   memory length: 33601   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 178   score: 0.0   memory length: 33724   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 179   score: 1.0   memory length: 33897   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 180   score: 1.0   memory length: 34068   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 181   score: 0.0   memory length: 34192   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 182   score: 3.0   memory length: 34459   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 183   score: 0.0   memory length: 34583   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 184   score: 0.0   memory length: 34707   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 185   score: 1.0   memory length: 34859   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 186   score: 4.0   memory length: 35116   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 187   score: 1.0   memory length: 35289   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 188   score: 0.0   memory length: 35413   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 189   score: 0.0   memory length: 35537   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 190   score: 3.0   memory length: 35764   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 191   score: 0.0   memory length: 35888   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 192   score: 4.0   memory length: 36147   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 193   score: 1.0   memory length: 36319   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 194   score: 1.0   memory length: 36488   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 195   score: 2.0   memory length: 36710   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 196   score: 1.0   memory length: 36879   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 197   score: 2.0   memory length: 37096   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 198   score: 3.0   memory length: 37363   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 199   score: 2.0   memory length: 37562   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 200   score: 2.0   memory length: 37761   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 201   score: 0.0   memory length: 37885   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 202   score: 6.0   memory length: 38247   epsilon: 1.0    steps: 362    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 203   score: 5.0   memory length: 38554   epsilon: 1.0    steps: 307    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 204   score: 2.0   memory length: 38752   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 205   score: 0.0   memory length: 38876   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 206   score: 1.0   memory length: 39049   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 207   score: 2.0   memory length: 39248   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 208   score: 5.0   memory length: 39595   epsilon: 1.0    steps: 347    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 209   score: 0.0   memory length: 39719   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 210   score: 3.0   memory length: 39987   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 211   score: 3.0   memory length: 40236   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 212   score: 0.0   memory length: 40360   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 213   score: 1.0   memory length: 40532   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 214   score: 3.0   memory length: 40779   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 215   score: 4.0   memory length: 41055   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 216   score: 0.0   memory length: 41179   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 217   score: 1.0   memory length: 41330   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 218   score: 0.0   memory length: 41454   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 219   score: 1.0   memory length: 41606   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 220   score: 1.0   memory length: 41777   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 221   score: 2.0   memory length: 41978   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 222   score: 1.0   memory length: 42147   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 223   score: 3.0   memory length: 42376   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 224   score: 2.0   memory length: 42574   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 225   score: 0.0   memory length: 42698   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 226   score: 0.0   memory length: 42822   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 227   score: 1.0   memory length: 42993   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 228   score: 3.0   memory length: 43240   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 229   score: 1.0   memory length: 43410   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 230   score: 3.0   memory length: 43659   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 231   score: 1.0   memory length: 43828   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 232   score: 2.0   memory length: 44031   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 233   score: 0.0   memory length: 44154   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 234   score: 0.0   memory length: 44277   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 235   score: 2.0   memory length: 44494   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 236   score: 1.0   memory length: 44665   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 237   score: 1.0   memory length: 44836   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 238   score: 1.0   memory length: 44988   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 239   score: 0.0   memory length: 45112   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 240   score: 4.0   memory length: 45392   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 241   score: 2.0   memory length: 45590   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 242   score: 1.0   memory length: 45759   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 243   score: 2.0   memory length: 45975   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 244   score: 1.0   memory length: 46127   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 245   score: 3.0   memory length: 46395   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 246   score: 0.0   memory length: 46519   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 247   score: 0.0   memory length: 46643   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 248   score: 2.0   memory length: 46841   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 249   score: 0.0   memory length: 46964   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 250   score: 0.0   memory length: 47088   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 251   score: 2.0   memory length: 47311   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 252   score: 3.0   memory length: 47541   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 253   score: 3.0   memory length: 47792   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 254   score: 2.0   memory length: 48011   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 255   score: 2.0   memory length: 48229   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 256   score: 0.0   memory length: 48352   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 257   score: 0.0   memory length: 48476   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 258   score: 0.0   memory length: 48599   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 259   score: 2.0   memory length: 48798   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 260   score: 0.0   memory length: 48922   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 261   score: 4.0   memory length: 49219   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 262   score: 2.0   memory length: 49418   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 263   score: 1.0   memory length: 49570   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 264   score: 1.0   memory length: 49740   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 265   score: 2.0   memory length: 49959   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 266   score: 0.0   memory length: 50083   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 267   score: 1.0   memory length: 50235   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 268   score: 1.0   memory length: 50405   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 269   score: 1.0   memory length: 50556   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 270   score: 2.0   memory length: 50754   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 271   score: 4.0   memory length: 51030   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 272   score: 1.0   memory length: 51182   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 273   score: 3.0   memory length: 51448   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 274   score: 2.0   memory length: 51669   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 275   score: 2.0   memory length: 51851   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 276   score: 2.0   memory length: 52070   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 277   score: 2.0   memory length: 52272   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 278   score: 1.0   memory length: 52442   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 279   score: 1.0   memory length: 52612   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 280   score: 0.0   memory length: 52735   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 281   score: 0.0   memory length: 52858   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 282   score: 1.0   memory length: 53009   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 283   score: 0.0   memory length: 53132   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 284   score: 1.0   memory length: 53302   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 285   score: 2.0   memory length: 53518   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 286   score: 0.0   memory length: 53642   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 287   score: 3.0   memory length: 53873   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 288   score: 1.0   memory length: 54025   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 289   score: 2.0   memory length: 54224   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 290   score: 4.0   memory length: 54483   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 291   score: 2.0   memory length: 54682   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 292   score: 2.0   memory length: 54881   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 293   score: 3.0   memory length: 55128   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 294   score: 1.0   memory length: 55298   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 295   score: 1.0   memory length: 55470   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 296   score: 2.0   memory length: 55689   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 297   score: 3.0   memory length: 55937   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 298   score: 1.0   memory length: 56088   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 299   score: 2.0   memory length: 56307   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 300   score: 2.0   memory length: 56506   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 301   score: 1.0   memory length: 56676   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 302   score: 4.0   memory length: 56972   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 303   score: 1.0   memory length: 57141   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 304   score: 2.0   memory length: 57360   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 305   score: 3.0   memory length: 57587   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 306   score: 2.0   memory length: 57786   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 307   score: 0.0   memory length: 57910   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 308   score: 4.0   memory length: 58212   epsilon: 1.0    steps: 302    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 309   score: 0.0   memory length: 58335   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 310   score: 1.0   memory length: 58486   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 311   score: 0.0   memory length: 58609   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 312   score: 2.0   memory length: 58808   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 313   score: 1.0   memory length: 58977   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 314   score: 3.0   memory length: 59243   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 315   score: 3.0   memory length: 59472   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 316   score: 0.0   memory length: 59596   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 317   score: 2.0   memory length: 59816   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 318   score: 2.0   memory length: 60036   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 319   score: 0.0   memory length: 60159   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 320   score: 0.0   memory length: 60283   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 321   score: 4.0   memory length: 60581   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 322   score: 3.0   memory length: 60809   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 323   score: 0.0   memory length: 60933   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 324   score: 0.0   memory length: 61057   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 325   score: 0.0   memory length: 61181   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 326   score: 1.0   memory length: 61350   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 327   score: 0.0   memory length: 61473   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 328   score: 0.0   memory length: 61596   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 329   score: 3.0   memory length: 61862   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 330   score: 0.0   memory length: 61985   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 331   score: 1.0   memory length: 62137   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 332   score: 1.0   memory length: 62308   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 333   score: 0.0   memory length: 62432   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 334   score: 1.0   memory length: 62602   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 335   score: 0.0   memory length: 62726   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 336   score: 1.0   memory length: 62898   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 337   score: 2.0   memory length: 63098   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 338   score: 0.0   memory length: 63222   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 339   score: 3.0   memory length: 63471   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 340   score: 0.0   memory length: 63595   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 341   score: 2.0   memory length: 63794   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 342   score: 4.0   memory length: 64093   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 343   score: 1.0   memory length: 64263   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 344   score: 0.0   memory length: 64387   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 345   score: 0.0   memory length: 64510   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 346   score: 0.0   memory length: 64634   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 347   score: 1.0   memory length: 64804   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 348   score: 0.0   memory length: 64928   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 349   score: 3.0   memory length: 65195   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 350   score: 2.0   memory length: 65415   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 351   score: 1.0   memory length: 65587   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 352   score: 2.0   memory length: 65786   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 353   score: 5.0   memory length: 66134   epsilon: 1.0    steps: 348    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 354   score: 2.0   memory length: 66333   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 355   score: 3.0   memory length: 66601   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 356   score: 3.0   memory length: 66867   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 357   score: 2.0   memory length: 67086   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 358   score: 0.0   memory length: 67209   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 359   score: 2.0   memory length: 67426   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 360   score: 2.0   memory length: 67624   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 361   score: 0.0   memory length: 67748   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 362   score: 4.0   memory length: 68004   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 363   score: 3.0   memory length: 68231   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 364   score: 1.0   memory length: 68401   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 365   score: 2.0   memory length: 68600   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 366   score: 0.0   memory length: 68724   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 367   score: 3.0   memory length: 68952   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 368   score: 0.0   memory length: 69076   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 369   score: 0.0   memory length: 69199   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 370   score: 2.0   memory length: 69400   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 371   score: 2.0   memory length: 69598   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 372   score: 3.0   memory length: 69825   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 373   score: 1.0   memory length: 69977   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 374   score: 0.0   memory length: 70101   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 375   score: 1.0   memory length: 70252   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 376   score: 1.0   memory length: 70422   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 377   score: 0.0   memory length: 70546   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 378   score: 1.0   memory length: 70716   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 379   score: 1.0   memory length: 70867   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 380   score: 1.0   memory length: 71037   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 381   score: 2.0   memory length: 71239   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 382   score: 4.0   memory length: 71518   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 383   score: 1.0   memory length: 71670   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 384   score: 0.0   memory length: 71794   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 385   score: 0.0   memory length: 71917   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 386   score: 2.0   memory length: 72115   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 387   score: 1.0   memory length: 72287   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 388   score: 0.0   memory length: 72411   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 389   score: 2.0   memory length: 72630   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 390   score: 2.0   memory length: 72847   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 391   score: 2.0   memory length: 73046   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 392   score: 0.0   memory length: 73169   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 393   score: 1.0   memory length: 73321   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 394   score: 1.0   memory length: 73490   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 395   score: 2.0   memory length: 73688   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 396   score: 0.0   memory length: 73812   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 397   score: 1.0   memory length: 73963   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 398   score: 7.0   memory length: 74284   epsilon: 1.0    steps: 321    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 399   score: 1.0   memory length: 74435   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 400   score: 2.0   memory length: 74634   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 401   score: 4.0   memory length: 74931   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 402   score: 1.0   memory length: 75101   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 403   score: 0.0   memory length: 75225   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 404   score: 1.0   memory length: 75378   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 405   score: 0.0   memory length: 75501   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 406   score: 0.0   memory length: 75625   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 407   score: 0.0   memory length: 75749   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 408   score: 1.0   memory length: 75901   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 409   score: 3.0   memory length: 76166   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 410   score: 1.0   memory length: 76337   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 411   score: 0.0   memory length: 76460   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 412   score: 2.0   memory length: 76679   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 413   score: 2.0   memory length: 76878   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 414   score: 0.0   memory length: 77002   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 415   score: 1.0   memory length: 77172   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 416   score: 2.0   memory length: 77390   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 417   score: 1.0   memory length: 77542   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 418   score: 3.0   memory length: 77806   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 419   score: 0.0   memory length: 77929   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 420   score: 2.0   memory length: 78148   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 421   score: 3.0   memory length: 78415   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 422   score: 3.0   memory length: 78662   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 423   score: 2.0   memory length: 78882   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 424   score: 0.0   memory length: 79005   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 425   score: 0.0   memory length: 79129   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 426   score: 2.0   memory length: 79348   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 427   score: 1.0   memory length: 79517   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 428   score: 0.0   memory length: 79641   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 429   score: 6.0   memory length: 79982   epsilon: 1.0    steps: 341    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 430   score: 0.0   memory length: 80105   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 431   score: 2.0   memory length: 80326   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 432   score: 3.0   memory length: 80573   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 433   score: 0.0   memory length: 80697   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 434   score: 2.0   memory length: 80896   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 435   score: 2.0   memory length: 81113   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 436   score: 2.0   memory length: 81315   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 437   score: 3.0   memory length: 81585   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 438   score: 3.0   memory length: 81830   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 439   score: 2.0   memory length: 82048   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 440   score: 0.0   memory length: 82172   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 441   score: 2.0   memory length: 82370   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 442   score: 1.0   memory length: 82540   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 443   score: 2.0   memory length: 82739   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 444   score: 3.0   memory length: 83007   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 445   score: 4.0   memory length: 83327   epsilon: 1.0    steps: 320    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 446   score: 0.0   memory length: 83451   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 447   score: 1.0   memory length: 83621   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 448   score: 1.0   memory length: 83793   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 449   score: 0.0   memory length: 83917   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 450   score: 0.0   memory length: 84041   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 451   score: 1.0   memory length: 84210   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 452   score: 1.0   memory length: 84362   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 453   score: 2.0   memory length: 84560   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 454   score: 1.0   memory length: 84712   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 455   score: 0.0   memory length: 84835   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 456   score: 5.0   memory length: 85129   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 457   score: 3.0   memory length: 85360   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 458   score: 4.0   memory length: 85676   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 459   score: 0.0   memory length: 85800   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 460   score: 0.0   memory length: 85923   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 461   score: 0.0   memory length: 86047   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 462   score: 3.0   memory length: 86295   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 463   score: 2.0   memory length: 86493   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 464   score: 3.0   memory length: 86739   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 465   score: 0.0   memory length: 86863   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 466   score: 2.0   memory length: 87044   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 467   score: 2.0   memory length: 87245   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 468   score: 1.0   memory length: 87416   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 469   score: 1.0   memory length: 87585   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 470   score: 0.0   memory length: 87709   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 471   score: 1.0   memory length: 87861   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 472   score: 0.0   memory length: 87985   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 473   score: 0.0   memory length: 88109   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 474   score: 0.0   memory length: 88233   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 475   score: 2.0   memory length: 88432   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 476   score: 2.0   memory length: 88614   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 477   score: 0.0   memory length: 88738   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 478   score: 0.0   memory length: 88861   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 479   score: 4.0   memory length: 89157   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 480   score: 2.0   memory length: 89379   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 481   score: 2.0   memory length: 89578   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 482   score: 3.0   memory length: 89808   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 483   score: 0.0   memory length: 89932   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 484   score: 3.0   memory length: 90161   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 485   score: 2.0   memory length: 90378   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 486   score: 0.0   memory length: 90501   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 487   score: 0.0   memory length: 90625   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 488   score: 2.0   memory length: 90823   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 489   score: 0.0   memory length: 90946   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 490   score: 0.0   memory length: 91070   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 491   score: 2.0   memory length: 91269   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 492   score: 2.0   memory length: 91469   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 493   score: 1.0   memory length: 91638   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 494   score: 4.0   memory length: 91879   epsilon: 1.0    steps: 241    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 495   score: 0.0   memory length: 92003   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 496   score: 5.0   memory length: 92321   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 497   score: 2.0   memory length: 92538   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 498   score: 4.0   memory length: 92813   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 499   score: 0.0   memory length: 92937   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 500   score: 1.0   memory length: 93107   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 501   score: 1.0   memory length: 93276   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 502   score: 1.0   memory length: 93427   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 503   score: 1.0   memory length: 93598   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 504   score: 2.0   memory length: 93814   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 505   score: 1.0   memory length: 93965   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 506   score: 4.0   memory length: 94283   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 507   score: 1.0   memory length: 94452   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 508   score: 2.0   memory length: 94651   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 509   score: 4.0   memory length: 94929   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 510   score: 1.0   memory length: 95098   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 511   score: 0.0   memory length: 95222   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 512   score: 1.0   memory length: 95374   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 513   score: 0.0   memory length: 95498   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 514   score: 1.0   memory length: 95651   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 515   score: 0.0   memory length: 95774   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 516   score: 2.0   memory length: 95977   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 517   score: 1.0   memory length: 96146   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 518   score: 3.0   memory length: 96378   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 519   score: 1.0   memory length: 96530   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 520   score: 0.0   memory length: 96654   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 521   score: 0.0   memory length: 96778   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 522   score: 0.0   memory length: 96902   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 523   score: 0.0   memory length: 97026   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 524   score: 3.0   memory length: 97295   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 525   score: 0.0   memory length: 97419   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 526   score: 1.0   memory length: 97590   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 527   score: 1.0   memory length: 97763   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 528   score: 1.0   memory length: 97934   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 529   score: 1.0   memory length: 98086   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 530   score: 2.0   memory length: 98285   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 531   score: 2.0   memory length: 98508   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 532   score: 2.0   memory length: 98726   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 533   score: 2.0   memory length: 98925   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 534   score: 1.0   memory length: 99095   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 535   score: 1.0   memory length: 99246   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 536   score: 4.0   memory length: 99524   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 537   score: 0.0   memory length: 99648   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 538   score: 0.0   memory length: 99771   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 539   score: 2.0   memory length: 99990   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nbsyxx\\Desktop\\assignment5\\assignment5_materials\\agent_double.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 540   score: 1.0   memory length: 100000   epsilon: 0.9996812200000069    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 541   score: 0.0   memory length: 100000   epsilon: 0.9994357000000123    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 542   score: 0.0   memory length: 100000   epsilon: 0.9991901800000176    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 543   score: 2.0   memory length: 100000   epsilon: 0.9987981400000261    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 544   score: 1.0   memory length: 100000   epsilon: 0.9984615400000334    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 545   score: 2.0   memory length: 100000   epsilon: 0.9980318800000427    steps: 217    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 546   score: 2.0   memory length: 100000   epsilon: 0.9975982600000521    steps: 219    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 547   score: 0.0   memory length: 100000   epsilon: 0.9973527400000575    steps: 124    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 548   score: 0.0   memory length: 100000   epsilon: 0.9971092000000628    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 549   score: 1.0   memory length: 100000   epsilon: 0.9967726000000701    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 550   score: 3.0   memory length: 100000   epsilon: 0.9962835400000807    steps: 247    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 551   score: 1.0   memory length: 100000   epsilon: 0.9959410000000881    steps: 173    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 552   score: 0.0   memory length: 100000   epsilon: 0.9956954800000934    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 553   score: 0.0   memory length: 100000   epsilon: 0.9954519400000987    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 554   score: 2.0   memory length: 100000   epsilon: 0.9950203000001081    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 555   score: 4.0   memory length: 100000   epsilon: 0.9944896600001196    steps: 268    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 556   score: 2.0   memory length: 100000   epsilon: 0.9940956400001282    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 557   score: 1.0   memory length: 100000   epsilon: 0.9937590400001355    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 558   score: 3.0   memory length: 100000   epsilon: 0.993228400000147    steps: 268    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 559   score: 0.0   memory length: 100000   epsilon: 0.9929848600001523    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 560   score: 2.0   memory length: 100000   epsilon: 0.9925908400001608    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 561   score: 2.0   memory length: 100000   epsilon: 0.9921611800001702    steps: 217    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 562   score: 0.0   memory length: 100000   epsilon: 0.9919156600001755    steps: 124    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 563   score: 2.0   memory length: 100000   epsilon: 0.9914860000001848    steps: 217    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 564   score: 1.0   memory length: 100000   epsilon: 0.9911454400001922    steps: 172    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 565   score: 0.0   memory length: 100000   epsilon: 0.9909019000001975    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 566   score: 2.0   memory length: 100000   epsilon: 0.9904682800002069    steps: 219    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 567   score: 2.0   memory length: 100000   epsilon: 0.9900762400002154    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 568   score: 2.0   memory length: 100000   epsilon: 0.9896426200002248    steps: 219    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 569   score: 1.0   memory length: 100000   epsilon: 0.9893020600002322    steps: 172    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 570   score: 2.0   memory length: 100000   epsilon: 0.9889397200002401    steps: 183    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 571   score: 0.0   memory length: 100000   epsilon: 0.9886942000002454    steps: 124    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 572   score: 2.0   memory length: 100000   epsilon: 0.988300180000254    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 573   score: 1.0   memory length: 100000   epsilon: 0.9879596200002614    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 574   score: 0.0   memory length: 100000   epsilon: 0.9877141000002667    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 575   score: 1.0   memory length: 100000   epsilon: 0.9874151200002732    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 576   score: 3.0   memory length: 100000   epsilon: 0.9868864600002847    steps: 267    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 577   score: 2.0   memory length: 100000   epsilon: 0.9864924400002932    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 578   score: 0.0   memory length: 100000   epsilon: 0.9862469200002986    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 579   score: 0.0   memory length: 100000   epsilon: 0.9860014000003039    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 580   score: 2.0   memory length: 100000   epsilon: 0.9856073800003124    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 581   score: 1.0   memory length: 100000   epsilon: 0.985306420000319    steps: 152    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 582   score: 0.0   memory length: 100000   epsilon: 0.9850628800003243    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 583   score: 1.0   memory length: 100000   epsilon: 0.9847243000003316    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 584   score: 1.0   memory length: 100000   epsilon: 0.9843896800003389    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 585   score: 0.0   memory length: 100000   epsilon: 0.9841441600003442    steps: 124    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 586   score: 0.0   memory length: 100000   epsilon: 0.9838986400003495    steps: 124    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 587   score: 0.0   memory length: 100000   epsilon: 0.9836551000003548    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 588   score: 0.0   memory length: 100000   epsilon: 0.9834095800003602    steps: 124    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 589   score: 1.0   memory length: 100000   epsilon: 0.9830729800003675    steps: 170    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 590   score: 0.0   memory length: 100000   epsilon: 0.9828274600003728    steps: 124    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 591   score: 0.0   memory length: 100000   epsilon: 0.9825839200003781    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 592   score: 0.0   memory length: 100000   epsilon: 0.9823403800003834    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 593   score: 4.0   memory length: 100000   epsilon: 0.9817879600003954    steps: 279    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 594   score: 2.0   memory length: 100000   epsilon: 0.9813543400004048    steps: 219    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 595   score: 5.0   memory length: 100000   epsilon: 0.9807029200004189    steps: 329    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 596   score: 0.0   memory length: 100000   epsilon: 0.9804593800004242    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 597   score: 1.0   memory length: 100000   epsilon: 0.9801247600004315    steps: 169    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 598   score: 0.0   memory length: 100000   epsilon: 0.9798792400004368    steps: 124    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 599   score: 1.0   memory length: 100000   epsilon: 0.9795782800004433    steps: 152    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 600   score: 2.0   memory length: 100000   epsilon: 0.9792159400004512    steps: 183    lr: 0.0001     evaluation reward: 1.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 601   score: 0.0   memory length: 100000   epsilon: 0.9789704200004565    steps: 124    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 602   score: 1.0   memory length: 100000   epsilon: 0.978671440000463    steps: 151    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 603   score: 3.0   memory length: 100000   epsilon: 0.9781804000004737    steps: 248    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 604   score: 3.0   memory length: 100000   epsilon: 0.9777329200004834    steps: 226    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 605   score: 1.0   memory length: 100000   epsilon: 0.9774319600004899    steps: 152    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 606   score: 2.0   memory length: 100000   epsilon: 0.9770379400004985    steps: 199    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 607   score: 1.0   memory length: 100000   epsilon: 0.976736980000505    steps: 152    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 608   score: 0.0   memory length: 100000   epsilon: 0.9764934400005103    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 609   score: 3.0   memory length: 100000   epsilon: 0.975998440000521    steps: 250    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 610   score: 0.0   memory length: 100000   epsilon: 0.9757549000005263    steps: 123    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 611   score: 5.0   memory length: 100000   epsilon: 0.9751114000005403    steps: 325    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 612   score: 2.0   memory length: 100000   epsilon: 0.974713420000549    steps: 201    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 613   score: 0.0   memory length: 100000   epsilon: 0.9744698800005542    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 614   score: 1.0   memory length: 100000   epsilon: 0.9741709000005607    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 615   score: 0.0   memory length: 100000   epsilon: 0.973927360000566    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 616   score: 1.0   memory length: 100000   epsilon: 0.9735927400005733    steps: 169    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 617   score: 2.0   memory length: 100000   epsilon: 0.9731531800005828    steps: 222    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 618   score: 5.0   memory length: 100000   epsilon: 0.9725077000005968    steps: 326    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 619   score: 2.0   memory length: 100000   epsilon: 0.9720721000006063    steps: 220    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 620   score: 0.0   memory length: 100000   epsilon: 0.9718265800006116    steps: 124    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 621   score: 0.0   memory length: 100000   epsilon: 0.971581060000617    steps: 124    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 622   score: 0.0   memory length: 100000   epsilon: 0.9713355400006223    steps: 124    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 623   score: 3.0   memory length: 100000   epsilon: 0.970886080000632    steps: 227    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 624   score: 0.0   memory length: 100000   epsilon: 0.9706405600006374    steps: 124    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 625   score: 0.0   memory length: 100000   epsilon: 0.9703970200006427    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 626   score: 0.0   memory length: 100000   epsilon: 0.9701534800006479    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 627   score: 2.0   memory length: 100000   epsilon: 0.9697238200006573    steps: 217    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 628   score: 1.0   memory length: 100000   epsilon: 0.9693812800006647    steps: 173    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 629   score: 3.0   memory length: 100000   epsilon: 0.9689278600006745    steps: 229    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 630   score: 3.0   memory length: 100000   epsilon: 0.9683972200006861    steps: 268    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 631   score: 0.0   memory length: 100000   epsilon: 0.9681536800006914    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 632   score: 3.0   memory length: 100000   epsilon: 0.9677359000007004    steps: 211    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 633   score: 1.0   memory length: 100000   epsilon: 0.967434940000707    steps: 152    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 634   score: 2.0   memory length: 100000   epsilon: 0.9670429000007155    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 635   score: 2.0   memory length: 100000   epsilon: 0.9666112600007248    steps: 218    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 636   score: 2.0   memory length: 100000   epsilon: 0.9662172400007334    steps: 199    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 637   score: 0.0   memory length: 100000   epsilon: 0.9659717200007387    steps: 124    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 638   score: 1.0   memory length: 100000   epsilon: 0.965635120000746    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 639   score: 1.0   memory length: 100000   epsilon: 0.9653341600007526    steps: 152    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 640   score: 1.0   memory length: 100000   epsilon: 0.9649995400007598    steps: 169    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 641   score: 1.0   memory length: 100000   epsilon: 0.9646629400007671    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 642   score: 2.0   memory length: 100000   epsilon: 0.9642689200007757    steps: 199    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 643   score: 1.0   memory length: 100000   epsilon: 0.963932320000783    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 644   score: 0.0   memory length: 100000   epsilon: 0.9636868000007883    steps: 124    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 645   score: 3.0   memory length: 100000   epsilon: 0.9631898200007991    steps: 251    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 646   score: 2.0   memory length: 100000   epsilon: 0.9627977800008076    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 647   score: 4.0   memory length: 100000   epsilon: 0.9622473400008196    steps: 278    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 648   score: 2.0   memory length: 100000   epsilon: 0.9618473800008283    steps: 202    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 649   score: 1.0   memory length: 100000   epsilon: 0.9615088000008356    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 650   score: 1.0   memory length: 100000   epsilon: 0.9611722000008429    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 651   score: 2.0   memory length: 100000   epsilon: 0.9607801600008514    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 652   score: 1.0   memory length: 100000   epsilon: 0.9604811800008579    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 653   score: 2.0   memory length: 100000   epsilon: 0.9600871600008665    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 654   score: 1.0   memory length: 100000   epsilon: 0.959786200000873    steps: 152    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 655   score: 2.0   memory length: 100000   epsilon: 0.9593882200008816    steps: 201    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 656   score: 1.0   memory length: 100000   epsilon: 0.9590892400008881    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 657   score: 0.0   memory length: 100000   epsilon: 0.9588457000008934    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 658   score: 0.0   memory length: 100000   epsilon: 0.9586001800008987    steps: 124    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 659   score: 1.0   memory length: 100000   epsilon: 0.9582992200009053    steps: 152    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 660   score: 1.0   memory length: 100000   epsilon: 0.9579586600009127    steps: 172    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 661   score: 1.0   memory length: 100000   epsilon: 0.95762206000092    steps: 170    lr: 0.0001     evaluation reward: 1.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 662   score: 4.0   memory length: 100000   epsilon: 0.9570399400009326    steps: 294    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 663   score: 2.0   memory length: 100000   epsilon: 0.9566696800009407    steps: 187    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 664   score: 0.0   memory length: 100000   epsilon: 0.956424160000946    steps: 124    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 665   score: 2.0   memory length: 100000   epsilon: 0.9560222200009547    steps: 203    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 666   score: 0.0   memory length: 100000   epsilon: 0.95577670000096    steps: 124    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 667   score: 0.0   memory length: 100000   epsilon: 0.9555311800009654    steps: 124    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 668   score: 1.0   memory length: 100000   epsilon: 0.9551945800009727    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 669   score: 1.0   memory length: 100000   epsilon: 0.9548936200009792    steps: 152    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 670   score: 2.0   memory length: 100000   epsilon: 0.9544936600009879    steps: 202    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 671   score: 1.0   memory length: 100000   epsilon: 0.9541570600009952    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 672   score: 1.0   memory length: 100000   epsilon: 0.9538204600010025    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 673   score: 0.0   memory length: 100000   epsilon: 0.9535749400010078    steps: 124    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 674   score: 1.0   memory length: 100000   epsilon: 0.9532403200010151    steps: 169    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 675   score: 0.0   memory length: 100000   epsilon: 0.9529948000010204    steps: 124    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 676   score: 1.0   memory length: 100000   epsilon: 0.952693840001027    steps: 152    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 677   score: 1.0   memory length: 100000   epsilon: 0.9523948600010335    steps: 151    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 678   score: 0.0   memory length: 100000   epsilon: 0.9521493400010388    steps: 124    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 679   score: 4.0   memory length: 100000   epsilon: 0.9516365200010499    steps: 259    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 680   score: 2.0   memory length: 100000   epsilon: 0.9512405200010585    steps: 200    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 681   score: 0.0   memory length: 100000   epsilon: 0.9509950000010639    steps: 124    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 682   score: 3.0   memory length: 100000   epsilon: 0.9504623800010754    steps: 269    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 683   score: 3.0   memory length: 100000   epsilon: 0.949973320001086    steps: 247    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 684   score: 1.0   memory length: 100000   epsilon: 0.9496327600010934    steps: 172    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 685   score: 3.0   memory length: 100000   epsilon: 0.9491417200011041    steps: 248    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 686   score: 0.0   memory length: 100000   epsilon: 0.9488962000011094    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 687   score: 2.0   memory length: 100000   epsilon: 0.948502180001118    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 688   score: 2.0   memory length: 100000   epsilon: 0.9481061800011266    steps: 200    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 689   score: 0.0   memory length: 100000   epsilon: 0.9478606600011319    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 690   score: 1.0   memory length: 100000   epsilon: 0.9475201000011393    steps: 172    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 691   score: 0.0   memory length: 100000   epsilon: 0.9472765600011446    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 692   score: 0.0   memory length: 100000   epsilon: 0.9470310400011499    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 693   score: 3.0   memory length: 100000   epsilon: 0.9465043600011613    steps: 266    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 694   score: 2.0   memory length: 100000   epsilon: 0.9460707400011708    steps: 219    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 695   score: 0.0   memory length: 100000   epsilon: 0.9458252200011761    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 696   score: 3.0   memory length: 100000   epsilon: 0.9453718000011859    steps: 229    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 697   score: 2.0   memory length: 100000   epsilon: 0.9450074800011938    steps: 184    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 698   score: 1.0   memory length: 100000   epsilon: 0.9446728600012011    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 699   score: 2.0   memory length: 100000   epsilon: 0.9442788400012097    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 700   score: 1.0   memory length: 100000   epsilon: 0.943938280001217    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 701   score: 1.0   memory length: 100000   epsilon: 0.9435977200012244    steps: 172    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 702   score: 3.0   memory length: 100000   epsilon: 0.943108660001235    steps: 247    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 703   score: 0.0   memory length: 100000   epsilon: 0.9428631400012404    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 704   score: 2.0   memory length: 100000   epsilon: 0.9424295200012498    steps: 219    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 705   score: 1.0   memory length: 100000   epsilon: 0.9421285600012563    steps: 152    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 706   score: 0.0   memory length: 100000   epsilon: 0.9418850200012616    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 707   score: 2.0   memory length: 100000   epsilon: 0.941453380001271    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 708   score: 4.0   memory length: 100000   epsilon: 0.9408613600012838    steps: 299    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 709   score: 2.0   memory length: 100000   epsilon: 0.9404673400012924    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 710   score: 5.0   memory length: 100000   epsilon: 0.9397802800013073    steps: 347    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 711   score: 2.0   memory length: 100000   epsilon: 0.9393862600013159    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 712   score: 2.0   memory length: 100000   epsilon: 0.9389526400013253    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 713   score: 0.0   memory length: 100000   epsilon: 0.9387071200013306    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 714   score: 3.0   memory length: 100000   epsilon: 0.9382556800013404    steps: 228    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 715   score: 0.0   memory length: 100000   epsilon: 0.9380121400013457    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 716   score: 2.0   memory length: 100000   epsilon: 0.9376121800013544    steps: 202    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 717   score: 2.0   memory length: 100000   epsilon: 0.9372102400013631    steps: 203    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 718   score: 2.0   memory length: 100000   epsilon: 0.9368182000013716    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 719   score: 1.0   memory length: 100000   epsilon: 0.9364816000013789    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 720   score: 0.0   memory length: 100000   epsilon: 0.9362360800013843    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 721   score: 4.0   memory length: 100000   epsilon: 0.9357212800013954    steps: 260    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 722   score: 1.0   memory length: 100000   epsilon: 0.935420320001402    steps: 152    lr: 0.0001     evaluation reward: 1.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 723   score: 2.0   memory length: 100000   epsilon: 0.9350619400014097    steps: 181    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 724   score: 2.0   memory length: 100000   epsilon: 0.9346283200014192    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 725   score: 1.0   memory length: 100000   epsilon: 0.9342897400014265    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 726   score: 2.0   memory length: 100000   epsilon: 0.9338957200014351    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 727   score: 4.0   memory length: 100000   epsilon: 0.9333056800014479    steps: 298    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 728   score: 0.0   memory length: 100000   epsilon: 0.9330601600014532    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 729   score: 1.0   memory length: 100000   epsilon: 0.9327176200014606    steps: 173    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 730   score: 3.0   memory length: 100000   epsilon: 0.9321830200014722    steps: 270    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 731   score: 1.0   memory length: 100000   epsilon: 0.9318820600014788    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 732   score: 0.0   memory length: 100000   epsilon: 0.9316365400014841    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 733   score: 5.0   memory length: 100000   epsilon: 0.9309851200014982    steps: 329    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 734   score: 1.0   memory length: 100000   epsilon: 0.9306841600015048    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 735   score: 4.0   memory length: 100000   epsilon: 0.9300980800015175    steps: 296    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 736   score: 3.0   memory length: 100000   epsilon: 0.9296110000015281    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 737   score: 2.0   memory length: 100000   epsilon: 0.9292189600015366    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 738   score: 1.0   memory length: 100000   epsilon: 0.9288823600015439    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 739   score: 2.0   memory length: 100000   epsilon: 0.9284507200015533    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 740   score: 0.0   memory length: 100000   epsilon: 0.9282052000015586    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 741   score: 2.0   memory length: 100000   epsilon: 0.9278012800015674    steps: 204    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 742   score: 2.0   memory length: 100000   epsilon: 0.9274072600015759    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 743   score: 1.0   memory length: 100000   epsilon: 0.9270706600015832    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 744   score: 2.0   memory length: 100000   epsilon: 0.926712280001591    steps: 181    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 745   score: 2.0   memory length: 100000   epsilon: 0.9263202400015995    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 746   score: 1.0   memory length: 100000   epsilon: 0.926019280001606    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 747   score: 2.0   memory length: 100000   epsilon: 0.9255797200016156    steps: 222    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 748   score: 1.0   memory length: 100000   epsilon: 0.925239160001623    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 749   score: 4.0   memory length: 100000   epsilon: 0.9246946600016348    steps: 275    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 750   score: 6.0   memory length: 100000   epsilon: 0.9239086000016519    steps: 397    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 751   score: 0.0   memory length: 100000   epsilon: 0.9236630800016572    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 752   score: 3.0   memory length: 100000   epsilon: 0.923211640001667    steps: 228    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 753   score: 0.0   memory length: 100000   epsilon: 0.9229681000016723    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 754   score: 1.0   memory length: 100000   epsilon: 0.9226295200016796    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 755   score: 1.0   memory length: 100000   epsilon: 0.922288960001687    steps: 172    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 756   score: 3.0   memory length: 100000   epsilon: 0.9218395000016968    steps: 227    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 757   score: 3.0   memory length: 100000   epsilon: 0.9213484600017074    steps: 248    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 758   score: 1.0   memory length: 100000   epsilon: 0.9210494800017139    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 759   score: 2.0   memory length: 100000   epsilon: 0.9206138800017234    steps: 220    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 760   score: 2.0   memory length: 100000   epsilon: 0.9202218400017319    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 761   score: 3.0   memory length: 100000   epsilon: 0.9197308000017426    steps: 248    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 762   score: 2.0   memory length: 100000   epsilon: 0.919295200001752    steps: 220    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 763   score: 2.0   memory length: 100000   epsilon: 0.9188972200017607    steps: 201    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 764   score: 0.0   memory length: 100000   epsilon: 0.918653680001766    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 765   score: 0.0   memory length: 100000   epsilon: 0.9184081600017713    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 766   score: 3.0   memory length: 100000   epsilon: 0.9179191000017819    steps: 247    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 767   score: 3.0   memory length: 100000   epsilon: 0.9173963800017932    steps: 264    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 768   score: 5.0   memory length: 100000   epsilon: 0.9167113000018081    steps: 346    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 769   score: 5.0   memory length: 100000   epsilon: 0.9160638400018222    steps: 327    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 770   score: 2.0   memory length: 100000   epsilon: 0.9156995200018301    steps: 184    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 771   score: 4.0   memory length: 100000   epsilon: 0.9151174000018427    steps: 294    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 772   score: 0.0   memory length: 100000   epsilon: 0.914871880001848    steps: 124    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 773   score: 3.0   memory length: 100000   epsilon: 0.9143808400018587    steps: 248    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 774   score: 0.0   memory length: 100000   epsilon: 0.914137300001864    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 775   score: 0.0   memory length: 100000   epsilon: 0.9138917800018693    steps: 124    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 776   score: 1.0   memory length: 100000   epsilon: 0.9135551800018766    steps: 170    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 777   score: 4.0   memory length: 100000   epsilon: 0.9130087000018885    steps: 276    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 778   score: 4.0   memory length: 100000   epsilon: 0.9124246000019012    steps: 295    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 779   score: 4.0   memory length: 100000   epsilon: 0.9118444600019138    steps: 293    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 780   score: 2.0   memory length: 100000   epsilon: 0.9114504400019223    steps: 199    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 781   score: 5.0   memory length: 100000   epsilon: 0.9107614000019373    steps: 348    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 782   score: 0.0   memory length: 100000   epsilon: 0.9105158800019426    steps: 124    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 783   score: 2.0   memory length: 100000   epsilon: 0.9101159200019513    steps: 202    lr: 0.0001     evaluation reward: 1.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 784   score: 1.0   memory length: 100000   epsilon: 0.9097773400019586    steps: 171    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 785   score: 0.0   memory length: 100000   epsilon: 0.9095338000019639    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 786   score: 3.0   memory length: 100000   epsilon: 0.9090447400019745    steps: 247    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 787   score: 0.0   memory length: 100000   epsilon: 0.9088012000019798    steps: 123    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 788   score: 3.0   memory length: 100000   epsilon: 0.9083121400019905    steps: 247    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 789   score: 2.0   memory length: 100000   epsilon: 0.907920100001999    steps: 198    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 790   score: 0.0   memory length: 100000   epsilon: 0.9076745800020043    steps: 124    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 791   score: 0.0   memory length: 100000   epsilon: 0.9074310400020096    steps: 123    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 792   score: 3.0   memory length: 100000   epsilon: 0.9069716800020196    steps: 232    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 793   score: 4.0   memory length: 100000   epsilon: 0.9064608400020306    steps: 258    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 794   score: 3.0   memory length: 100000   epsilon: 0.9060391000020398    steps: 213    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 795   score: 2.0   memory length: 100000   epsilon: 0.9056767600020477    steps: 183    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 796   score: 2.0   memory length: 100000   epsilon: 0.905245120002057    steps: 218    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 797   score: 1.0   memory length: 100000   epsilon: 0.9049441600020636    steps: 152    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 798   score: 5.0   memory length: 100000   epsilon: 0.9042947200020777    steps: 328    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 799   score: 3.0   memory length: 100000   epsilon: 0.9038036800020883    steps: 248    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 800   score: 2.0   memory length: 100000   epsilon: 0.9034096600020969    steps: 199    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 801   score: 5.0   memory length: 100000   epsilon: 0.9027681400021108    steps: 324    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 802   score: 1.0   memory length: 100000   epsilon: 0.9024335200021181    steps: 169    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 803   score: 2.0   memory length: 100000   epsilon: 0.9020395000021266    steps: 199    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 804   score: 2.0   memory length: 100000   epsilon: 0.9016791400021344    steps: 182    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 805   score: 0.0   memory length: 100000   epsilon: 0.9014336200021398    steps: 124    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 806   score: 2.0   memory length: 100000   epsilon: 0.9010396000021483    steps: 199    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 807   score: 3.0   memory length: 100000   epsilon: 0.9005069800021599    steps: 269    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 808   score: 3.0   memory length: 100000   epsilon: 0.8999803000021713    steps: 266    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 809   score: 3.0   memory length: 100000   epsilon: 0.8995308400021811    steps: 227    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 810   score: 2.0   memory length: 100000   epsilon: 0.8991368200021896    steps: 199    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 811   score: 2.0   memory length: 100000   epsilon: 0.8987447800021982    steps: 198    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 812   score: 1.0   memory length: 100000   epsilon: 0.8984438200022047    steps: 152    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 813   score: 4.0   memory length: 100000   epsilon: 0.8978973400022165    steps: 276    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 814   score: 1.0   memory length: 100000   epsilon: 0.8975607400022239    steps: 170    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 815   score: 3.0   memory length: 100000   epsilon: 0.8971132600022336    steps: 226    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 816   score: 1.0   memory length: 100000   epsilon: 0.8968123000022401    steps: 152    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 817   score: 0.0   memory length: 100000   epsilon: 0.8965687600022454    steps: 123    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 818   score: 4.0   memory length: 100000   epsilon: 0.895941100002259    steps: 317    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 819   score: 2.0   memory length: 100000   epsilon: 0.8955431200022677    steps: 201    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 820   score: 5.0   memory length: 100000   epsilon: 0.8949629800022803    steps: 293    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 821   score: 2.0   memory length: 100000   epsilon: 0.8945689600022888    steps: 199    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 822   score: 5.0   memory length: 100000   epsilon: 0.8938858600023036    steps: 345    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 823   score: 4.0   memory length: 100000   epsilon: 0.8933354200023156    steps: 278    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 824   score: 3.0   memory length: 100000   epsilon: 0.8928503200023261    steps: 245    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 825   score: 2.0   memory length: 100000   epsilon: 0.8924206600023354    steps: 217    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 826   score: 2.0   memory length: 100000   epsilon: 0.8919850600023449    steps: 220    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 827   score: 2.0   memory length: 100000   epsilon: 0.8915930200023534    steps: 198    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 828   score: 1.0   memory length: 100000   epsilon: 0.8912920600023599    steps: 152    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 829   score: 5.0   memory length: 100000   epsilon: 0.8906069800023748    steps: 346    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 830   score: 2.0   memory length: 100000   epsilon: 0.8901694000023843    steps: 221    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 831   score: 0.0   memory length: 100000   epsilon: 0.8899258600023896    steps: 123    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 832   score: 4.0   memory length: 100000   epsilon: 0.8893774000024015    steps: 277    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 833   score: 1.0   memory length: 100000   epsilon: 0.889076440002408    steps: 152    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 834   score: 0.0   memory length: 100000   epsilon: 0.8888309200024134    steps: 124    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 835   score: 4.0   memory length: 100000   epsilon: 0.8882408800024262    steps: 298    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 836   score: 2.0   memory length: 100000   epsilon: 0.887878540002434    steps: 183    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 837   score: 1.0   memory length: 100000   epsilon: 0.8875379800024414    steps: 172    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 838   score: 1.0   memory length: 100000   epsilon: 0.8872013800024487    steps: 170    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 839   score: 0.0   memory length: 100000   epsilon: 0.8869558600024541    steps: 124    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 840   score: 0.0   memory length: 100000   epsilon: 0.8867103400024594    steps: 124    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 841   score: 4.0   memory length: 100000   epsilon: 0.8861163400024723    steps: 300    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 842   score: 1.0   memory length: 100000   epsilon: 0.8858153800024788    steps: 152    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 843   score: 1.0   memory length: 100000   epsilon: 0.8855144200024854    steps: 152    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 844   score: 3.0   memory length: 100000   epsilon: 0.8850273400024959    steps: 246    lr: 0.0001     evaluation reward: 2.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 845   score: 4.0   memory length: 100000   epsilon: 0.884516500002507    steps: 258    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 846   score: 1.0   memory length: 100000   epsilon: 0.8841799000025143    steps: 170    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 847   score: 3.0   memory length: 100000   epsilon: 0.8836532200025258    steps: 266    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 848   score: 5.0   memory length: 100000   epsilon: 0.8830711000025384    steps: 294    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 849   score: 0.0   memory length: 100000   epsilon: 0.8828255800025437    steps: 124    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 850   score: 4.0   memory length: 100000   epsilon: 0.8823107800025549    steps: 260    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 851   score: 2.0   memory length: 100000   epsilon: 0.8819167600025635    steps: 199    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 852   score: 6.0   memory length: 100000   epsilon: 0.8811722800025796    steps: 376    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 853   score: 5.0   memory length: 100000   epsilon: 0.8805347200025935    steps: 322    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 854   score: 0.0   memory length: 100000   epsilon: 0.8802892000025988    steps: 124    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 855   score: 2.0   memory length: 100000   epsilon: 0.8798951800026074    steps: 199    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 856   score: 0.0   memory length: 100000   epsilon: 0.8796516400026126    steps: 123    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 857   score: 0.0   memory length: 100000   epsilon: 0.879406120002618    steps: 124    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 858   score: 1.0   memory length: 100000   epsilon: 0.8791051600026245    steps: 152    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 859   score: 3.0   memory length: 100000   epsilon: 0.8786418400026346    steps: 234    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 860   score: 1.0   memory length: 100000   epsilon: 0.8783052400026419    steps: 170    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 861   score: 4.0   memory length: 100000   epsilon: 0.8777211400026546    steps: 295    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 862   score: 2.0   memory length: 100000   epsilon: 0.8772895000026639    steps: 218    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 863   score: 1.0   memory length: 100000   epsilon: 0.8769885400026705    steps: 152    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 864   score: 5.0   memory length: 100000   epsilon: 0.8763034600026853    steps: 346    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 865   score: 5.0   memory length: 100000   epsilon: 0.8756421400026997    steps: 334    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 866   score: 4.0   memory length: 100000   epsilon: 0.8750164600027133    steps: 316    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 867   score: 1.0   memory length: 100000   epsilon: 0.8746778800027206    steps: 171    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 868   score: 4.0   memory length: 100000   epsilon: 0.8741353600027324    steps: 274    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 869   score: 0.0   memory length: 100000   epsilon: 0.8738918200027377    steps: 123    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 870   score: 3.0   memory length: 100000   epsilon: 0.8734007800027483    steps: 248    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 871   score: 0.0   memory length: 100000   epsilon: 0.8731572400027536    steps: 123    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 872   score: 2.0   memory length: 100000   epsilon: 0.8727216400027631    steps: 220    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 873   score: 4.0   memory length: 100000   epsilon: 0.8721335800027759    steps: 297    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 874   score: 2.0   memory length: 100000   epsilon: 0.8717395600027844    steps: 199    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 875   score: 2.0   memory length: 100000   epsilon: 0.871341580002793    steps: 201    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 876   score: 2.0   memory length: 100000   epsilon: 0.8709475600028016    steps: 199    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 877   score: 2.0   memory length: 100000   epsilon: 0.8705872000028094    steps: 182    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 878   score: 1.0   memory length: 100000   epsilon: 0.870286240002816    steps: 152    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 879   score: 1.0   memory length: 100000   epsilon: 0.8699852800028225    steps: 152    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 880   score: 3.0   memory length: 100000   epsilon: 0.8695298800028324    steps: 230    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 881   score: 1.0   memory length: 100000   epsilon: 0.869226940002839    steps: 153    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 882   score: 3.0   memory length: 100000   epsilon: 0.8687378800028496    steps: 247    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 883   score: 3.0   memory length: 100000   epsilon: 0.8682864400028594    steps: 228    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 884   score: 1.0   memory length: 100000   epsilon: 0.8679478600028667    steps: 171    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 885   score: 2.0   memory length: 100000   epsilon: 0.8675894800028745    steps: 181    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 886   score: 4.0   memory length: 100000   epsilon: 0.8670786400028856    steps: 258    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 887   score: 3.0   memory length: 100000   epsilon: 0.8665876000028963    steps: 248    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 888   score: 4.0   memory length: 100000   epsilon: 0.8660371600029082    steps: 278    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 889   score: 2.0   memory length: 100000   epsilon: 0.8656391800029168    steps: 201    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 890   score: 1.0   memory length: 100000   epsilon: 0.8652986200029242    steps: 172    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 891   score: 2.0   memory length: 100000   epsilon: 0.8649026200029328    steps: 200    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 892   score: 1.0   memory length: 100000   epsilon: 0.8645660200029401    steps: 170    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 893   score: 1.0   memory length: 100000   epsilon: 0.8642650600029467    steps: 152    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 894   score: 2.0   memory length: 100000   epsilon: 0.8638730200029552    steps: 198    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 895   score: 3.0   memory length: 100000   epsilon: 0.8634235600029649    steps: 227    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 896   score: 5.0   memory length: 100000   epsilon: 0.8627464000029796    steps: 342    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 897   score: 2.0   memory length: 100000   epsilon: 0.8623523800029882    steps: 199    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 898   score: 2.0   memory length: 100000   epsilon: 0.8619583600029967    steps: 199    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 899   score: 3.0   memory length: 100000   epsilon: 0.8615108800030065    steps: 226    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 900   score: 2.0   memory length: 100000   epsilon: 0.861116860003015    steps: 199    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 901   score: 1.0   memory length: 100000   epsilon: 0.8607743200030225    steps: 173    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 902   score: 3.0   memory length: 100000   epsilon: 0.860289220003033    steps: 245    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 903   score: 4.0   memory length: 100000   epsilon: 0.8597764000030441    steps: 259    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 904   score: 4.0   memory length: 100000   epsilon: 0.8592992200030545    steps: 241    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 905   score: 2.0   memory length: 100000   epsilon: 0.858905200003063    steps: 199    lr: 0.0001     evaluation reward: 2.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 906   score: 1.0   memory length: 100000   epsilon: 0.8586022600030696    steps: 153    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 907   score: 1.0   memory length: 100000   epsilon: 0.8582656600030769    steps: 170    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 908   score: 4.0   memory length: 100000   epsilon: 0.8577132400030889    steps: 279    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 909   score: 2.0   memory length: 100000   epsilon: 0.8573528800030967    steps: 182    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 910   score: 3.0   memory length: 100000   epsilon: 0.8568618400031074    steps: 248    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 911   score: 2.0   memory length: 100000   epsilon: 0.8565034600031152    steps: 181    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 912   score: 3.0   memory length: 100000   epsilon: 0.8560183600031257    steps: 245    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 913   score: 7.0   memory length: 100000   epsilon: 0.855315460003141    steps: 355    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 914   score: 4.0   memory length: 100000   epsilon: 0.8548006600031521    steps: 260    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 915   score: 3.0   memory length: 100000   epsilon: 0.8543175400031626    steps: 244    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 916   score: 4.0   memory length: 100000   epsilon: 0.8537334400031753    steps: 295    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 917   score: 3.0   memory length: 100000   epsilon: 0.8532780400031852    steps: 230    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 918   score: 3.0   memory length: 100000   epsilon: 0.8527513600031966    steps: 266    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 919   score: 3.0   memory length: 100000   epsilon: 0.8522484400032075    steps: 254    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 920   score: 3.0   memory length: 100000   epsilon: 0.8517989800032173    steps: 227    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 921   score: 2.0   memory length: 100000   epsilon: 0.8514010000032259    steps: 201    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 922   score: 3.0   memory length: 100000   epsilon: 0.8509099600032366    steps: 248    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 923   score: 3.0   memory length: 100000   epsilon: 0.8504169400032473    steps: 249    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 924   score: 0.0   memory length: 100000   epsilon: 0.8501714200032526    steps: 124    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 925   score: 2.0   memory length: 100000   epsilon: 0.849739780003262    steps: 218    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 926   score: 1.0   memory length: 100000   epsilon: 0.8494012000032694    steps: 171    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 927   score: 3.0   memory length: 100000   epsilon: 0.8488705600032809    steps: 268    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 928   score: 3.0   memory length: 100000   epsilon: 0.8484191200032907    steps: 228    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 929   score: 5.0   memory length: 100000   epsilon: 0.847807300003304    steps: 309    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 930   score: 2.0   memory length: 100000   epsilon: 0.8474132800033125    steps: 199    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 931   score: 5.0   memory length: 100000   epsilon: 0.8468173000033254    steps: 301    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 932   score: 2.0   memory length: 100000   epsilon: 0.8463836800033349    steps: 219    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 933   score: 2.0   memory length: 100000   epsilon: 0.8459896600033434    steps: 199    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 934   score: 2.0   memory length: 100000   epsilon: 0.8455976200033519    steps: 198    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 935   score: 2.0   memory length: 100000   epsilon: 0.8451620200033614    steps: 220    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 936   score: 6.0   memory length: 100000   epsilon: 0.8444630800033766    steps: 353    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 937   score: 4.0   memory length: 100000   epsilon: 0.84384334000339    steps: 313    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 938   score: 4.0   memory length: 100000   epsilon: 0.8432612200034026    steps: 294    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 939   score: 3.0   memory length: 100000   epsilon: 0.8427701800034133    steps: 248    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 940   score: 0.0   memory length: 100000   epsilon: 0.8425266400034186    steps: 123    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 941   score: 7.0   memory length: 100000   epsilon: 0.8417188000034361    steps: 408    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 942   score: 3.0   memory length: 100000   epsilon: 0.8412990400034452    steps: 212    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 943   score: 3.0   memory length: 100000   epsilon: 0.8408436400034551    steps: 230    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 944   score: 3.0   memory length: 100000   epsilon: 0.8403922000034649    steps: 228    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 945   score: 1.0   memory length: 100000   epsilon: 0.8400556000034722    steps: 170    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 946   score: 2.0   memory length: 100000   epsilon: 0.8396200000034817    steps: 220    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 947   score: 3.0   memory length: 100000   epsilon: 0.8391646000034916    steps: 230    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 948   score: 4.0   memory length: 100000   epsilon: 0.8385785200035043    steps: 296    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 949   score: 4.0   memory length: 100000   epsilon: 0.8380716400035153    steps: 256    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 950   score: 2.0   memory length: 100000   epsilon: 0.8376776200035239    steps: 199    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 951   score: 1.0   memory length: 100000   epsilon: 0.8373410200035312    steps: 170    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 952   score: 3.0   memory length: 100000   epsilon: 0.8368757200035413    steps: 235    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 953   score: 0.0   memory length: 100000   epsilon: 0.8366321800035466    steps: 123    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 954   score: 7.0   memory length: 100000   epsilon: 0.8358401800035637    steps: 400    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 955   score: 0.0   memory length: 100000   epsilon: 0.8355946600035691    steps: 124    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 956   score: 2.0   memory length: 100000   epsilon: 0.8351966800035777    steps: 201    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 957   score: 2.0   memory length: 100000   epsilon: 0.8348006800035863    steps: 200    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 958   score: 1.0   memory length: 100000   epsilon: 0.8344621000035937    steps: 171    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 959   score: 2.0   memory length: 100000   epsilon: 0.8340601600036024    steps: 203    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 960   score: 3.0   memory length: 100000   epsilon: 0.8336067400036122    steps: 229    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 961   score: 4.0   memory length: 100000   epsilon: 0.833018680003625    steps: 297    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 962   score: 4.0   memory length: 100000   epsilon: 0.832468240003637    steps: 278    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 963   score: 7.0   memory length: 100000   epsilon: 0.8316227800036553    steps: 427    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 964   score: 1.0   memory length: 100000   epsilon: 0.8312861800036626    steps: 170    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 965   score: 2.0   memory length: 100000   epsilon: 0.8308505800036721    steps: 220    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 966   score: 9.0   memory length: 100000   epsilon: 0.8300962000036884    steps: 381    lr: 0.0001     evaluation reward: 2.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 967   score: 3.0   memory length: 100000   epsilon: 0.829611100003699    steps: 245    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 968   score: 0.0   memory length: 100000   epsilon: 0.8293655800037043    steps: 124    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 969   score: 6.0   memory length: 100000   epsilon: 0.8285933800037211    steps: 390    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 970   score: 2.0   memory length: 100000   epsilon: 0.8281538200037306    steps: 222    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 971   score: 2.0   memory length: 100000   epsilon: 0.8277598000037392    steps: 199    lr: 0.0001     evaluation reward: 2.73\n",
      "episode: 972   score: 0.0   memory length: 100000   epsilon: 0.8275142800037445    steps: 124    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 973   score: 1.0   memory length: 100000   epsilon: 0.827215300003751    steps: 151    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 974   score: 5.0   memory length: 100000   epsilon: 0.8266213000037639    steps: 300    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 975   score: 4.0   memory length: 100000   epsilon: 0.8260867000037755    steps: 270    lr: 0.0001     evaluation reward: 2.73\n",
      "episode: 976   score: 3.0   memory length: 100000   epsilon: 0.8256352600037853    steps: 228    lr: 0.0001     evaluation reward: 2.74\n",
      "episode: 977   score: 4.0   memory length: 100000   epsilon: 0.8251204600037965    steps: 260    lr: 0.0001     evaluation reward: 2.76\n",
      "episode: 978   score: 0.0   memory length: 100000   epsilon: 0.8248749400038018    steps: 124    lr: 0.0001     evaluation reward: 2.75\n",
      "episode: 979   score: 2.0   memory length: 100000   epsilon: 0.8244452800038111    steps: 217    lr: 0.0001     evaluation reward: 2.76\n",
      "episode: 980   score: 0.0   memory length: 100000   epsilon: 0.8241997600038165    steps: 124    lr: 0.0001     evaluation reward: 2.73\n",
      "episode: 981   score: 2.0   memory length: 100000   epsilon: 0.8238017800038251    steps: 201    lr: 0.0001     evaluation reward: 2.74\n",
      "episode: 982   score: 1.0   memory length: 100000   epsilon: 0.8235008200038316    steps: 152    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 983   score: 2.0   memory length: 100000   epsilon: 0.8231068000038402    steps: 199    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 984   score: 2.0   memory length: 100000   epsilon: 0.8226672400038497    steps: 222    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 985   score: 2.0   memory length: 100000   epsilon: 0.8222712400038583    steps: 200    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 986   score: 1.0   memory length: 100000   epsilon: 0.8219287000038658    steps: 173    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 987   score: 3.0   memory length: 100000   epsilon: 0.8214396400038764    steps: 247    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 988   score: 0.0   memory length: 100000   epsilon: 0.8211961000038817    steps: 123    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 989   score: 8.0   memory length: 100000   epsilon: 0.8203605400038998    steps: 422    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 990   score: 7.0   memory length: 100000   epsilon: 0.8195190400039181    steps: 425    lr: 0.0001     evaluation reward: 2.77\n",
      "episode: 991   score: 5.0   memory length: 100000   epsilon: 0.8188379200039329    steps: 344    lr: 0.0001     evaluation reward: 2.8\n",
      "episode: 992   score: 4.0   memory length: 100000   epsilon: 0.8182498600039456    steps: 297    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 993   score: 5.0   memory length: 100000   epsilon: 0.8176043800039596    steps: 326    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 994   score: 8.0   memory length: 100000   epsilon: 0.8167193200039788    steps: 447    lr: 0.0001     evaluation reward: 2.93\n",
      "episode: 995   score: 2.0   memory length: 100000   epsilon: 0.8163589600039867    steps: 182    lr: 0.0001     evaluation reward: 2.92\n",
      "episode: 996   score: 3.0   memory length: 100000   epsilon: 0.8158679200039973    steps: 248    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 997   score: 2.0   memory length: 100000   epsilon: 0.8155075600040051    steps: 182    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 998   score: 3.0   memory length: 100000   epsilon: 0.8150145400040159    steps: 249    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 999   score: 7.0   memory length: 100000   epsilon: 0.8142680800040321    steps: 377    lr: 0.0001     evaluation reward: 2.95\n",
      "episode: 1000   score: 1.0   memory length: 100000   epsilon: 0.8139314800040394    steps: 170    lr: 0.0001     evaluation reward: 2.94\n",
      "episode: 1001   score: 2.0   memory length: 100000   epsilon: 0.8135374600040479    steps: 199    lr: 0.0001     evaluation reward: 2.95\n",
      "episode: 1002   score: 4.0   memory length: 100000   epsilon: 0.8130226600040591    steps: 260    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 1003   score: 4.0   memory length: 100000   epsilon: 0.8125058800040703    steps: 261    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 1004   score: 6.0   memory length: 100000   epsilon: 0.811781200004086    steps: 366    lr: 0.0001     evaluation reward: 2.98\n",
      "episode: 1005   score: 9.0   memory length: 100000   epsilon: 0.8109456400041042    steps: 422    lr: 0.0001     evaluation reward: 3.05\n",
      "episode: 1006   score: 2.0   memory length: 100000   epsilon: 0.8105516200041127    steps: 199    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1007   score: 2.0   memory length: 100000   epsilon: 0.8101536400041214    steps: 201    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1008   score: 3.0   memory length: 100000   epsilon: 0.8096982400041313    steps: 230    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1009   score: 2.0   memory length: 100000   epsilon: 0.8092567000041408    steps: 223    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1010   score: 2.0   memory length: 100000   epsilon: 0.8088626800041494    steps: 199    lr: 0.0001     evaluation reward: 3.05\n",
      "episode: 1011   score: 5.0   memory length: 100000   epsilon: 0.8081716600041644    steps: 349    lr: 0.0001     evaluation reward: 3.08\n",
      "episode: 1012   score: 3.0   memory length: 100000   epsilon: 0.8076449800041758    steps: 266    lr: 0.0001     evaluation reward: 3.08\n",
      "episode: 1013   score: 4.0   memory length: 100000   epsilon: 0.8070886000041879    steps: 281    lr: 0.0001     evaluation reward: 3.05\n",
      "episode: 1014   score: 3.0   memory length: 100000   epsilon: 0.8066411200041976    steps: 226    lr: 0.0001     evaluation reward: 3.04\n",
      "episode: 1015   score: 1.0   memory length: 100000   epsilon: 0.8063401600042042    steps: 152    lr: 0.0001     evaluation reward: 3.02\n",
      "episode: 1016   score: 8.0   memory length: 100000   epsilon: 0.8054353000042238    steps: 457    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1017   score: 4.0   memory length: 100000   epsilon: 0.8048512000042365    steps: 295    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1018   score: 5.0   memory length: 100000   epsilon: 0.8042710600042491    steps: 293    lr: 0.0001     evaluation reward: 3.09\n",
      "episode: 1019   score: 4.0   memory length: 100000   epsilon: 0.803722600004261    steps: 277    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1020   score: 2.0   memory length: 100000   epsilon: 0.8033246200042696    steps: 201    lr: 0.0001     evaluation reward: 3.09\n",
      "episode: 1021   score: 8.0   memory length: 100000   epsilon: 0.8024653000042883    steps: 434    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1022   score: 5.0   memory length: 100000   epsilon: 0.8018178400043023    steps: 327    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1023   score: 5.0   memory length: 100000   epsilon: 0.801190180004316    steps: 317    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1024   score: 4.0   memory length: 100000   epsilon: 0.8006060800043286    steps: 295    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1025   score: 3.0   memory length: 100000   epsilon: 0.8001130600043393    steps: 249    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1026   score: 3.0   memory length: 100000   epsilon: 0.799578460004351    steps: 270    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1027   score: 9.0   memory length: 100000   epsilon: 0.7986201400043718    steps: 484    lr: 4e-05     evaluation reward: 3.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1028   score: 6.0   memory length: 100000   epsilon: 0.7979073400043872    steps: 360    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1029   score: 3.0   memory length: 100000   epsilon: 0.797459860004397    steps: 226    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1030   score: 6.0   memory length: 100000   epsilon: 0.7967530000044123    steps: 357    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1031   score: 3.0   memory length: 100000   epsilon: 0.7962302800044236    steps: 264    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1032   score: 3.0   memory length: 100000   epsilon: 0.7957412200044343    steps: 247    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1033   score: 6.0   memory length: 100000   epsilon: 0.7950007000044503    steps: 374    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1034   score: 3.0   memory length: 100000   epsilon: 0.7945136200044609    steps: 246    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1035   score: 7.0   memory length: 100000   epsilon: 0.7937335000044778    steps: 394    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1036   score: 3.0   memory length: 100000   epsilon: 0.7932701800044879    steps: 234    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1037   score: 5.0   memory length: 100000   epsilon: 0.7925890600045027    steps: 344    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1038   score: 4.0   memory length: 100000   epsilon: 0.7920722800045139    steps: 261    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1039   score: 2.0   memory length: 100000   epsilon: 0.7917099400045218    steps: 183    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1040   score: 1.0   memory length: 100000   epsilon: 0.791375320004529    steps: 169    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1041   score: 1.0   memory length: 100000   epsilon: 0.7910743600045356    steps: 152    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1042   score: 4.0   memory length: 100000   epsilon: 0.7906031200045458    steps: 238    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1043   score: 3.0   memory length: 100000   epsilon: 0.7901140600045564    steps: 247    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1044   score: 2.0   memory length: 100000   epsilon: 0.789674500004566    steps: 222    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1045   score: 4.0   memory length: 100000   epsilon: 0.789163660004577    steps: 258    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1046   score: 4.0   memory length: 100000   epsilon: 0.7886171800045889    steps: 276    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1047   score: 5.0   memory length: 100000   epsilon: 0.7879717000046029    steps: 326    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1048   score: 2.0   memory length: 100000   epsilon: 0.7876133200046107    steps: 181    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1049   score: 5.0   memory length: 100000   epsilon: 0.7869262600046256    steps: 347    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1050   score: 3.0   memory length: 100000   epsilon: 0.7864352200046363    steps: 248    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1051   score: 3.0   memory length: 100000   epsilon: 0.7859778400046462    steps: 231    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1052   score: 2.0   memory length: 100000   epsilon: 0.7855481800046555    steps: 217    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1053   score: 6.0   memory length: 100000   epsilon: 0.7848749800046702    steps: 340    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1054   score: 3.0   memory length: 100000   epsilon: 0.7843799800046809    steps: 250    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1055   score: 3.0   memory length: 100000   epsilon: 0.7839186400046909    steps: 233    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1056   score: 5.0   memory length: 100000   epsilon: 0.7832315800047058    steps: 347    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1057   score: 4.0   memory length: 100000   epsilon: 0.7826059000047194    steps: 316    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1058   score: 1.0   memory length: 100000   epsilon: 0.7822673200047268    steps: 171    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1059   score: 5.0   memory length: 100000   epsilon: 0.7816198600047408    steps: 327    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1060   score: 3.0   memory length: 100000   epsilon: 0.7811605000047508    steps: 232    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1061   score: 3.0   memory length: 100000   epsilon: 0.7806694600047615    steps: 248    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1062   score: 1.0   memory length: 100000   epsilon: 0.7803704800047679    steps: 151    lr: 4e-05     evaluation reward: 3.55\n",
      "episode: 1063   score: 3.0   memory length: 100000   epsilon: 0.779950720004777    steps: 212    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1064   score: 3.0   memory length: 100000   epsilon: 0.7794973000047869    steps: 229    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1065   score: 3.0   memory length: 100000   epsilon: 0.7790003200047977    steps: 251    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1066   score: 3.0   memory length: 100000   epsilon: 0.7784677000048092    steps: 269    lr: 4e-05     evaluation reward: 3.48\n",
      "episode: 1067   score: 4.0   memory length: 100000   epsilon: 0.7779509200048205    steps: 261    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1068   score: 4.0   memory length: 100000   epsilon: 0.7774044400048323    steps: 276    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1069   score: 4.0   memory length: 100000   epsilon: 0.7768520200048443    steps: 279    lr: 4e-05     evaluation reward: 3.51\n",
      "episode: 1070   score: 4.0   memory length: 100000   epsilon: 0.7763015800048563    steps: 278    lr: 4e-05     evaluation reward: 3.53\n",
      "episode: 1071   score: 3.0   memory length: 100000   epsilon: 0.7758145000048668    steps: 246    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1072   score: 0.0   memory length: 100000   epsilon: 0.7755689800048722    steps: 124    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1073   score: 5.0   memory length: 100000   epsilon: 0.7748819200048871    steps: 347    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1074   score: 7.0   memory length: 100000   epsilon: 0.7741315000049034    steps: 379    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1075   score: 7.0   memory length: 100000   epsilon: 0.7734385000049184    steps: 350    lr: 4e-05     evaluation reward: 3.63\n",
      "episode: 1076   score: 2.0   memory length: 100000   epsilon: 0.7730761600049263    steps: 183    lr: 4e-05     evaluation reward: 3.62\n",
      "episode: 1077   score: 2.0   memory length: 100000   epsilon: 0.7726821400049348    steps: 199    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1078   score: 1.0   memory length: 100000   epsilon: 0.7723831600049413    steps: 151    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1079   score: 4.0   memory length: 100000   epsilon: 0.7718366800049532    steps: 276    lr: 4e-05     evaluation reward: 3.63\n",
      "episode: 1080   score: 4.0   memory length: 100000   epsilon: 0.771248620004966    steps: 297    lr: 4e-05     evaluation reward: 3.67\n",
      "episode: 1081   score: 2.0   memory length: 100000   epsilon: 0.7708466800049747    steps: 203    lr: 4e-05     evaluation reward: 3.67\n",
      "episode: 1082   score: 5.0   memory length: 100000   epsilon: 0.7702091200049885    steps: 322    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1083   score: 2.0   memory length: 100000   epsilon: 0.7698527200049963    steps: 180    lr: 4e-05     evaluation reward: 3.71\n",
      "episode: 1084   score: 4.0   memory length: 100000   epsilon: 0.7692270400050099    steps: 316    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1085   score: 4.0   memory length: 100000   epsilon: 0.7687102600050211    steps: 261    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1086   score: 6.0   memory length: 100000   epsilon: 0.7680370600050357    steps: 340    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1087   score: 2.0   memory length: 100000   epsilon: 0.7676331400050445    steps: 204    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1088   score: 5.0   memory length: 100000   epsilon: 0.7669837000050586    steps: 328    lr: 4e-05     evaluation reward: 3.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1089   score: 3.0   memory length: 100000   epsilon: 0.7665322600050684    steps: 228    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1090   score: 2.0   memory length: 100000   epsilon: 0.7661382400050769    steps: 199    lr: 4e-05     evaluation reward: 3.74\n",
      "episode: 1091   score: 6.0   memory length: 100000   epsilon: 0.7654492000050919    steps: 348    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1092   score: 7.0   memory length: 100000   epsilon: 0.7646433400051094    steps: 407    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1093   score: 8.0   memory length: 100000   epsilon: 0.7638058000051275    steps: 423    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1094   score: 4.0   memory length: 100000   epsilon: 0.7632236800051402    steps: 294    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1095   score: 4.0   memory length: 100000   epsilon: 0.7626712600051522    steps: 279    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1096   score: 5.0   memory length: 100000   epsilon: 0.762035680005166    steps: 321    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1097   score: 5.0   memory length: 100000   epsilon: 0.7614595000051785    steps: 291    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1098   score: 6.0   memory length: 100000   epsilon: 0.7607249200051944    steps: 371    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1099   score: 4.0   memory length: 100000   epsilon: 0.7600933000052081    steps: 319    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1100   score: 1.0   memory length: 100000   epsilon: 0.7597567000052154    steps: 170    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1101   score: 3.0   memory length: 100000   epsilon: 0.7592656600052261    steps: 248    lr: 4e-05     evaluation reward: 3.85\n",
      "episode: 1102   score: 5.0   memory length: 100000   epsilon: 0.75862612000524    steps: 323    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1103   score: 6.0   memory length: 100000   epsilon: 0.7578737200052563    steps: 380    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1104   score: 2.0   memory length: 100000   epsilon: 0.7574797000052649    steps: 199    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1105   score: 2.0   memory length: 100000   epsilon: 0.7570777600052736    steps: 203    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1106   score: 5.0   memory length: 100000   epsilon: 0.7564243600052878    steps: 330    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1107   score: 3.0   memory length: 100000   epsilon: 0.7559333200052984    steps: 248    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1108   score: 3.0   memory length: 100000   epsilon: 0.7554858400053082    steps: 226    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1109   score: 5.0   memory length: 100000   epsilon: 0.7548720400053215    steps: 310    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1110   score: 4.0   memory length: 100000   epsilon: 0.7543097200053337    steps: 284    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1111   score: 4.0   memory length: 100000   epsilon: 0.7537632400053456    steps: 276    lr: 4e-05     evaluation reward: 3.85\n",
      "episode: 1112   score: 5.0   memory length: 100000   epsilon: 0.7531078600053598    steps: 331    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1113   score: 1.0   memory length: 100000   epsilon: 0.7527712600053671    steps: 170    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1114   score: 3.0   memory length: 100000   epsilon: 0.7523554600053761    steps: 210    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1115   score: 3.0   memory length: 100000   epsilon: 0.7519376800053852    steps: 211    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1116   score: 5.0   memory length: 100000   epsilon: 0.7512823000053994    steps: 331    lr: 4e-05     evaluation reward: 3.83\n",
      "episode: 1117   score: 10.0   memory length: 100000   epsilon: 0.7502923000054209    steps: 500    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1118   score: 2.0   memory length: 100000   epsilon: 0.7498586800054303    steps: 219    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1119   score: 4.0   memory length: 100000   epsilon: 0.7493122000054422    steps: 276    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1120   score: 5.0   memory length: 100000   epsilon: 0.7485875200054579    steps: 366    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1121   score: 4.0   memory length: 100000   epsilon: 0.7480430200054697    steps: 275    lr: 4e-05     evaluation reward: 3.85\n",
      "episode: 1122   score: 0.0   memory length: 100000   epsilon: 0.7477975000054751    steps: 124    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1123   score: 2.0   memory length: 100000   epsilon: 0.7474034800054836    steps: 199    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1124   score: 3.0   memory length: 100000   epsilon: 0.7469520400054934    steps: 228    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1125   score: 2.0   memory length: 100000   epsilon: 0.7465184200055028    steps: 219    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1126   score: 4.0   memory length: 100000   epsilon: 0.7459679800055148    steps: 278    lr: 4e-05     evaluation reward: 3.76\n",
      "episode: 1127   score: 3.0   memory length: 100000   epsilon: 0.7455185200055245    steps: 227    lr: 4e-05     evaluation reward: 3.7\n",
      "episode: 1128   score: 8.0   memory length: 100000   epsilon: 0.7446116800055442    steps: 458    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1129   score: 3.0   memory length: 100000   epsilon: 0.7441523200055542    steps: 232    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1130   score: 4.0   memory length: 100000   epsilon: 0.7436355400055654    steps: 261    lr: 4e-05     evaluation reward: 3.7\n",
      "episode: 1131   score: 3.0   memory length: 100000   epsilon: 0.7431385600055762    steps: 251    lr: 4e-05     evaluation reward: 3.7\n",
      "episode: 1132   score: 2.0   memory length: 100000   epsilon: 0.7427029600055857    steps: 220    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1133   score: 6.0   memory length: 100000   epsilon: 0.7419367000056023    steps: 387    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1134   score: 2.0   memory length: 100000   epsilon: 0.7414971400056118    steps: 222    lr: 4e-05     evaluation reward: 3.68\n",
      "episode: 1135   score: 6.0   memory length: 100000   epsilon: 0.7408259200056264    steps: 339    lr: 4e-05     evaluation reward: 3.67\n",
      "episode: 1136   score: 5.0   memory length: 100000   epsilon: 0.7401408400056413    steps: 346    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1137   score: 2.0   memory length: 100000   epsilon: 0.7397468200056498    steps: 199    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1138   score: 3.0   memory length: 100000   epsilon: 0.7392934000056597    steps: 229    lr: 4e-05     evaluation reward: 3.65\n",
      "episode: 1139   score: 3.0   memory length: 100000   epsilon: 0.7388419600056695    steps: 228    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1140   score: 5.0   memory length: 100000   epsilon: 0.7381925200056836    steps: 328    lr: 4e-05     evaluation reward: 3.7\n",
      "episode: 1141   score: 4.0   memory length: 100000   epsilon: 0.7376856400056946    steps: 256    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1142   score: 6.0   memory length: 100000   epsilon: 0.7369807600057099    steps: 356    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1143   score: 3.0   memory length: 100000   epsilon: 0.7364877400057206    steps: 249    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1144   score: 5.0   memory length: 100000   epsilon: 0.735871960005734    steps: 311    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1145   score: 4.0   memory length: 100000   epsilon: 0.7353254800057458    steps: 276    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1146   score: 7.0   memory length: 100000   epsilon: 0.7345532800057626    steps: 390    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1147   score: 7.0   memory length: 100000   epsilon: 0.7337692000057796    steps: 396    lr: 4e-05     evaluation reward: 3.83\n",
      "episode: 1148   score: 3.0   memory length: 100000   epsilon: 0.7333435000057889    steps: 215    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1149   score: 3.0   memory length: 100000   epsilon: 0.7329296800057978    steps: 209    lr: 4e-05     evaluation reward: 3.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1150   score: 5.0   memory length: 100000   epsilon: 0.7322743000058121    steps: 331    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1151   score: 2.0   memory length: 100000   epsilon: 0.7318763200058207    steps: 201    lr: 4e-05     evaluation reward: 3.83\n",
      "episode: 1152   score: 3.0   memory length: 100000   epsilon: 0.7314229000058305    steps: 229    lr: 4e-05     evaluation reward: 3.84\n",
      "episode: 1153   score: 2.0   memory length: 100000   epsilon: 0.7310288800058391    steps: 199    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1154   score: 2.0   memory length: 100000   epsilon: 0.7306348600058477    steps: 199    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1155   score: 6.0   memory length: 100000   epsilon: 0.7299616600058623    steps: 340    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1156   score: 2.0   memory length: 100000   epsilon: 0.7295260600058717    steps: 220    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1157   score: 10.0   memory length: 100000   epsilon: 0.7284944800058941    steps: 521    lr: 4e-05     evaluation reward: 3.85\n",
      "episode: 1158   score: 4.0   memory length: 100000   epsilon: 0.7279440400059061    steps: 278    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1159   score: 3.0   memory length: 100000   epsilon: 0.7274530000059167    steps: 248    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1160   score: 3.0   memory length: 100000   epsilon: 0.7270332400059258    steps: 212    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1161   score: 5.0   memory length: 100000   epsilon: 0.7264531000059384    steps: 293    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1162   score: 5.0   memory length: 100000   epsilon: 0.7257660400059534    steps: 347    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1163   score: 4.0   memory length: 100000   epsilon: 0.7251740200059662    steps: 299    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1164   score: 6.0   memory length: 100000   epsilon: 0.7244335000059823    steps: 374    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1165   score: 5.0   memory length: 100000   epsilon: 0.7238593000059947    steps: 290    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1166   score: 3.0   memory length: 100000   epsilon: 0.7234078600060045    steps: 228    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1167   score: 6.0   memory length: 100000   epsilon: 0.7226614000060207    steps: 377    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1168   score: 2.0   memory length: 100000   epsilon: 0.7222990600060286    steps: 183    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1169   score: 4.0   memory length: 100000   epsilon: 0.7217486200060406    steps: 278    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1170   score: 0.0   memory length: 100000   epsilon: 0.7215031000060459    steps: 124    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1171   score: 3.0   memory length: 100000   epsilon: 0.7210536400060557    steps: 227    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1172   score: 2.0   memory length: 100000   epsilon: 0.7206616000060642    steps: 198    lr: 4e-05     evaluation reward: 3.96\n",
      "episode: 1173   score: 3.0   memory length: 100000   epsilon: 0.7202121400060739    steps: 227    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1174   score: 6.0   memory length: 100000   epsilon: 0.7195389400060885    steps: 340    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1175   score: 7.0   memory length: 100000   epsilon: 0.7187370400061059    steps: 405    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1176   score: 3.0   memory length: 100000   epsilon: 0.7182836200061158    steps: 229    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1177   score: 3.0   memory length: 100000   epsilon: 0.717859900006125    steps: 214    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1178   score: 1.0   memory length: 100000   epsilon: 0.7175609200061315    steps: 151    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1179   score: 3.0   memory length: 100000   epsilon: 0.7171411600061406    steps: 212    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1180   score: 5.0   memory length: 100000   epsilon: 0.7165273600061539    steps: 310    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1181   score: 2.0   memory length: 100000   epsilon: 0.7160858200061635    steps: 223    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1182   score: 7.0   memory length: 100000   epsilon: 0.7153453000061796    steps: 374    lr: 4e-05     evaluation reward: 3.97\n",
      "episode: 1183   score: 8.0   memory length: 100000   epsilon: 0.7144562800061989    steps: 449    lr: 4e-05     evaluation reward: 4.03\n",
      "episode: 1184   score: 4.0   memory length: 100000   epsilon: 0.713899900006211    steps: 281    lr: 4e-05     evaluation reward: 4.03\n",
      "episode: 1185   score: 6.0   memory length: 100000   epsilon: 0.7131712600062268    steps: 368    lr: 4e-05     evaluation reward: 4.05\n",
      "episode: 1186   score: 3.0   memory length: 100000   epsilon: 0.7127138800062367    steps: 231    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1187   score: 3.0   memory length: 100000   epsilon: 0.7122545200062467    steps: 232    lr: 4e-05     evaluation reward: 4.03\n",
      "episode: 1188   score: 3.0   memory length: 100000   epsilon: 0.7118050600062564    steps: 227    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1189   score: 0.0   memory length: 100000   epsilon: 0.7115595400062618    steps: 124    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1190   score: 3.0   memory length: 100000   epsilon: 0.7111061200062716    steps: 229    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1191   score: 6.0   memory length: 100000   epsilon: 0.7104349000062862    steps: 339    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1192   score: 7.0   memory length: 100000   epsilon: 0.7096369600063035    steps: 403    lr: 4e-05     evaluation reward: 3.99\n",
      "episode: 1193   score: 1.0   memory length: 100000   epsilon: 0.70933600000631    steps: 152    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1194   score: 4.0   memory length: 100000   epsilon: 0.708783580006322    steps: 279    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1195   score: 4.0   memory length: 100000   epsilon: 0.7082707600063332    steps: 259    lr: 4e-05     evaluation reward: 3.92\n",
      "episode: 1196   score: 4.0   memory length: 100000   epsilon: 0.7076846800063459    steps: 296    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1197   score: 3.0   memory length: 100000   epsilon: 0.7072332400063557    steps: 228    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1198   score: 3.0   memory length: 100000   epsilon: 0.7067719000063657    steps: 233    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1199   score: 6.0   memory length: 100000   epsilon: 0.7060630600063811    steps: 358    lr: 4e-05     evaluation reward: 3.88\n",
      "episode: 1200   score: 3.0   memory length: 100000   epsilon: 0.7055423200063924    steps: 263    lr: 4e-05     evaluation reward: 3.9\n",
      "episode: 1201   score: 6.0   memory length: 100000   epsilon: 0.7047602200064094    steps: 395    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1202   score: 3.0   memory length: 100000   epsilon: 0.7042612600064202    steps: 252    lr: 4e-05     evaluation reward: 3.91\n",
      "episode: 1203   score: 2.0   memory length: 100000   epsilon: 0.7038672400064288    steps: 199    lr: 4e-05     evaluation reward: 3.87\n",
      "episode: 1204   score: 4.0   memory length: 100000   epsilon: 0.7033247200064405    steps: 274    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1205   score: 6.0   memory length: 100000   epsilon: 0.7026376600064554    steps: 347    lr: 4e-05     evaluation reward: 3.93\n",
      "episode: 1206   score: 6.0   memory length: 100000   epsilon: 0.7018931800064716    steps: 376    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1207   score: 3.0   memory length: 100000   epsilon: 0.7014437200064814    steps: 227    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1208   score: 7.0   memory length: 100000   epsilon: 0.7006002400064997    steps: 426    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1209   score: 5.0   memory length: 100000   epsilon: 0.6999468400065139    steps: 330    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1210   score: 15.0   memory length: 100000   epsilon: 0.6987845800065391    steps: 587    lr: 4e-05     evaluation reward: 4.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1211   score: 8.0   memory length: 100000   epsilon: 0.6978539800065593    steps: 470    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1212   score: 4.0   memory length: 100000   epsilon: 0.6972877000065716    steps: 286    lr: 4e-05     evaluation reward: 4.12\n",
      "episode: 1213   score: 6.0   memory length: 100000   epsilon: 0.6966164800065862    steps: 339    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1214   score: 4.0   memory length: 100000   epsilon: 0.6960284200065989    steps: 297    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1215   score: 2.0   memory length: 100000   epsilon: 0.6956700400066067    steps: 181    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1216   score: 6.0   memory length: 100000   epsilon: 0.6949592200066221    steps: 359    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1217   score: 3.0   memory length: 100000   epsilon: 0.6944959000066322    steps: 234    lr: 4e-05     evaluation reward: 4.11\n",
      "episode: 1218   score: 4.0   memory length: 100000   epsilon: 0.693905860006645    steps: 298    lr: 4e-05     evaluation reward: 4.13\n",
      "episode: 1219   score: 2.0   memory length: 100000   epsilon: 0.6934722400066544    steps: 219    lr: 4e-05     evaluation reward: 4.11\n",
      "episode: 1220   score: 9.0   memory length: 100000   epsilon: 0.6924862000066758    steps: 498    lr: 4e-05     evaluation reward: 4.15\n",
      "episode: 1221   score: 9.0   memory length: 100000   epsilon: 0.6915655000066958    steps: 465    lr: 4e-05     evaluation reward: 4.2\n",
      "episode: 1222   score: 2.0   memory length: 100000   epsilon: 0.6911734600067043    steps: 198    lr: 4e-05     evaluation reward: 4.22\n",
      "episode: 1223   score: 7.0   memory length: 100000   epsilon: 0.6903893800067213    steps: 396    lr: 4e-05     evaluation reward: 4.27\n",
      "episode: 1224   score: 6.0   memory length: 100000   epsilon: 0.6896825200067367    steps: 357    lr: 4e-05     evaluation reward: 4.3\n",
      "episode: 1225   score: 3.0   memory length: 100000   epsilon: 0.6891538600067482    steps: 267    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1226   score: 4.0   memory length: 100000   epsilon: 0.6886430200067593    steps: 258    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1227   score: 11.0   memory length: 100000   epsilon: 0.6877837000067779    steps: 434    lr: 4e-05     evaluation reward: 4.39\n",
      "episode: 1228   score: 1.0   memory length: 100000   epsilon: 0.6874827400067844    steps: 152    lr: 4e-05     evaluation reward: 4.32\n",
      "episode: 1229   score: 5.0   memory length: 100000   epsilon: 0.6868313200067986    steps: 329    lr: 4e-05     evaluation reward: 4.34\n",
      "episode: 1230   score: 7.0   memory length: 100000   epsilon: 0.6860096200068164    steps: 415    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1231   score: 4.0   memory length: 100000   epsilon: 0.6854611600068283    steps: 277    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1232   score: 7.0   memory length: 100000   epsilon: 0.684690940006845    steps: 389    lr: 4e-05     evaluation reward: 4.43\n",
      "episode: 1233   score: 3.0   memory length: 100000   epsilon: 0.6841979200068558    steps: 249    lr: 4e-05     evaluation reward: 4.4\n",
      "episode: 1234   score: 6.0   memory length: 100000   epsilon: 0.6834890800068711    steps: 358    lr: 4e-05     evaluation reward: 4.44\n",
      "episode: 1235   score: 2.0   memory length: 100000   epsilon: 0.6830950600068797    steps: 199    lr: 4e-05     evaluation reward: 4.4\n",
      "episode: 1236   score: 3.0   memory length: 100000   epsilon: 0.6826099600068902    steps: 245    lr: 4e-05     evaluation reward: 4.38\n",
      "episode: 1237   score: 8.0   memory length: 100000   epsilon: 0.6816932200069101    steps: 463    lr: 4e-05     evaluation reward: 4.44\n",
      "episode: 1238   score: 9.0   memory length: 100000   epsilon: 0.680867560006928    steps: 417    lr: 4e-05     evaluation reward: 4.5\n",
      "episode: 1239   score: 2.0   memory length: 100000   epsilon: 0.6804339400069375    steps: 219    lr: 4e-05     evaluation reward: 4.49\n",
      "episode: 1240   score: 8.0   memory length: 100000   epsilon: 0.6795211600069573    steps: 461    lr: 4e-05     evaluation reward: 4.52\n",
      "episode: 1241   score: 9.0   memory length: 100000   epsilon: 0.6786519400069762    steps: 439    lr: 4e-05     evaluation reward: 4.57\n",
      "episode: 1242   score: 8.0   memory length: 100000   epsilon: 0.6780658600069889    steps: 296    lr: 4e-05     evaluation reward: 4.59\n",
      "episode: 1243   score: 8.0   memory length: 100000   epsilon: 0.6771669400070084    steps: 454    lr: 4e-05     evaluation reward: 4.64\n",
      "episode: 1244   score: 4.0   memory length: 100000   epsilon: 0.6765788800070212    steps: 297    lr: 4e-05     evaluation reward: 4.63\n",
      "episode: 1245   score: 6.0   memory length: 100000   epsilon: 0.6759254800070353    steps: 330    lr: 4e-05     evaluation reward: 4.65\n",
      "episode: 1246   score: 4.0   memory length: 100000   epsilon: 0.6753730600070473    steps: 279    lr: 4e-05     evaluation reward: 4.62\n",
      "episode: 1247   score: 4.0   memory length: 100000   epsilon: 0.6748325200070591    steps: 273    lr: 4e-05     evaluation reward: 4.59\n",
      "episode: 1248   score: 8.0   memory length: 100000   epsilon: 0.674008840007077    steps: 416    lr: 4e-05     evaluation reward: 4.64\n",
      "episode: 1249   score: 3.0   memory length: 100000   epsilon: 0.6735890800070861    steps: 212    lr: 4e-05     evaluation reward: 4.64\n",
      "episode: 1250   score: 6.0   memory length: 100000   epsilon: 0.6729198400071006    steps: 338    lr: 4e-05     evaluation reward: 4.65\n",
      "episode: 1251   score: 4.0   memory length: 100000   epsilon: 0.6723694000071125    steps: 278    lr: 4e-05     evaluation reward: 4.67\n",
      "episode: 1252   score: 1.0   memory length: 100000   epsilon: 0.6720684400071191    steps: 152    lr: 4e-05     evaluation reward: 4.65\n",
      "episode: 1253   score: 2.0   memory length: 100000   epsilon: 0.6716348200071285    steps: 219    lr: 4e-05     evaluation reward: 4.65\n",
      "episode: 1254   score: 7.0   memory length: 100000   epsilon: 0.6708566800071454    steps: 393    lr: 4e-05     evaluation reward: 4.7\n",
      "episode: 1255   score: 4.0   memory length: 100000   epsilon: 0.6703102000071572    steps: 276    lr: 4e-05     evaluation reward: 4.68\n",
      "episode: 1256   score: 2.0   memory length: 100000   epsilon: 0.669953800007165    steps: 180    lr: 4e-05     evaluation reward: 4.68\n",
      "episode: 1257   score: 2.0   memory length: 100000   epsilon: 0.6695597800071735    steps: 199    lr: 4e-05     evaluation reward: 4.6\n",
      "episode: 1258   score: 3.0   memory length: 100000   epsilon: 0.6690667600071842    steps: 249    lr: 4e-05     evaluation reward: 4.59\n",
      "episode: 1259   score: 7.0   memory length: 100000   epsilon: 0.6682727800072015    steps: 401    lr: 4e-05     evaluation reward: 4.63\n",
      "episode: 1260   score: 9.0   memory length: 100000   epsilon: 0.6673619800072212    steps: 460    lr: 4e-05     evaluation reward: 4.69\n",
      "episode: 1261   score: 5.0   memory length: 100000   epsilon: 0.6667402600072347    steps: 314    lr: 4e-05     evaluation reward: 4.69\n",
      "episode: 1262   score: 3.0   memory length: 100000   epsilon: 0.6662076400072463    steps: 269    lr: 4e-05     evaluation reward: 4.67\n",
      "episode: 1263   score: 5.0   memory length: 100000   epsilon: 0.6655958200072596    steps: 309    lr: 4e-05     evaluation reward: 4.68\n",
      "episode: 1264   score: 0.0   memory length: 100000   epsilon: 0.6653503000072649    steps: 124    lr: 4e-05     evaluation reward: 4.62\n",
      "episode: 1265   score: 3.0   memory length: 100000   epsilon: 0.6648592600072756    steps: 248    lr: 4e-05     evaluation reward: 4.6\n",
      "episode: 1266   score: 7.0   memory length: 100000   epsilon: 0.6641128000072918    steps: 377    lr: 4e-05     evaluation reward: 4.64\n",
      "episode: 1267   score: 1.0   memory length: 100000   epsilon: 0.6638138200072983    steps: 151    lr: 4e-05     evaluation reward: 4.59\n",
      "episode: 1268   score: 3.0   memory length: 100000   epsilon: 0.6633247600073089    steps: 247    lr: 4e-05     evaluation reward: 4.6\n",
      "episode: 1269   score: 5.0   memory length: 100000   epsilon: 0.6626495800073235    steps: 341    lr: 4e-05     evaluation reward: 4.61\n",
      "episode: 1270   score: 1.0   memory length: 100000   epsilon: 0.6623446600073302    steps: 154    lr: 4e-05     evaluation reward: 4.62\n",
      "episode: 1271   score: 4.0   memory length: 100000   epsilon: 0.6618219400073415    steps: 264    lr: 4e-05     evaluation reward: 4.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1272   score: 8.0   memory length: 100000   epsilon: 0.6609091600073613    steps: 461    lr: 4e-05     evaluation reward: 4.69\n",
      "episode: 1273   score: 4.0   memory length: 100000   epsilon: 0.6603607000073732    steps: 277    lr: 4e-05     evaluation reward: 4.7\n",
      "episode: 1274   score: 3.0   memory length: 100000   epsilon: 0.659911240007383    steps: 227    lr: 4e-05     evaluation reward: 4.67\n",
      "episode: 1275   score: 5.0   memory length: 100000   epsilon: 0.6593073400073961    steps: 305    lr: 4e-05     evaluation reward: 4.65\n",
      "episode: 1276   score: 5.0   memory length: 100000   epsilon: 0.6585846400074118    steps: 365    lr: 4e-05     evaluation reward: 4.67\n",
      "episode: 1277   score: 6.0   memory length: 100000   epsilon: 0.657883720007427    steps: 354    lr: 4e-05     evaluation reward: 4.7\n",
      "episode: 1278   score: 9.0   memory length: 100000   epsilon: 0.6569036200074483    steps: 495    lr: 4e-05     evaluation reward: 4.78\n",
      "episode: 1279   score: 7.0   memory length: 100000   epsilon: 0.6560997400074657    steps: 406    lr: 4e-05     evaluation reward: 4.82\n",
      "episode: 1280   score: 3.0   memory length: 100000   epsilon: 0.6556087000074764    steps: 248    lr: 4e-05     evaluation reward: 4.8\n",
      "episode: 1281   score: 2.0   memory length: 100000   epsilon: 0.6552503200074842    steps: 181    lr: 4e-05     evaluation reward: 4.8\n",
      "episode: 1282   score: 5.0   memory length: 100000   epsilon: 0.6546028600074982    steps: 327    lr: 4e-05     evaluation reward: 4.78\n",
      "episode: 1283   score: 6.0   memory length: 100000   epsilon: 0.6539296600075128    steps: 340    lr: 4e-05     evaluation reward: 4.76\n",
      "episode: 1284   score: 4.0   memory length: 100000   epsilon: 0.6534524800075232    steps: 241    lr: 4e-05     evaluation reward: 4.76\n",
      "episode: 1285   score: 6.0   memory length: 100000   epsilon: 0.6526703800075402    steps: 395    lr: 4e-05     evaluation reward: 4.76\n",
      "episode: 1286   score: 7.0   memory length: 100000   epsilon: 0.6518863000075572    steps: 396    lr: 4e-05     evaluation reward: 4.8\n",
      "episode: 1287   score: 6.0   memory length: 100000   epsilon: 0.651252700007571    steps: 320    lr: 4e-05     evaluation reward: 4.83\n",
      "episode: 1288   score: 10.0   memory length: 100000   epsilon: 0.6503042800075916    steps: 479    lr: 4e-05     evaluation reward: 4.9\n",
      "episode: 1289   score: 9.0   memory length: 100000   epsilon: 0.6493578400076121    steps: 478    lr: 4e-05     evaluation reward: 4.99\n",
      "episode: 1290   score: 4.0   memory length: 100000   epsilon: 0.648811360007624    steps: 276    lr: 4e-05     evaluation reward: 5.0\n",
      "episode: 1291   score: 2.0   memory length: 100000   epsilon: 0.6484173400076325    steps: 199    lr: 4e-05     evaluation reward: 4.96\n",
      "episode: 1292   score: 5.0   memory length: 100000   epsilon: 0.6477718600076465    steps: 326    lr: 4e-05     evaluation reward: 4.94\n",
      "episode: 1293   score: 7.0   memory length: 100000   epsilon: 0.647013520007663    steps: 383    lr: 4e-05     evaluation reward: 5.0\n",
      "episode: 1294   score: 2.0   memory length: 100000   epsilon: 0.6466195000076715    steps: 199    lr: 4e-05     evaluation reward: 4.98\n",
      "episode: 1295   score: 5.0   memory length: 100000   epsilon: 0.6459700600076856    steps: 328    lr: 4e-05     evaluation reward: 4.99\n",
      "episode: 1296   score: 5.0   memory length: 100000   epsilon: 0.6452849800077005    steps: 346    lr: 4e-05     evaluation reward: 5.0\n",
      "episode: 1297   score: 5.0   memory length: 100000   epsilon: 0.6447127600077129    steps: 289    lr: 4e-05     evaluation reward: 5.02\n",
      "episode: 1298   score: 6.0   memory length: 100000   epsilon: 0.6440593600077271    steps: 330    lr: 4e-05     evaluation reward: 5.05\n",
      "episode: 1299   score: 5.0   memory length: 100000   epsilon: 0.6433861600077417    steps: 340    lr: 4e-05     evaluation reward: 5.04\n",
      "episode: 1300   score: 6.0   memory length: 100000   epsilon: 0.6426555400077576    steps: 369    lr: 4e-05     evaluation reward: 5.07\n",
      "episode: 1301   score: 7.0   memory length: 100000   epsilon: 0.6418675000077747    steps: 398    lr: 4e-05     evaluation reward: 5.08\n",
      "episode: 1302   score: 3.0   memory length: 100000   epsilon: 0.6413784400077853    steps: 247    lr: 4e-05     evaluation reward: 5.08\n",
      "episode: 1303   score: 5.0   memory length: 100000   epsilon: 0.6407290000077994    steps: 328    lr: 4e-05     evaluation reward: 5.11\n",
      "episode: 1304   score: 1.0   memory length: 100000   epsilon: 0.640428040007806    steps: 152    lr: 4e-05     evaluation reward: 5.08\n",
      "episode: 1305   score: 5.0   memory length: 100000   epsilon: 0.6397865200078199    steps: 324    lr: 4e-05     evaluation reward: 5.07\n",
      "episode: 1306   score: 2.0   memory length: 100000   epsilon: 0.6393529000078293    steps: 219    lr: 4e-05     evaluation reward: 5.03\n",
      "episode: 1307   score: 3.0   memory length: 100000   epsilon: 0.6388658200078399    steps: 246    lr: 4e-05     evaluation reward: 5.03\n",
      "episode: 1308   score: 5.0   memory length: 100000   epsilon: 0.6382579600078531    steps: 307    lr: 4e-05     evaluation reward: 5.01\n",
      "episode: 1309   score: 5.0   memory length: 100000   epsilon: 0.6376481200078663    steps: 308    lr: 4e-05     evaluation reward: 5.01\n",
      "episode: 1310   score: 4.0   memory length: 100000   epsilon: 0.63701848000788    steps: 318    lr: 4e-05     evaluation reward: 4.9\n",
      "episode: 1311   score: 7.0   memory length: 100000   epsilon: 0.6362304400078971    steps: 398    lr: 4e-05     evaluation reward: 4.89\n",
      "episode: 1312   score: 5.0   memory length: 100000   epsilon: 0.635586940007911    steps: 325    lr: 4e-05     evaluation reward: 4.9\n",
      "episode: 1313   score: 2.0   memory length: 100000   epsilon: 0.6351949000079196    steps: 198    lr: 4e-05     evaluation reward: 4.86\n",
      "episode: 1314   score: 6.0   memory length: 100000   epsilon: 0.6344464600079358    steps: 378    lr: 4e-05     evaluation reward: 4.88\n",
      "episode: 1315   score: 6.0   memory length: 100000   epsilon: 0.6337376200079512    steps: 358    lr: 4e-05     evaluation reward: 4.92\n",
      "episode: 1316   score: 10.0   memory length: 100000   epsilon: 0.6326763400079742    steps: 536    lr: 4e-05     evaluation reward: 4.96\n",
      "episode: 1317   score: 8.0   memory length: 100000   epsilon: 0.6318209800079928    steps: 432    lr: 4e-05     evaluation reward: 5.01\n",
      "episode: 1318   score: 6.0   memory length: 100000   epsilon: 0.631076500008009    steps: 376    lr: 4e-05     evaluation reward: 5.03\n",
      "episode: 1319   score: 6.0   memory length: 100000   epsilon: 0.6304211200080232    steps: 331    lr: 4e-05     evaluation reward: 5.07\n",
      "episode: 1320   score: 6.0   memory length: 100000   epsilon: 0.6296707000080395    steps: 379    lr: 4e-05     evaluation reward: 5.04\n",
      "episode: 1321   score: 3.0   memory length: 100000   epsilon: 0.6292192600080493    steps: 228    lr: 4e-05     evaluation reward: 4.98\n",
      "episode: 1322   score: 5.0   memory length: 100000   epsilon: 0.6285322000080642    steps: 347    lr: 4e-05     evaluation reward: 5.01\n",
      "episode: 1323   score: 12.0   memory length: 100000   epsilon: 0.6276293200080838    steps: 456    lr: 4e-05     evaluation reward: 5.06\n",
      "episode: 1324   score: 5.0   memory length: 100000   epsilon: 0.6269383000080988    steps: 349    lr: 4e-05     evaluation reward: 5.05\n",
      "episode: 1325   score: 6.0   memory length: 100000   epsilon: 0.6262433200081139    steps: 351    lr: 4e-05     evaluation reward: 5.08\n",
      "episode: 1326   score: 7.0   memory length: 100000   epsilon: 0.6254434000081313    steps: 404    lr: 4e-05     evaluation reward: 5.11\n",
      "episode: 1327   score: 5.0   memory length: 100000   epsilon: 0.6248335600081445    steps: 308    lr: 4e-05     evaluation reward: 5.05\n",
      "episode: 1328   score: 8.0   memory length: 100000   epsilon: 0.6238851400081651    steps: 479    lr: 4e-05     evaluation reward: 5.12\n",
      "episode: 1329   score: 6.0   memory length: 100000   epsilon: 0.6231743200081805    steps: 359    lr: 4e-05     evaluation reward: 5.13\n",
      "episode: 1330   score: 3.0   memory length: 100000   epsilon: 0.6227228800081903    steps: 228    lr: 4e-05     evaluation reward: 5.09\n",
      "episode: 1331   score: 11.0   memory length: 100000   epsilon: 0.6218932600082083    steps: 419    lr: 4e-05     evaluation reward: 5.16\n",
      "episode: 1332   score: 6.0   memory length: 100000   epsilon: 0.6212200600082229    steps: 340    lr: 4e-05     evaluation reward: 5.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1333   score: 6.0   memory length: 100000   epsilon: 0.6205666600082371    steps: 330    lr: 4e-05     evaluation reward: 5.18\n",
      "episode: 1334   score: 7.0   memory length: 100000   epsilon: 0.6197588200082547    steps: 408    lr: 4e-05     evaluation reward: 5.19\n",
      "episode: 1335   score: 5.0   memory length: 100000   epsilon: 0.6190697800082696    steps: 348    lr: 4e-05     evaluation reward: 5.22\n",
      "episode: 1336   score: 7.0   memory length: 100000   epsilon: 0.618224320008288    steps: 427    lr: 4e-05     evaluation reward: 5.26\n",
      "episode: 1337   score: 6.0   memory length: 100000   epsilon: 0.6175055800083036    steps: 363    lr: 4e-05     evaluation reward: 5.24\n",
      "episode: 1338   score: 7.0   memory length: 100000   epsilon: 0.6167710000083195    steps: 371    lr: 4e-05     evaluation reward: 5.22\n",
      "episode: 1339   score: 6.0   memory length: 100000   epsilon: 0.6160364200083355    steps: 371    lr: 4e-05     evaluation reward: 5.26\n",
      "episode: 1340   score: 13.0   memory length: 100000   epsilon: 0.6148721800083607    steps: 588    lr: 4e-05     evaluation reward: 5.31\n",
      "episode: 1341   score: 4.0   memory length: 100000   epsilon: 0.6143237200083727    steps: 277    lr: 4e-05     evaluation reward: 5.26\n",
      "episode: 1342   score: 1.0   memory length: 100000   epsilon: 0.6140227600083792    steps: 152    lr: 4e-05     evaluation reward: 5.19\n",
      "episode: 1343   score: 3.0   memory length: 100000   epsilon: 0.6136010200083883    steps: 213    lr: 4e-05     evaluation reward: 5.14\n",
      "episode: 1344   score: 3.0   memory length: 100000   epsilon: 0.613111960008399    steps: 247    lr: 4e-05     evaluation reward: 5.13\n",
      "episode: 1345   score: 4.0   memory length: 100000   epsilon: 0.612555580008411    steps: 281    lr: 4e-05     evaluation reward: 5.11\n",
      "episode: 1346   score: 4.0   memory length: 100000   epsilon: 0.612007120008423    steps: 277    lr: 4e-05     evaluation reward: 5.11\n",
      "episode: 1347   score: 2.0   memory length: 100000   epsilon: 0.6115675600084325    steps: 222    lr: 4e-05     evaluation reward: 5.09\n",
      "episode: 1348   score: 6.0   memory length: 100000   epsilon: 0.6108468400084481    steps: 364    lr: 4e-05     evaluation reward: 5.07\n",
      "episode: 1349   score: 6.0   memory length: 100000   epsilon: 0.6101776000084627    steps: 338    lr: 4e-05     evaluation reward: 5.1\n",
      "episode: 1350   score: 4.0   memory length: 100000   epsilon: 0.6096271600084746    steps: 278    lr: 4e-05     evaluation reward: 5.08\n",
      "episode: 1351   score: 7.0   memory length: 100000   epsilon: 0.6088807000084908    steps: 377    lr: 4e-05     evaluation reward: 5.11\n",
      "episode: 1352   score: 11.0   memory length: 100000   epsilon: 0.6081085000085076    steps: 390    lr: 4e-05     evaluation reward: 5.21\n",
      "episode: 1353   score: 5.0   memory length: 100000   epsilon: 0.6074630200085216    steps: 326    lr: 4e-05     evaluation reward: 5.24\n",
      "episode: 1354   score: 2.0   memory length: 100000   epsilon: 0.6070690000085301    steps: 199    lr: 4e-05     evaluation reward: 5.19\n",
      "episode: 1355   score: 3.0   memory length: 100000   epsilon: 0.6066452800085393    steps: 214    lr: 4e-05     evaluation reward: 5.18\n",
      "episode: 1356   score: 8.0   memory length: 100000   epsilon: 0.6057958600085578    steps: 429    lr: 4e-05     evaluation reward: 5.24\n",
      "episode: 1357   score: 4.0   memory length: 100000   epsilon: 0.6052513600085696    steps: 275    lr: 4e-05     evaluation reward: 5.26\n",
      "episode: 1358   score: 5.0   memory length: 100000   epsilon: 0.6045682600085844    steps: 345    lr: 4e-05     evaluation reward: 5.28\n",
      "episode: 1359   score: 10.0   memory length: 100000   epsilon: 0.6034614400086085    steps: 559    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
      "episode: 1360   score: 10.0   memory length: 100000   epsilon: 0.6023744200086321    steps: 549    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1361   score: 9.0   memory length: 100000   epsilon: 0.601501240008651    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1362   score: 1.0   memory length: 100000   epsilon: 0.6012002800086576    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 5.34\n",
      "episode: 1363   score: 10.0   memory length: 100000   epsilon: 0.6002538400086781    steps: 478    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1364   score: 8.0   memory length: 100000   epsilon: 0.5993965000086967    steps: 433    lr: 1.6000000000000003e-05     evaluation reward: 5.47\n",
      "episode: 1365   score: 4.0   memory length: 100000   epsilon: 0.5988460600087087    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1366   score: 7.0   memory length: 100000   epsilon: 0.5981491000087238    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1367   score: 5.0   memory length: 100000   epsilon: 0.5975056000087378    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1368   score: 4.0   memory length: 100000   epsilon: 0.5969135800087506    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1369   score: 3.0   memory length: 100000   epsilon: 0.5964185800087614    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1370   score: 7.0   memory length: 100000   epsilon: 0.5956721200087776    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1371   score: 4.0   memory length: 100000   epsilon: 0.5950939600087901    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1372   score: 6.0   memory length: 100000   epsilon: 0.5944702600088037    steps: 315    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1373   score: 7.0   memory length: 100000   epsilon: 0.5937990400088182    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1374   score: 4.0   memory length: 100000   epsilon: 0.5933218600088286    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 5.59\n",
      "episode: 1375   score: 6.0   memory length: 100000   epsilon: 0.5926348000088435    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
      "episode: 1376   score: 5.0   memory length: 100000   epsilon: 0.5919477400088584    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
      "episode: 1377   score: 4.0   memory length: 100000   epsilon: 0.5914012600088703    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1378   score: 5.0   memory length: 100000   epsilon: 0.5907736000088839    steps: 317    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1379   score: 3.0   memory length: 100000   epsilon: 0.5903201800088937    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.5\n",
      "episode: 1380   score: 8.0   memory length: 100000   epsilon: 0.5894509600089126    steps: 439    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1381   score: 7.0   memory length: 100000   epsilon: 0.5887322200089282    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
      "episode: 1382   score: 1.0   memory length: 100000   epsilon: 0.5884312600089348    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1383   score: 4.0   memory length: 100000   epsilon: 0.5878432000089475    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1384   score: 4.0   memory length: 100000   epsilon: 0.5873244400089588    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1385   score: 4.0   memory length: 100000   epsilon: 0.5867324200089716    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1386   score: 6.0   memory length: 100000   epsilon: 0.5860315000089868    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1387   score: 4.0   memory length: 100000   epsilon: 0.5855206600089979    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 5.49\n",
      "episode: 1388   score: 11.0   memory length: 100000   epsilon: 0.584413840009022    steps: 559    lr: 1.6000000000000003e-05     evaluation reward: 5.5\n",
      "episode: 1389   score: 7.0   memory length: 100000   epsilon: 0.5836099600090394    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1390   score: 8.0   memory length: 100000   epsilon: 0.5827387600090583    steps: 440    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1391   score: 6.0   memory length: 100000   epsilon: 0.581972500009075    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1392   score: 7.0   memory length: 100000   epsilon: 0.581235940009091    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1393   score: 5.0   memory length: 100000   epsilon: 0.5806894600091028    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
      "episode: 1394   score: 4.0   memory length: 100000   epsilon: 0.5801370400091148    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1395   score: 8.0   memory length: 100000   epsilon: 0.5795054200091285    steps: 319    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
      "episode: 1396   score: 9.0   memory length: 100000   epsilon: 0.5785649200091489    steps: 475    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
      "episode: 1397   score: 2.0   memory length: 100000   epsilon: 0.578192680009157    steps: 188    lr: 1.6000000000000003e-05     evaluation reward: 5.62\n",
      "episode: 1398   score: 3.0   memory length: 100000   epsilon: 0.5777729200091661    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.59\n",
      "episode: 1399   score: 9.0   memory length: 100000   epsilon: 0.5768957800091852    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1400   score: 2.0   memory length: 100000   epsilon: 0.5764601800091946    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 5.59\n",
      "episode: 1401   score: 5.0   memory length: 100000   epsilon: 0.575844400009208    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1402   score: 4.0   memory length: 100000   epsilon: 0.5752603000092207    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1403   score: 0.0   memory length: 100000   epsilon: 0.575014780009226    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1404   score: 3.0   memory length: 100000   epsilon: 0.5745574000092359    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 5.55\n",
      "episode: 1405   score: 13.0   memory length: 100000   epsilon: 0.5734565200092598    steps: 556    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1406   score: 6.0   memory length: 100000   epsilon: 0.5727437200092753    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
      "episode: 1407   score: 3.0   memory length: 100000   epsilon: 0.5722546600092859    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.67\n",
      "episode: 1408   score: 15.0   memory length: 100000   epsilon: 0.5710864600093113    steps: 590    lr: 1.6000000000000003e-05     evaluation reward: 5.77\n",
      "episode: 1409   score: 5.0   memory length: 100000   epsilon: 0.5704746400093246    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 5.77\n",
      "episode: 1410   score: 7.0   memory length: 100000   epsilon: 0.5697915400093394    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1411   score: 5.0   memory length: 100000   epsilon: 0.5691401200093535    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 5.78\n",
      "episode: 1412   score: 10.0   memory length: 100000   epsilon: 0.5683877200093699    steps: 380    lr: 1.6000000000000003e-05     evaluation reward: 5.83\n",
      "episode: 1413   score: 8.0   memory length: 100000   epsilon: 0.5675957200093871    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1414   score: 6.0   memory length: 100000   epsilon: 0.5668868800094025    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1415   score: 6.0   memory length: 100000   epsilon: 0.5661839800094177    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1416   score: 4.0   memory length: 100000   epsilon: 0.5656691800094289    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.83\n",
      "episode: 1417   score: 6.0   memory length: 100000   epsilon: 0.565019740009443    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1418   score: 2.0   memory length: 100000   epsilon: 0.5646257200094515    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 5.77\n",
      "episode: 1419   score: 10.0   memory length: 100000   epsilon: 0.5637030400094716    steps: 466    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1420   score: 6.0   memory length: 100000   epsilon: 0.5629605400094877    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1421   score: 6.0   memory length: 100000   epsilon: 0.5622734800095026    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1422   score: 6.0   memory length: 100000   epsilon: 0.5615230600095189    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
      "episode: 1423   score: 6.0   memory length: 100000   epsilon: 0.5608518400095335    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
      "episode: 1424   score: 4.0   memory length: 100000   epsilon: 0.5602618000095463    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 5.78\n",
      "episode: 1425   score: 7.0   memory length: 100000   epsilon: 0.5595113800095626    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
      "episode: 1426   score: 6.0   memory length: 100000   epsilon: 0.5588758000095764    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 5.78\n",
      "episode: 1427   score: 11.0   memory length: 100000   epsilon: 0.5577828400096001    steps: 552    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1428   score: 11.0   memory length: 100000   epsilon: 0.5565968200096258    steps: 599    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
      "episode: 1429   score: 3.0   memory length: 100000   epsilon: 0.5561711200096351    steps: 215    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1430   score: 3.0   memory length: 100000   epsilon: 0.5556781000096458    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1431   score: 7.0   memory length: 100000   epsilon: 0.5548781800096632    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1432   score: 9.0   memory length: 100000   epsilon: 0.5539515400096833    steps: 468    lr: 1.6000000000000003e-05     evaluation reward: 5.83\n",
      "episode: 1433   score: 7.0   memory length: 100000   epsilon: 0.5531140000097015    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1434   score: 7.0   memory length: 100000   epsilon: 0.5523121000097189    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 5.84\n",
      "episode: 1435   score: 4.0   memory length: 100000   epsilon: 0.5517161200097318    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 5.83\n",
      "episode: 1436   score: 4.0   memory length: 100000   epsilon: 0.551201320009743    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1437   score: 5.0   memory length: 100000   epsilon: 0.5506271200097554    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
      "episode: 1438   score: 3.0   memory length: 100000   epsilon: 0.5501756800097652    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.75\n",
      "episode: 1439   score: 13.0   memory length: 100000   epsilon: 0.549082720009789    steps: 552    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1440   score: 5.0   memory length: 100000   epsilon: 0.5485045600098015    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 5.74\n",
      "episode: 1441   score: 12.0   memory length: 100000   epsilon: 0.547425460009825    steps: 545    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1442   score: 10.0   memory length: 100000   epsilon: 0.546366160009848    steps: 535    lr: 1.6000000000000003e-05     evaluation reward: 5.91\n",
      "episode: 1443   score: 9.0   memory length: 100000   epsilon: 0.5453761600098694    steps: 500    lr: 1.6000000000000003e-05     evaluation reward: 5.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1444   score: 5.0   memory length: 100000   epsilon: 0.544798000009882    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 5.99\n",
      "episode: 1445   score: 2.0   memory length: 100000   epsilon: 0.5443980400098907    steps: 202    lr: 1.6000000000000003e-05     evaluation reward: 5.97\n",
      "episode: 1446   score: 8.0   memory length: 100000   epsilon: 0.5435050600099101    steps: 451    lr: 1.6000000000000003e-05     evaluation reward: 6.01\n",
      "episode: 1447   score: 6.0   memory length: 100000   epsilon: 0.542770480009926    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 6.05\n",
      "episode: 1448   score: 7.0   memory length: 100000   epsilon: 0.5420220400099423    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 6.06\n",
      "episode: 1449   score: 5.0   memory length: 100000   epsilon: 0.5414478400099547    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 6.05\n",
      "episode: 1450   score: 6.0   memory length: 100000   epsilon: 0.5407330600099702    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 6.07\n",
      "episode: 1451   score: 4.0   memory length: 100000   epsilon: 0.5401826200099822    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 6.04\n",
      "episode: 1452   score: 10.0   memory length: 100000   epsilon: 0.5391906400100037    steps: 501    lr: 1.6000000000000003e-05     evaluation reward: 6.03\n",
      "episode: 1453   score: 3.0   memory length: 100000   epsilon: 0.5386996000100144    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 6.01\n",
      "episode: 1454   score: 6.0   memory length: 100000   epsilon: 0.5380006600100296    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 6.05\n",
      "episode: 1455   score: 6.0   memory length: 100000   epsilon: 0.5372462800100459    steps: 381    lr: 1.6000000000000003e-05     evaluation reward: 6.08\n",
      "episode: 1456   score: 10.0   memory length: 100000   epsilon: 0.5362186600100682    steps: 519    lr: 1.6000000000000003e-05     evaluation reward: 6.1\n",
      "episode: 1457   score: 5.0   memory length: 100000   epsilon: 0.5355771400100822    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 6.11\n",
      "episode: 1458   score: 9.0   memory length: 100000   epsilon: 0.5345574400101043    steps: 515    lr: 1.6000000000000003e-05     evaluation reward: 6.15\n",
      "episode: 1459   score: 7.0   memory length: 100000   epsilon: 0.5337575200101217    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 6.12\n",
      "episode: 1460   score: 7.0   memory length: 100000   epsilon: 0.5329655200101389    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 6.09\n",
      "episode: 1461   score: 8.0   memory length: 100000   epsilon: 0.5321576800101564    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 6.08\n",
      "episode: 1462   score: 4.0   memory length: 100000   epsilon: 0.5316329800101678    steps: 265    lr: 1.6000000000000003e-05     evaluation reward: 6.11\n",
      "episode: 1463   score: 12.0   memory length: 100000   epsilon: 0.5307400000101872    steps: 451    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
      "episode: 1464   score: 11.0   memory length: 100000   epsilon: 0.5295935800102121    steps: 579    lr: 1.6000000000000003e-05     evaluation reward: 6.16\n",
      "episode: 1465   score: 5.0   memory length: 100000   epsilon: 0.5289936400102251    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 6.17\n",
      "episode: 1466   score: 8.0   memory length: 100000   epsilon: 0.5281283800102439    steps: 437    lr: 1.6000000000000003e-05     evaluation reward: 6.18\n",
      "episode: 1467   score: 7.0   memory length: 100000   epsilon: 0.5273898400102599    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 6.2\n",
      "episode: 1468   score: 8.0   memory length: 100000   epsilon: 0.5264909200102794    steps: 454    lr: 1.6000000000000003e-05     evaluation reward: 6.24\n",
      "episode: 1469   score: 5.0   memory length: 100000   epsilon: 0.5258078200102942    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1470   score: 9.0   memory length: 100000   epsilon: 0.524946520010313    steps: 435    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
      "episode: 1471   score: 8.0   memory length: 100000   epsilon: 0.5241367000103305    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 6.32\n",
      "episode: 1472   score: 5.0   memory length: 100000   epsilon: 0.5235367600103435    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
      "episode: 1473   score: 7.0   memory length: 100000   epsilon: 0.5227843600103599    steps: 380    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
      "episode: 1474   score: 8.0   memory length: 100000   epsilon: 0.5219567200103779    steps: 418    lr: 1.6000000000000003e-05     evaluation reward: 6.35\n",
      "episode: 1475   score: 9.0   memory length: 100000   epsilon: 0.5209944400103987    steps: 486    lr: 1.6000000000000003e-05     evaluation reward: 6.38\n",
      "episode: 1476   score: 7.0   memory length: 100000   epsilon: 0.520150960010417    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 6.4\n",
      "episode: 1477   score: 6.0   memory length: 100000   epsilon: 0.5194223200104329    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 6.42\n",
      "episode: 1478   score: 15.0   memory length: 100000   epsilon: 0.5182323400104587    steps: 601    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
      "episode: 1479   score: 6.0   memory length: 100000   epsilon: 0.5175195400104742    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
      "episode: 1480   score: 15.0   memory length: 100000   epsilon: 0.5161751200105034    steps: 679    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
      "episode: 1481   score: 5.0   memory length: 100000   epsilon: 0.5155296400105174    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 6.6\n",
      "episode: 1482   score: 8.0   memory length: 100000   epsilon: 0.514672300010536    steps: 433    lr: 1.6000000000000003e-05     evaluation reward: 6.67\n",
      "episode: 1483   score: 5.0   memory length: 100000   epsilon: 0.5139911800105508    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 6.68\n",
      "episode: 1484   score: 7.0   memory length: 100000   epsilon: 0.513149680010569    steps: 425    lr: 1.6000000000000003e-05     evaluation reward: 6.71\n",
      "episode: 1485   score: 7.0   memory length: 100000   epsilon: 0.5124408400105844    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 6.74\n",
      "episode: 1486   score: 3.0   memory length: 100000   epsilon: 0.5119854400105943    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 6.71\n",
      "episode: 1487   score: 11.0   memory length: 100000   epsilon: 0.5110033600106156    steps: 496    lr: 1.6000000000000003e-05     evaluation reward: 6.78\n",
      "episode: 1488   score: 13.0   memory length: 100000   epsilon: 0.510017320010637    steps: 498    lr: 1.6000000000000003e-05     evaluation reward: 6.8\n",
      "episode: 1489   score: 6.0   memory length: 100000   epsilon: 0.5092688800106533    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 6.79\n",
      "episode: 1490   score: 10.0   memory length: 100000   epsilon: 0.5082392800106756    steps: 520    lr: 1.6000000000000003e-05     evaluation reward: 6.81\n",
      "episode: 1491   score: 5.0   memory length: 100000   epsilon: 0.5075423200106908    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 6.8\n",
      "episode: 1492   score: 7.0   memory length: 100000   epsilon: 0.5067384400107082    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 6.8\n",
      "episode: 1493   score: 8.0   memory length: 100000   epsilon: 0.5059167400107261    steps: 415    lr: 1.6000000000000003e-05     evaluation reward: 6.83\n",
      "episode: 1494   score: 2.0   memory length: 100000   epsilon: 0.5055544000107339    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 6.81\n",
      "episode: 1495   score: 4.0   memory length: 100000   epsilon: 0.5049584200107469    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 6.77\n",
      "episode: 1496   score: 5.0   memory length: 100000   epsilon: 0.5043565000107599    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
      "episode: 1497   score: 6.0   memory length: 100000   epsilon: 0.5036516200107752    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 6.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1498   score: 4.0   memory length: 100000   epsilon: 0.5031051400107871    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 6.78\n",
      "episode: 1499   score: 9.0   memory length: 100000   epsilon: 0.5021309800108082    steps: 492    lr: 1.6000000000000003e-05     evaluation reward: 6.78\n",
      "episode: 1500   score: 6.0   memory length: 100000   epsilon: 0.5014795600108224    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 6.82\n",
      "episode: 1501   score: 4.0   memory length: 100000   epsilon: 0.5009608000108337    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 6.81\n",
      "episode: 1502   score: 5.0   memory length: 100000   epsilon: 0.500345020010847    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 6.82\n",
      "episode: 1503   score: 4.0   memory length: 100000   epsilon: 0.49979062001085317    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 6.86\n",
      "episode: 1504   score: 7.0   memory length: 100000   epsilon: 0.4990243600108483    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 6.9\n",
      "episode: 1505   score: 9.0   memory length: 100000   epsilon: 0.498024460010842    steps: 505    lr: 1.6000000000000003e-05     evaluation reward: 6.86\n",
      "episode: 1506   score: 10.0   memory length: 100000   epsilon: 0.49704040001083577    steps: 497    lr: 1.6000000000000003e-05     evaluation reward: 6.9\n",
      "episode: 1507   score: 10.0   memory length: 100000   epsilon: 0.4959553600108289    steps: 548    lr: 1.6000000000000003e-05     evaluation reward: 6.97\n",
      "episode: 1508   score: 2.0   memory length: 100000   epsilon: 0.4955573800108264    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 6.84\n",
      "episode: 1509   score: 6.0   memory length: 100000   epsilon: 0.4948921000108222    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 6.85\n",
      "episode: 1510   score: 4.0   memory length: 100000   epsilon: 0.49430206001081844    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 6.82\n",
      "episode: 1511   score: 7.0   memory length: 100000   epsilon: 0.49357342001081383    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 6.84\n",
      "episode: 1512   score: 4.0   memory length: 100000   epsilon: 0.4930269400108104    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 6.78\n",
      "episode: 1513   score: 4.0   memory length: 100000   epsilon: 0.49251610001080715    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 6.74\n",
      "episode: 1514   score: 14.0   memory length: 100000   epsilon: 0.4911815800107987    steps: 674    lr: 1.6000000000000003e-05     evaluation reward: 6.82\n",
      "episode: 1515   score: 11.0   memory length: 100000   epsilon: 0.4900925800107918    steps: 550    lr: 1.6000000000000003e-05     evaluation reward: 6.87\n",
      "episode: 1516   score: 4.0   memory length: 100000   epsilon: 0.48957778001078855    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 6.87\n",
      "episode: 1517   score: 8.0   memory length: 100000   epsilon: 0.48875806001078337    steps: 414    lr: 1.6000000000000003e-05     evaluation reward: 6.89\n",
      "episode: 1518   score: 10.0   memory length: 100000   epsilon: 0.48787300001077777    steps: 447    lr: 1.6000000000000003e-05     evaluation reward: 6.97\n",
      "episode: 1519   score: 4.0   memory length: 100000   epsilon: 0.48731662001077425    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 6.91\n",
      "episode: 1520   score: 5.0   memory length: 100000   epsilon: 0.4866731200107702    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 6.9\n",
      "episode: 1521   score: 4.0   memory length: 100000   epsilon: 0.4861226800107667    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 6.88\n",
      "episode: 1522   score: 8.0   memory length: 100000   epsilon: 0.48526534001076127    steps: 433    lr: 1.6000000000000003e-05     evaluation reward: 6.9\n",
      "episode: 1523   score: 9.0   memory length: 100000   epsilon: 0.4843565200107555    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
      "episode: 1524   score: 6.0   memory length: 100000   epsilon: 0.4837090600107514    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 6.95\n",
      "episode: 1525   score: 5.0   memory length: 100000   epsilon: 0.4830279400107471    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
      "episode: 1526   score: 12.0   memory length: 100000   epsilon: 0.4818102400107394    steps: 615    lr: 1.6000000000000003e-05     evaluation reward: 6.99\n",
      "episode: 1527   score: 5.0   memory length: 100000   epsilon: 0.48120040001073555    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
      "episode: 1528   score: 12.0   memory length: 100000   epsilon: 0.4802579200107296    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 6.94\n",
      "episode: 1529   score: 10.0   memory length: 100000   epsilon: 0.47912932001072245    steps: 570    lr: 1.6000000000000003e-05     evaluation reward: 7.01\n",
      "episode: 1530   score: 5.0   memory length: 100000   epsilon: 0.4785214600107186    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 7.03\n",
      "episode: 1531   score: 5.0   memory length: 100000   epsilon: 0.4778700400107145    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 7.01\n",
      "episode: 1532   score: 7.0   memory length: 100000   epsilon: 0.4770681400107094    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 6.99\n",
      "episode: 1533   score: 6.0   memory length: 100000   epsilon: 0.47636128001070493    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 6.98\n",
      "episode: 1534   score: 14.0   memory length: 100000   epsilon: 0.47504854001069663    steps: 663    lr: 1.6000000000000003e-05     evaluation reward: 7.05\n",
      "episode: 1535   score: 8.0   memory length: 100000   epsilon: 0.4741733800106911    steps: 442    lr: 1.6000000000000003e-05     evaluation reward: 7.09\n",
      "episode: 1536   score: 6.0   memory length: 100000   epsilon: 0.4734982000106868    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 7.11\n",
      "episode: 1537   score: 9.0   memory length: 100000   epsilon: 0.47252206001068064    steps: 493    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
      "episode: 1538   score: 4.0   memory length: 100000   epsilon: 0.4719320200106769    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 7.16\n",
      "episode: 1539   score: 7.0   memory length: 100000   epsilon: 0.47119546001067225    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 7.1\n",
      "episode: 1540   score: 6.0   memory length: 100000   epsilon: 0.47051830001066797    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 7.11\n",
      "episode: 1541   score: 16.0   memory length: 100000   epsilon: 0.4693243600106604    steps: 603    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
      "episode: 1542   score: 4.0   memory length: 100000   epsilon: 0.46877590001065694    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 7.09\n",
      "episode: 1543   score: 10.0   memory length: 100000   epsilon: 0.46777996001065064    steps: 503    lr: 1.6000000000000003e-05     evaluation reward: 7.1\n",
      "episode: 1544   score: 11.0   memory length: 100000   epsilon: 0.4669978600106457    steps: 395    lr: 1.6000000000000003e-05     evaluation reward: 7.16\n",
      "episode: 1545   score: 8.0   memory length: 100000   epsilon: 0.4660633000106398    steps: 472    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
      "episode: 1546   score: 6.0   memory length: 100000   epsilon: 0.46539208001063553    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 7.2\n",
      "episode: 1547   score: 5.0   memory length: 100000   epsilon: 0.4647822400106317    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
      "episode: 1548   score: 9.0   memory length: 100000   epsilon: 0.46387540001062594    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 7.21\n",
      "episode: 1549   score: 11.0   memory length: 100000   epsilon: 0.4629982600106204    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 7.27\n",
      "episode: 1550   score: 5.0   memory length: 100000   epsilon: 0.4623488200106163    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 7.26\n",
      "episode: 1551   score: 7.0   memory length: 100000   epsilon: 0.46151920001061103    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 7.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1552   score: 13.0   memory length: 100000   epsilon: 0.4605173200106047    steps: 506    lr: 1.6000000000000003e-05     evaluation reward: 7.32\n",
      "episode: 1553   score: 5.0   memory length: 100000   epsilon: 0.45991342001060087    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 7.34\n",
      "episode: 1554   score: 4.0   memory length: 100000   epsilon: 0.4593669400105974    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 7.32\n",
      "episode: 1555   score: 9.0   memory length: 100000   epsilon: 0.4584917800105919    steps: 442    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
      "episode: 1556   score: 8.0   memory length: 100000   epsilon: 0.45769978001058687    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 7.33\n",
      "episode: 1557   score: 7.0   memory length: 100000   epsilon: 0.45681472001058127    steps: 447    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
      "episode: 1558   score: 7.0   memory length: 100000   epsilon: 0.45608806001057667    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 7.33\n",
      "episode: 1559   score: 3.0   memory length: 100000   epsilon: 0.4556386000105738    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 7.29\n",
      "episode: 1560   score: 12.0   memory length: 100000   epsilon: 0.454719880010568    steps: 464    lr: 1.6000000000000003e-05     evaluation reward: 7.34\n",
      "episode: 1561   score: 6.0   memory length: 100000   epsilon: 0.4539952000105634    steps: 366    lr: 1.6000000000000003e-05     evaluation reward: 7.32\n",
      "episode: 1562   score: 11.0   memory length: 100000   epsilon: 0.4528507600105562    steps: 578    lr: 1.6000000000000003e-05     evaluation reward: 7.39\n",
      "episode: 1563   score: 5.0   memory length: 100000   epsilon: 0.45224686001055237    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 7.32\n",
      "episode: 1564   score: 5.0   memory length: 100000   epsilon: 0.45164692001054857    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 7.26\n",
      "episode: 1565   score: 4.0   memory length: 100000   epsilon: 0.45110638001054515    steps: 273    lr: 1.6000000000000003e-05     evaluation reward: 7.25\n",
      "episode: 1566   score: 10.0   memory length: 100000   epsilon: 0.45006292001053855    steps: 527    lr: 1.6000000000000003e-05     evaluation reward: 7.27\n",
      "episode: 1567   score: 3.0   memory length: 100000   epsilon: 0.44964910001053593    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 7.23\n",
      "episode: 1568   score: 7.0   memory length: 100000   epsilon: 0.448867000010531    steps: 395    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
      "episode: 1569   score: 6.0   memory length: 100000   epsilon: 0.4482037000105268    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 7.23\n",
      "episode: 1570   score: 10.0   memory length: 100000   epsilon: 0.4471919200105204    steps: 511    lr: 1.6000000000000003e-05     evaluation reward: 7.24\n",
      "episode: 1571   score: 7.0   memory length: 100000   epsilon: 0.44641378001051546    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 7.23\n",
      "episode: 1572   score: 8.0   memory length: 100000   epsilon: 0.4456158400105104    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 7.26\n",
      "episode: 1573   score: 6.0   memory length: 100000   epsilon: 0.44483374001050546    steps: 395    lr: 1.6000000000000003e-05     evaluation reward: 7.25\n",
      "episode: 1574   score: 7.0   memory length: 100000   epsilon: 0.44411896001050094    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 7.24\n",
      "episode: 1575   score: 12.0   memory length: 100000   epsilon: 0.44310718001049454    steps: 511    lr: 1.6000000000000003e-05     evaluation reward: 7.27\n",
      "episode: 1576   score: 4.0   memory length: 100000   epsilon: 0.44262010001049146    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 7.24\n",
      "episode: 1577   score: 8.0   memory length: 100000   epsilon: 0.44171524001048573    steps: 457    lr: 1.6000000000000003e-05     evaluation reward: 7.26\n",
      "episode: 1578   score: 5.0   memory length: 100000   epsilon: 0.44110144001048185    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 7.16\n",
      "episode: 1579   score: 12.0   memory length: 100000   epsilon: 0.4401292600104757    steps: 491    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
      "episode: 1580   score: 12.0   memory length: 100000   epsilon: 0.4389293800104681    steps: 606    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
      "episode: 1581   score: 5.0   memory length: 100000   epsilon: 0.4383433000104644    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
      "episode: 1582   score: 4.0   memory length: 100000   epsilon: 0.43785820001046133    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
      "episode: 1583   score: 6.0   memory length: 100000   epsilon: 0.43711768001045664    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 7.16\n",
      "episode: 1584   score: 8.0   memory length: 100000   epsilon: 0.43626826001045127    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 7.17\n",
      "episode: 1585   score: 12.0   memory length: 100000   epsilon: 0.43500304001044326    steps: 639    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
      "episode: 1586   score: 9.0   memory length: 100000   epsilon: 0.4339615600104367    steps: 526    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
      "episode: 1587   score: 8.0   memory length: 100000   epsilon: 0.4330289800104308    steps: 471    lr: 1.6000000000000003e-05     evaluation reward: 7.25\n",
      "episode: 1588   score: 14.0   memory length: 100000   epsilon: 0.4317340600104226    steps: 654    lr: 1.6000000000000003e-05     evaluation reward: 7.26\n",
      "episode: 1589   score: 4.0   memory length: 100000   epsilon: 0.4312489600104195    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 7.24\n",
      "episode: 1590   score: 8.0   memory length: 100000   epsilon: 0.4303460800104138    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
      "episode: 1591   score: 6.0   memory length: 100000   epsilon: 0.42964120001040934    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 7.23\n",
      "episode: 1592   score: 3.0   memory length: 100000   epsilon: 0.4291481800104062    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
      "episode: 1593   score: 10.0   memory length: 100000   epsilon: 0.42812848001039977    steps: 515    lr: 1.6000000000000003e-05     evaluation reward: 7.21\n",
      "episode: 1594   score: 8.0   memory length: 100000   epsilon: 0.4273087600103946    steps: 414    lr: 1.6000000000000003e-05     evaluation reward: 7.27\n",
      "episode: 1595   score: 9.0   memory length: 100000   epsilon: 0.4263286600103884    steps: 495    lr: 1.6000000000000003e-05     evaluation reward: 7.32\n",
      "episode: 1596   score: 8.0   memory length: 100000   epsilon: 0.42550498001038317    steps: 416    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
      "episode: 1597   score: 10.0   memory length: 100000   epsilon: 0.4244199400103763    steps: 548    lr: 1.6000000000000003e-05     evaluation reward: 7.39\n",
      "episode: 1598   score: 8.0   memory length: 100000   epsilon: 0.42361804001037123    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 7.43\n",
      "episode: 1599   score: 6.0   memory length: 100000   epsilon: 0.422948800010367    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 7.4\n",
      "episode: 1600   score: 12.0   memory length: 100000   epsilon: 0.42200632001036104    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 7.46\n",
      "episode: 1601   score: 13.0   memory length: 100000   epsilon: 0.421048000010355    steps: 484    lr: 1.6000000000000003e-05     evaluation reward: 7.55\n",
      "episode: 1602   score: 13.0   memory length: 100000   epsilon: 0.41979664001034706    steps: 632    lr: 1.6000000000000003e-05     evaluation reward: 7.63\n",
      "episode: 1603   score: 10.0   memory length: 100000   epsilon: 0.4188878200103413    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 7.69\n",
      "episode: 1604   score: 6.0   memory length: 100000   epsilon: 0.41823244001033716    steps: 331    lr: 1.6000000000000003e-05     evaluation reward: 7.68\n",
      "episode: 1605   score: 10.0   memory length: 100000   epsilon: 0.41720284001033064    steps: 520    lr: 1.6000000000000003e-05     evaluation reward: 7.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1606   score: 9.0   memory length: 100000   epsilon: 0.4162762000103248    steps: 468    lr: 1.6000000000000003e-05     evaluation reward: 7.68\n",
      "episode: 1607   score: 4.0   memory length: 100000   epsilon: 0.4157237800103213    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 7.62\n",
      "episode: 1608   score: 15.0   memory length: 100000   epsilon: 0.41459518001031415    steps: 570    lr: 1.6000000000000003e-05     evaluation reward: 7.75\n",
      "episode: 1609   score: 6.0   memory length: 100000   epsilon: 0.41388436001030965    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 7.75\n",
      "episode: 1610   score: 8.0   memory length: 100000   epsilon: 0.41299930001030405    steps: 447    lr: 1.6000000000000003e-05     evaluation reward: 7.79\n",
      "episode: 1611   score: 14.0   memory length: 100000   epsilon: 0.4119974200102977    steps: 506    lr: 1.6000000000000003e-05     evaluation reward: 7.86\n",
      "episode: 1612   score: 8.0   memory length: 100000   epsilon: 0.4111757200102925    steps: 415    lr: 1.6000000000000003e-05     evaluation reward: 7.9\n",
      "episode: 1613   score: 9.0   memory length: 100000   epsilon: 0.4102273000102865    steps: 479    lr: 1.6000000000000003e-05     evaluation reward: 7.95\n",
      "episode: 1614   score: 8.0   memory length: 100000   epsilon: 0.4094155000102814    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 7.89\n",
      "episode: 1615   score: 7.0   memory length: 100000   epsilon: 0.40855618001027594    steps: 434    lr: 1.6000000000000003e-05     evaluation reward: 7.85\n",
      "episode: 1616   score: 6.0   memory length: 100000   epsilon: 0.4078552600102715    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 7.87\n",
      "episode: 1617   score: 5.0   memory length: 100000   epsilon: 0.40727710001026785    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 7.84\n",
      "episode: 1618   score: 13.0   memory length: 100000   epsilon: 0.40604554001026005    steps: 622    lr: 1.6000000000000003e-05     evaluation reward: 7.87\n",
      "episode: 1619   score: 10.0   memory length: 100000   epsilon: 0.40497040001025325    steps: 543    lr: 6.400000000000001e-06     evaluation reward: 7.93\n",
      "episode: 1620   score: 7.0   memory length: 100000   epsilon: 0.40424374001024865    steps: 367    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
      "episode: 1621   score: 9.0   memory length: 100000   epsilon: 0.40337452001024315    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 8.0\n",
      "episode: 1622   score: 20.0   memory length: 100000   epsilon: 0.4021152400102352    steps: 636    lr: 6.400000000000001e-06     evaluation reward: 8.12\n",
      "episode: 1623   score: 7.0   memory length: 100000   epsilon: 0.4014202600102308    steps: 351    lr: 6.400000000000001e-06     evaluation reward: 8.1\n",
      "episode: 1624   score: 14.0   memory length: 100000   epsilon: 0.4003669000102241    steps: 532    lr: 6.400000000000001e-06     evaluation reward: 8.18\n",
      "episode: 1625   score: 10.0   memory length: 100000   epsilon: 0.3995293600102188    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 8.23\n",
      "episode: 1626   score: 8.0   memory length: 100000   epsilon: 0.3987056800102136    steps: 416    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1627   score: 8.0   memory length: 100000   epsilon: 0.3977731000102077    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 8.22\n",
      "episode: 1628   score: 10.0   memory length: 100000   epsilon: 0.3967633000102013    steps: 510    lr: 6.400000000000001e-06     evaluation reward: 8.2\n",
      "episode: 1629   score: 10.0   memory length: 100000   epsilon: 0.39585052001019555    steps: 461    lr: 6.400000000000001e-06     evaluation reward: 8.2\n",
      "episode: 1630   score: 8.0   memory length: 100000   epsilon: 0.3950011000101902    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 8.23\n",
      "episode: 1631   score: 8.0   memory length: 100000   epsilon: 0.39423484001018533    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 8.26\n",
      "episode: 1632   score: 7.0   memory length: 100000   epsilon: 0.3934428400101803    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 8.26\n",
      "episode: 1633   score: 6.0   memory length: 100000   epsilon: 0.39273994001017587    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 8.26\n",
      "episode: 1634   score: 3.0   memory length: 100000   epsilon: 0.39224692001017275    steps: 249    lr: 6.400000000000001e-06     evaluation reward: 8.15\n",
      "episode: 1635   score: 13.0   memory length: 100000   epsilon: 0.3913202800101669    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 8.2\n",
      "episode: 1636   score: 4.0   memory length: 100000   epsilon: 0.39076192001016335    steps: 282    lr: 6.400000000000001e-06     evaluation reward: 8.18\n",
      "episode: 1637   score: 10.0   memory length: 100000   epsilon: 0.3897442000101569    steps: 514    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1638   score: 11.0   memory length: 100000   epsilon: 0.3885284800101492    steps: 614    lr: 6.400000000000001e-06     evaluation reward: 8.26\n",
      "episode: 1639   score: 4.0   memory length: 100000   epsilon: 0.388017640010146    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 8.23\n",
      "episode: 1640   score: 3.0   memory length: 100000   epsilon: 0.3875285800101429    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 8.2\n",
      "episode: 1641   score: 4.0   memory length: 100000   epsilon: 0.38698210001013944    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 8.08\n",
      "episode: 1642   score: 9.0   memory length: 100000   epsilon: 0.38606932001013367    steps: 461    lr: 6.400000000000001e-06     evaluation reward: 8.13\n",
      "episode: 1643   score: 9.0   memory length: 100000   epsilon: 0.38515258001012787    steps: 463    lr: 6.400000000000001e-06     evaluation reward: 8.12\n",
      "episode: 1644   score: 8.0   memory length: 100000   epsilon: 0.3843506800101228    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 8.09\n",
      "episode: 1645   score: 13.0   memory length: 100000   epsilon: 0.3830894200101148    steps: 637    lr: 6.400000000000001e-06     evaluation reward: 8.14\n",
      "episode: 1646   score: 5.0   memory length: 100000   epsilon: 0.38243404001011067    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 8.13\n",
      "episode: 1647   score: 6.0   memory length: 100000   epsilon: 0.38181826001010677    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 8.14\n",
      "episode: 1648   score: 7.0   memory length: 100000   epsilon: 0.38105992001010197    steps: 383    lr: 6.400000000000001e-06     evaluation reward: 8.12\n",
      "episode: 1649   score: 1.0   memory length: 100000   epsilon: 0.38075896001010007    steps: 152    lr: 6.400000000000001e-06     evaluation reward: 8.02\n",
      "episode: 1650   score: 11.0   memory length: 100000   epsilon: 0.3797095600100934    steps: 530    lr: 6.400000000000001e-06     evaluation reward: 8.08\n",
      "episode: 1651   score: 3.0   memory length: 100000   epsilon: 0.37928584001009075    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 8.04\n",
      "episode: 1652   score: 10.0   memory length: 100000   epsilon: 0.37827208001008433    steps: 512    lr: 6.400000000000001e-06     evaluation reward: 8.01\n",
      "episode: 1653   score: 9.0   memory length: 100000   epsilon: 0.3773216800100783    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
      "episode: 1654   score: 8.0   memory length: 100000   epsilon: 0.3765138400100732    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 8.09\n",
      "episode: 1655   score: 18.0   memory length: 100000   epsilon: 0.37509814001006425    steps: 715    lr: 6.400000000000001e-06     evaluation reward: 8.18\n",
      "episode: 1656   score: 11.0   memory length: 100000   epsilon: 0.374110120010058    steps: 499    lr: 6.400000000000001e-06     evaluation reward: 8.21\n",
      "episode: 1657   score: 6.0   memory length: 100000   epsilon: 0.37340920001005357    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 8.2\n",
      "episode: 1658   score: 11.0   memory length: 100000   epsilon: 0.3723063400100466    steps: 557    lr: 6.400000000000001e-06     evaluation reward: 8.24\n",
      "episode: 1659   score: 14.0   memory length: 100000   epsilon: 0.3711856600100395    steps: 566    lr: 6.400000000000001e-06     evaluation reward: 8.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1660   score: 4.0   memory length: 100000   epsilon: 0.37059562001003576    steps: 298    lr: 6.400000000000001e-06     evaluation reward: 8.27\n",
      "episode: 1661   score: 6.0   memory length: 100000   epsilon: 0.3698551000100311    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 8.27\n",
      "episode: 1662   score: 11.0   memory length: 100000   epsilon: 0.36885124001002473    steps: 507    lr: 6.400000000000001e-06     evaluation reward: 8.27\n",
      "episode: 1663   score: 8.0   memory length: 100000   epsilon: 0.36790678001001875    steps: 477    lr: 6.400000000000001e-06     evaluation reward: 8.3\n",
      "episode: 1664   score: 9.0   memory length: 100000   epsilon: 0.3670593400100134    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 8.34\n",
      "episode: 1665   score: 6.0   memory length: 100000   epsilon: 0.36641980001000934    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1666   score: 5.0   memory length: 100000   epsilon: 0.3658614400100058    steps: 282    lr: 6.400000000000001e-06     evaluation reward: 8.31\n",
      "episode: 1667   score: 8.0   memory length: 100000   epsilon: 0.365105080010001    steps: 382    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1668   score: 10.0   memory length: 100000   epsilon: 0.3641329000099949    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 8.39\n",
      "episode: 1669   score: 10.0   memory length: 100000   epsilon: 0.363049840009988    steps: 547    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1670   score: 8.0   memory length: 100000   epsilon: 0.36213508000998224    steps: 462    lr: 6.400000000000001e-06     evaluation reward: 8.41\n",
      "episode: 1671   score: 10.0   memory length: 100000   epsilon: 0.361153000009976    steps: 496    lr: 6.400000000000001e-06     evaluation reward: 8.44\n",
      "episode: 1672   score: 9.0   memory length: 100000   epsilon: 0.3601570600099697    steps: 503    lr: 6.400000000000001e-06     evaluation reward: 8.45\n",
      "episode: 1673   score: 7.0   memory length: 100000   epsilon: 0.3594403000099652    steps: 362    lr: 6.400000000000001e-06     evaluation reward: 8.46\n",
      "episode: 1674   score: 12.0   memory length: 100000   epsilon: 0.3585552400099596    steps: 447    lr: 6.400000000000001e-06     evaluation reward: 8.51\n",
      "episode: 1675   score: 11.0   memory length: 100000   epsilon: 0.35749396000995287    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 8.5\n",
      "episode: 1676   score: 5.0   memory length: 100000   epsilon: 0.35676136000994824    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 8.51\n",
      "episode: 1677   score: 7.0   memory length: 100000   epsilon: 0.35595946000994316    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 8.5\n",
      "episode: 1678   score: 10.0   memory length: 100000   epsilon: 0.35481700000993593    steps: 577    lr: 6.400000000000001e-06     evaluation reward: 8.55\n",
      "episode: 1679   score: 10.0   memory length: 100000   epsilon: 0.35384086000992976    steps: 493    lr: 6.400000000000001e-06     evaluation reward: 8.53\n",
      "episode: 1680   score: 5.0   memory length: 100000   epsilon: 0.3532270600099259    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 8.46\n",
      "episode: 1681   score: 8.0   memory length: 100000   epsilon: 0.3524449600099209    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1682   score: 17.0   memory length: 100000   epsilon: 0.3511460800099127    steps: 656    lr: 6.400000000000001e-06     evaluation reward: 8.62\n",
      "episode: 1683   score: 7.0   memory length: 100000   epsilon: 0.35034616000990765    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 8.63\n",
      "episode: 1684   score: 5.0   memory length: 100000   epsilon: 0.3497026600099036    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 8.6\n",
      "episode: 1685   score: 6.0   memory length: 100000   epsilon: 0.3490750000098996    steps: 317    lr: 6.400000000000001e-06     evaluation reward: 8.54\n",
      "episode: 1686   score: 12.0   memory length: 100000   epsilon: 0.34784740000989184    steps: 620    lr: 6.400000000000001e-06     evaluation reward: 8.57\n",
      "episode: 1687   score: 11.0   memory length: 100000   epsilon: 0.3467960200098852    steps: 531    lr: 6.400000000000001e-06     evaluation reward: 8.6\n",
      "episode: 1688   score: 10.0   memory length: 100000   epsilon: 0.34580998000987895    steps: 498    lr: 6.400000000000001e-06     evaluation reward: 8.56\n",
      "episode: 1689   score: 7.0   memory length: 100000   epsilon: 0.34508134000987434    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 8.59\n",
      "episode: 1690   score: 7.0   memory length: 100000   epsilon: 0.3443527000098697    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 8.58\n",
      "episode: 1691   score: 10.0   memory length: 100000   epsilon: 0.3433171600098632    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 8.62\n",
      "episode: 1692   score: 17.0   memory length: 100000   epsilon: 0.3420559000098552    steps: 637    lr: 6.400000000000001e-06     evaluation reward: 8.76\n",
      "episode: 1693   score: 15.0   memory length: 100000   epsilon: 0.34097284000984834    steps: 547    lr: 6.400000000000001e-06     evaluation reward: 8.81\n",
      "episode: 1694   score: 10.0   memory length: 100000   epsilon: 0.34010164000984283    steps: 440    lr: 6.400000000000001e-06     evaluation reward: 8.83\n",
      "episode: 1695   score: 5.0   memory length: 100000   epsilon: 0.33945814000983876    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 8.79\n",
      "episode: 1696   score: 6.0   memory length: 100000   epsilon: 0.3387532600098343    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 8.77\n",
      "episode: 1697   score: 13.0   memory length: 100000   epsilon: 0.3375692200098268    steps: 598    lr: 6.400000000000001e-06     evaluation reward: 8.8\n",
      "episode: 1698   score: 8.0   memory length: 100000   epsilon: 0.33671980000982143    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 8.8\n",
      "episode: 1699   score: 11.0   memory length: 100000   epsilon: 0.3356585200098147    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 8.85\n",
      "episode: 1700   score: 6.0   memory length: 100000   epsilon: 0.3349001800098099    steps: 383    lr: 6.400000000000001e-06     evaluation reward: 8.79\n",
      "episode: 1701   score: 14.0   memory length: 100000   epsilon: 0.33383296000980317    steps: 539    lr: 6.400000000000001e-06     evaluation reward: 8.8\n",
      "episode: 1702   score: 9.0   memory length: 100000   epsilon: 0.33291226000979735    steps: 465    lr: 6.400000000000001e-06     evaluation reward: 8.76\n",
      "episode: 1703   score: 16.0   memory length: 100000   epsilon: 0.3317341600097899    steps: 595    lr: 6.400000000000001e-06     evaluation reward: 8.82\n",
      "episode: 1704   score: 12.0   memory length: 100000   epsilon: 0.3306610000097831    steps: 542    lr: 6.400000000000001e-06     evaluation reward: 8.88\n",
      "episode: 1705   score: 8.0   memory length: 100000   epsilon: 0.3297422800097773    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 8.86\n",
      "episode: 1706   score: 8.0   memory length: 100000   epsilon: 0.3289384000097722    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 8.85\n",
      "episode: 1707   score: 16.0   memory length: 100000   epsilon: 0.32782564000976516    steps: 562    lr: 6.400000000000001e-06     evaluation reward: 8.97\n",
      "episode: 1708   score: 7.0   memory length: 100000   epsilon: 0.32704948000976025    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 8.89\n",
      "episode: 1709   score: 10.0   memory length: 100000   epsilon: 0.32588128000975286    steps: 590    lr: 6.400000000000001e-06     evaluation reward: 8.93\n",
      "episode: 1710   score: 7.0   memory length: 100000   epsilon: 0.325115020009748    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 8.92\n",
      "episode: 1711   score: 7.0   memory length: 100000   epsilon: 0.3243111400097429    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 8.85\n",
      "episode: 1712   score: 13.0   memory length: 100000   epsilon: 0.323059780009735    steps: 632    lr: 6.400000000000001e-06     evaluation reward: 8.9\n",
      "episode: 1713   score: 9.0   memory length: 100000   epsilon: 0.32208760000972886    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1714   score: 7.0   memory length: 100000   epsilon: 0.3212401600097235    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 8.89\n",
      "episode: 1715   score: 14.0   memory length: 100000   epsilon: 0.32025214000971725    steps: 499    lr: 6.400000000000001e-06     evaluation reward: 8.96\n",
      "episode: 1716   score: 11.0   memory length: 100000   epsilon: 0.31916314000971036    steps: 550    lr: 6.400000000000001e-06     evaluation reward: 9.01\n",
      "episode: 1717   score: 14.0   memory length: 100000   epsilon: 0.3180444400097033    steps: 565    lr: 6.400000000000001e-06     evaluation reward: 9.1\n",
      "episode: 1718   score: 8.0   memory length: 100000   epsilon: 0.31714156000969757    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 9.05\n",
      "episode: 1719   score: 5.0   memory length: 100000   epsilon: 0.3165317200096937    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 9.0\n",
      "episode: 1720   score: 5.0   memory length: 100000   epsilon: 0.31587634000968956    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 8.98\n",
      "episode: 1721   score: 8.0   memory length: 100000   epsilon: 0.3150110800096841    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 8.97\n",
      "episode: 1722   score: 9.0   memory length: 100000   epsilon: 0.3141418600096786    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 8.86\n",
      "episode: 1723   score: 10.0   memory length: 100000   epsilon: 0.31326472000967304    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 8.89\n",
      "episode: 1724   score: 5.0   memory length: 100000   epsilon: 0.31268458000966937    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 8.8\n",
      "episode: 1725   score: 10.0   memory length: 100000   epsilon: 0.31173220000966334    steps: 481    lr: 6.400000000000001e-06     evaluation reward: 8.8\n",
      "episode: 1726   score: 10.0   memory length: 100000   epsilon: 0.3106946800096568    steps: 524    lr: 6.400000000000001e-06     evaluation reward: 8.82\n",
      "episode: 1727   score: 11.0   memory length: 100000   epsilon: 0.3095898400096498    steps: 558    lr: 6.400000000000001e-06     evaluation reward: 8.85\n",
      "episode: 1728   score: 11.0   memory length: 100000   epsilon: 0.3086612200096439    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 8.86\n",
      "episode: 1729   score: 10.0   memory length: 100000   epsilon: 0.3076652800096376    steps: 503    lr: 6.400000000000001e-06     evaluation reward: 8.86\n",
      "episode: 1730   score: 6.0   memory length: 100000   epsilon: 0.3069208000096329    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 8.84\n",
      "episode: 1731   score: 8.0   memory length: 100000   epsilon: 0.3060317800096273    steps: 449    lr: 6.400000000000001e-06     evaluation reward: 8.84\n",
      "episode: 1732   score: 5.0   memory length: 100000   epsilon: 0.30541204000962335    steps: 313    lr: 6.400000000000001e-06     evaluation reward: 8.82\n",
      "episode: 1733   score: 17.0   memory length: 100000   epsilon: 0.3041210800096152    steps: 652    lr: 6.400000000000001e-06     evaluation reward: 8.93\n",
      "episode: 1734   score: 12.0   memory length: 100000   epsilon: 0.30292714000960763    steps: 603    lr: 6.400000000000001e-06     evaluation reward: 9.02\n",
      "episode: 1735   score: 10.0   memory length: 100000   epsilon: 0.301875760009601    steps: 531    lr: 6.400000000000001e-06     evaluation reward: 8.99\n",
      "episode: 1736   score: 9.0   memory length: 100000   epsilon: 0.3009115000095949    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 9.04\n",
      "episode: 1737   score: 14.0   memory length: 100000   epsilon: 0.299670040009587    steps: 627    lr: 6.400000000000001e-06     evaluation reward: 9.08\n",
      "episode: 1738   score: 7.0   memory length: 100000   epsilon: 0.2988622000095819    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 9.04\n",
      "episode: 1739   score: 18.0   memory length: 100000   epsilon: 0.29741284000957274    steps: 732    lr: 6.400000000000001e-06     evaluation reward: 9.18\n",
      "episode: 1740   score: 14.0   memory length: 100000   epsilon: 0.29642086000956647    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 9.29\n",
      "episode: 1741   score: 13.0   memory length: 100000   epsilon: 0.2953873000095599    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
      "episode: 1742   score: 8.0   memory length: 100000   epsilon: 0.29450224000955433    steps: 447    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
      "episode: 1743   score: 9.0   memory length: 100000   epsilon: 0.293498380009548    steps: 507    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
      "episode: 1744   score: 7.0   memory length: 100000   epsilon: 0.29274994000954324    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 9.36\n",
      "episode: 1745   score: 10.0   memory length: 100000   epsilon: 0.2917342000095368    steps: 513    lr: 6.400000000000001e-06     evaluation reward: 9.33\n",
      "episode: 1746   score: 6.0   memory length: 100000   epsilon: 0.29099566000953214    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 9.34\n",
      "episode: 1747   score: 11.0   memory length: 100000   epsilon: 0.2901185200095266    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
      "episode: 1748   score: 8.0   memory length: 100000   epsilon: 0.28921168000952086    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
      "episode: 1749   score: 21.0   memory length: 100000   epsilon: 0.28761382000951075    steps: 807    lr: 6.400000000000001e-06     evaluation reward: 9.6\n",
      "episode: 1750   score: 15.0   memory length: 100000   epsilon: 0.2864238400095032    steps: 601    lr: 6.400000000000001e-06     evaluation reward: 9.64\n",
      "episode: 1751   score: 16.0   memory length: 100000   epsilon: 0.2852714800094959    steps: 582    lr: 6.400000000000001e-06     evaluation reward: 9.77\n",
      "episode: 1752   score: 22.0   memory length: 100000   epsilon: 0.28377064000948643    steps: 758    lr: 6.400000000000001e-06     evaluation reward: 9.89\n",
      "episode: 1753   score: 9.0   memory length: 100000   epsilon: 0.2827984600094803    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 9.89\n",
      "episode: 1754   score: 11.0   memory length: 100000   epsilon: 0.28176886000947376    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 9.92\n",
      "episode: 1755   score: 8.0   memory length: 100000   epsilon: 0.28097884000946877    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 9.82\n",
      "episode: 1756   score: 4.0   memory length: 100000   epsilon: 0.28045810000946547    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 9.75\n",
      "episode: 1757   score: 13.0   memory length: 100000   epsilon: 0.2791829800094574    steps: 644    lr: 6.400000000000001e-06     evaluation reward: 9.82\n",
      "episode: 1758   score: 8.0   memory length: 100000   epsilon: 0.27830584000945185    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 9.79\n",
      "episode: 1759   score: 8.0   memory length: 100000   epsilon: 0.27738712000944604    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 9.73\n",
      "episode: 1760   score: 8.0   memory length: 100000   epsilon: 0.2767970800094423    steps: 298    lr: 6.400000000000001e-06     evaluation reward: 9.77\n",
      "episode: 1761   score: 6.0   memory length: 100000   epsilon: 0.27612586000943806    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 9.77\n",
      "episode: 1762   score: 6.0   memory length: 100000   epsilon: 0.2754546400094338    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 9.72\n",
      "episode: 1763   score: 10.0   memory length: 100000   epsilon: 0.2744884000094277    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 9.74\n",
      "episode: 1764   score: 7.0   memory length: 100000   epsilon: 0.2737597600094231    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 9.72\n",
      "episode: 1765   score: 7.0   memory length: 100000   epsilon: 0.27291628000941776    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 9.73\n",
      "episode: 1766   score: 12.0   memory length: 100000   epsilon: 0.27177778000941055    steps: 575    lr: 6.400000000000001e-06     evaluation reward: 9.8\n",
      "episode: 1767   score: 11.0   memory length: 100000   epsilon: 0.27068680000940365    steps: 551    lr: 6.400000000000001e-06     evaluation reward: 9.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1768   score: 15.0   memory length: 100000   epsilon: 0.2693384200093951    steps: 681    lr: 6.400000000000001e-06     evaluation reward: 9.88\n",
      "episode: 1769   score: 11.0   memory length: 100000   epsilon: 0.2683900000093891    steps: 479    lr: 6.400000000000001e-06     evaluation reward: 9.89\n",
      "episode: 1770   score: 12.0   memory length: 100000   epsilon: 0.2672812000093821    steps: 560    lr: 6.400000000000001e-06     evaluation reward: 9.93\n",
      "episode: 1771   score: 11.0   memory length: 100000   epsilon: 0.26621596000937536    steps: 538    lr: 6.400000000000001e-06     evaluation reward: 9.94\n",
      "episode: 1772   score: 14.0   memory length: 100000   epsilon: 0.26494876000936735    steps: 640    lr: 6.400000000000001e-06     evaluation reward: 9.99\n",
      "episode: 1773   score: 10.0   memory length: 100000   epsilon: 0.26398846000936127    steps: 485    lr: 6.400000000000001e-06     evaluation reward: 10.02\n",
      "episode: 1774   score: 14.0   memory length: 100000   epsilon: 0.26291134000935446    steps: 544    lr: 6.400000000000001e-06     evaluation reward: 10.04\n",
      "episode: 1775   score: 9.0   memory length: 100000   epsilon: 0.26199262000934864    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 10.02\n",
      "episode: 1776   score: 10.0   memory length: 100000   epsilon: 0.26091748000934184    steps: 543    lr: 6.400000000000001e-06     evaluation reward: 10.07\n",
      "episode: 1777   score: 11.0   memory length: 100000   epsilon: 0.259836400009335    steps: 546    lr: 6.400000000000001e-06     evaluation reward: 10.11\n",
      "episode: 1778   score: 9.0   memory length: 100000   epsilon: 0.2588701600093289    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 10.1\n",
      "episode: 1779   score: 9.0   memory length: 100000   epsilon: 0.257937580009323    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 10.09\n",
      "episode: 1780   score: 7.0   memory length: 100000   epsilon: 0.2571931000093183    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 10.11\n",
      "episode: 1781   score: 5.0   memory length: 100000   epsilon: 0.25654366000931417    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 10.08\n",
      "episode: 1782   score: 11.0   memory length: 100000   epsilon: 0.25540516000930696    steps: 575    lr: 6.400000000000001e-06     evaluation reward: 10.02\n",
      "episode: 1783   score: 9.0   memory length: 100000   epsilon: 0.25459732000930185    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 10.04\n",
      "episode: 1784   score: 5.0   memory length: 100000   epsilon: 0.25401124000929814    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 10.04\n",
      "episode: 1785   score: 11.0   memory length: 100000   epsilon: 0.2529103600092912    steps: 556    lr: 6.400000000000001e-06     evaluation reward: 10.09\n",
      "episode: 1786   score: 14.0   memory length: 100000   epsilon: 0.2518055200092842    steps: 558    lr: 6.400000000000001e-06     evaluation reward: 10.11\n",
      "episode: 1787   score: 6.0   memory length: 100000   epsilon: 0.2511263800092799    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 10.06\n",
      "episode: 1788   score: 8.0   memory length: 100000   epsilon: 0.2503363600092749    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 10.04\n",
      "episode: 1789   score: 15.0   memory length: 100000   epsilon: 0.24889492000926577    steps: 728    lr: 6.400000000000001e-06     evaluation reward: 10.12\n",
      "episode: 1790   score: 13.0   memory length: 100000   epsilon: 0.24764158000925784    steps: 633    lr: 6.400000000000001e-06     evaluation reward: 10.18\n",
      "episode: 1791   score: 12.0   memory length: 100000   epsilon: 0.24649120000925057    steps: 581    lr: 6.400000000000001e-06     evaluation reward: 10.2\n",
      "episode: 1792   score: 13.0   memory length: 100000   epsilon: 0.24551308000924438    steps: 494    lr: 6.400000000000001e-06     evaluation reward: 10.16\n",
      "episode: 1793   score: 12.0   memory length: 100000   epsilon: 0.2443765600092372    steps: 574    lr: 6.400000000000001e-06     evaluation reward: 10.13\n",
      "episode: 1794   score: 12.0   memory length: 100000   epsilon: 0.24325984000923012    steps: 564    lr: 6.400000000000001e-06     evaluation reward: 10.15\n",
      "episode: 1795   score: 10.0   memory length: 100000   epsilon: 0.24237874000922455    steps: 445    lr: 6.400000000000001e-06     evaluation reward: 10.2\n",
      "episode: 1796   score: 14.0   memory length: 100000   epsilon: 0.24133528000921795    steps: 527    lr: 6.400000000000001e-06     evaluation reward: 10.28\n",
      "episode: 1797   score: 8.0   memory length: 100000   epsilon: 0.24043834000921227    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 10.23\n",
      "episode: 1798   score: 8.0   memory length: 100000   epsilon: 0.23959684000920695    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 10.23\n",
      "episode: 1799   score: 8.0   memory length: 100000   epsilon: 0.23884246000920217    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 10.2\n",
      "episode: 1800   score: 13.0   memory length: 100000   epsilon: 0.23765050000919463    steps: 602    lr: 6.400000000000001e-06     evaluation reward: 10.27\n",
      "episode: 1801   score: 16.0   memory length: 100000   epsilon: 0.236601100009188    steps: 530    lr: 6.400000000000001e-06     evaluation reward: 10.29\n",
      "episode: 1802   score: 21.0   memory length: 100000   epsilon: 0.2350052200091779    steps: 806    lr: 6.400000000000001e-06     evaluation reward: 10.41\n",
      "episode: 1803   score: 5.0   memory length: 100000   epsilon: 0.23438548000917397    steps: 313    lr: 6.400000000000001e-06     evaluation reward: 10.3\n",
      "episode: 1804   score: 13.0   memory length: 100000   epsilon: 0.23317570000916632    steps: 611    lr: 6.400000000000001e-06     evaluation reward: 10.31\n",
      "episode: 1805   score: 13.0   memory length: 100000   epsilon: 0.23189662000915823    steps: 646    lr: 6.400000000000001e-06     evaluation reward: 10.36\n",
      "episode: 1806   score: 3.0   memory length: 100000   epsilon: 0.23144320000915536    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 10.31\n",
      "episode: 1807   score: 12.0   memory length: 100000   epsilon: 0.2302967800091481    steps: 579    lr: 6.400000000000001e-06     evaluation reward: 10.27\n",
      "episode: 1808   score: 4.0   memory length: 100000   epsilon: 0.2297760400091448    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 10.24\n",
      "episode: 1809   score: 14.0   memory length: 100000   epsilon: 0.22845340000913644    steps: 668    lr: 6.400000000000001e-06     evaluation reward: 10.28\n",
      "episode: 1810   score: 7.0   memory length: 100000   epsilon: 0.22774654000913197    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 10.28\n",
      "episode: 1811   score: 7.0   memory length: 100000   epsilon: 0.22704364000912752    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 10.28\n",
      "episode: 1812   score: 10.0   memory length: 100000   epsilon: 0.2259962200091209    steps: 529    lr: 6.400000000000001e-06     evaluation reward: 10.25\n",
      "episode: 1813   score: 7.0   memory length: 100000   epsilon: 0.22525768000911622    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 10.23\n",
      "episode: 1814   score: 9.0   memory length: 100000   epsilon: 0.22434292000911044    steps: 462    lr: 6.400000000000001e-06     evaluation reward: 10.25\n",
      "episode: 1815   score: 10.0   memory length: 100000   epsilon: 0.22333114000910403    steps: 511    lr: 6.400000000000001e-06     evaluation reward: 10.21\n",
      "episode: 1816   score: 12.0   memory length: 100000   epsilon: 0.22232728000909768    steps: 507    lr: 6.400000000000001e-06     evaluation reward: 10.22\n",
      "episode: 1817   score: 10.0   memory length: 100000   epsilon: 0.22129174000909113    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 10.18\n",
      "episode: 1818   score: 11.0   memory length: 100000   epsilon: 0.22020472000908425    steps: 549    lr: 6.400000000000001e-06     evaluation reward: 10.21\n",
      "episode: 1819   score: 9.0   memory length: 100000   epsilon: 0.21923452000907812    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 10.25\n",
      "episode: 1820   score: 8.0   memory length: 100000   epsilon: 0.2184425200090731    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 10.28\n",
      "episode: 1821   score: 5.0   memory length: 100000   epsilon: 0.217795060009069    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 10.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1822   score: 9.0   memory length: 100000   epsilon: 0.21685654000906307    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 10.25\n",
      "episode: 1823   score: 11.0   memory length: 100000   epsilon: 0.21577744000905624    steps: 545    lr: 6.400000000000001e-06     evaluation reward: 10.26\n",
      "episode: 1824   score: 17.0   memory length: 100000   epsilon: 0.2144251000090477    steps: 683    lr: 6.400000000000001e-06     evaluation reward: 10.38\n",
      "episode: 1825   score: 11.0   memory length: 100000   epsilon: 0.21334600000904086    steps: 545    lr: 6.400000000000001e-06     evaluation reward: 10.39\n",
      "episode: 1826   score: 9.0   memory length: 100000   epsilon: 0.21247876000903537    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 10.38\n",
      "episode: 1827   score: 23.0   memory length: 100000   epsilon: 0.2108888200090253    steps: 803    lr: 6.400000000000001e-06     evaluation reward: 10.5\n",
      "episode: 1828   score: 12.0   memory length: 100000   epsilon: 0.2097028000090178    steps: 599    lr: 6.400000000000001e-06     evaluation reward: 10.51\n",
      "episode: 1829   score: 15.0   memory length: 100000   epsilon: 0.2085009400090102    steps: 607    lr: 6.400000000000001e-06     evaluation reward: 10.56\n",
      "episode: 1830   score: 14.0   memory length: 100000   epsilon: 0.20724760000900228    steps: 633    lr: 2.560000000000001e-06     evaluation reward: 10.64\n",
      "episode: 1831   score: 22.0   memory length: 100000   epsilon: 0.205624000008992    steps: 820    lr: 2.560000000000001e-06     evaluation reward: 10.78\n",
      "episode: 1832   score: 13.0   memory length: 100000   epsilon: 0.2044241200089844    steps: 606    lr: 2.560000000000001e-06     evaluation reward: 10.86\n",
      "episode: 1833   score: 8.0   memory length: 100000   epsilon: 0.20344600000897822    steps: 494    lr: 2.560000000000001e-06     evaluation reward: 10.77\n",
      "episode: 1834   score: 15.0   memory length: 100000   epsilon: 0.20242036000897173    steps: 518    lr: 2.560000000000001e-06     evaluation reward: 10.8\n",
      "episode: 1835   score: 10.0   memory length: 100000   epsilon: 0.2014501600089656    steps: 490    lr: 2.560000000000001e-06     evaluation reward: 10.8\n",
      "episode: 1836   score: 7.0   memory length: 100000   epsilon: 0.2006462800089605    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 10.78\n",
      "episode: 1837   score: 16.0   memory length: 100000   epsilon: 0.1996642000089543    steps: 496    lr: 2.560000000000001e-06     evaluation reward: 10.8\n",
      "episode: 1838   score: 10.0   memory length: 100000   epsilon: 0.19868806000894812    steps: 493    lr: 2.560000000000001e-06     evaluation reward: 10.83\n",
      "episode: 1839   score: 10.0   memory length: 100000   epsilon: 0.197721820008942    steps: 488    lr: 2.560000000000001e-06     evaluation reward: 10.75\n",
      "episode: 1840   score: 15.0   memory length: 100000   epsilon: 0.19669618000893552    steps: 518    lr: 2.560000000000001e-06     evaluation reward: 10.76\n",
      "episode: 1841   score: 14.0   memory length: 100000   epsilon: 0.19540720000892736    steps: 651    lr: 2.560000000000001e-06     evaluation reward: 10.77\n",
      "episode: 1842   score: 8.0   memory length: 100000   epsilon: 0.19447264000892145    steps: 472    lr: 2.560000000000001e-06     evaluation reward: 10.77\n",
      "episode: 1843   score: 21.0   memory length: 100000   epsilon: 0.19313020000891296    steps: 678    lr: 2.560000000000001e-06     evaluation reward: 10.89\n",
      "episode: 1844   score: 6.0   memory length: 100000   epsilon: 0.19243522000890856    steps: 351    lr: 2.560000000000001e-06     evaluation reward: 10.88\n",
      "episode: 1845   score: 11.0   memory length: 100000   epsilon: 0.19134820000890168    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 10.89\n",
      "episode: 1846   score: 23.0   memory length: 100000   epsilon: 0.1901483200088941    steps: 606    lr: 2.560000000000001e-06     evaluation reward: 11.06\n",
      "episode: 1847   score: 13.0   memory length: 100000   epsilon: 0.18913456000888768    steps: 512    lr: 2.560000000000001e-06     evaluation reward: 11.08\n",
      "episode: 1848   score: 18.0   memory length: 100000   epsilon: 0.18764956000887828    steps: 750    lr: 2.560000000000001e-06     evaluation reward: 11.18\n",
      "episode: 1849   score: 11.0   memory length: 100000   epsilon: 0.18657046000887145    steps: 545    lr: 2.560000000000001e-06     evaluation reward: 11.08\n",
      "episode: 1850   score: 11.0   memory length: 100000   epsilon: 0.18551116000886475    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 11.04\n",
      "episode: 1851   score: 11.0   memory length: 100000   epsilon: 0.1844776000088582    steps: 522    lr: 2.560000000000001e-06     evaluation reward: 10.99\n",
      "episode: 1852   score: 26.0   memory length: 100000   epsilon: 0.18267976000884684    steps: 908    lr: 2.560000000000001e-06     evaluation reward: 11.03\n",
      "episode: 1853   score: 15.0   memory length: 100000   epsilon: 0.18147790000883923    steps: 607    lr: 2.560000000000001e-06     evaluation reward: 11.09\n",
      "episode: 1854   score: 8.0   memory length: 100000   epsilon: 0.18063046000883387    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 11.06\n",
      "episode: 1855   score: 7.0   memory length: 100000   epsilon: 0.17988598000882916    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 11.05\n",
      "episode: 1856   score: 14.0   memory length: 100000   epsilon: 0.17898706000882347    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 11.15\n",
      "episode: 1857   score: 12.0   memory length: 100000   epsilon: 0.17792776000881677    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 11.14\n",
      "episode: 1858   score: 8.0   memory length: 100000   epsilon: 0.17705854000881127    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 11.14\n",
      "episode: 1859   score: 18.0   memory length: 100000   epsilon: 0.17584282000880358    steps: 614    lr: 2.560000000000001e-06     evaluation reward: 11.24\n",
      "episode: 1860   score: 11.0   memory length: 100000   epsilon: 0.1747855000087969    steps: 534    lr: 2.560000000000001e-06     evaluation reward: 11.27\n",
      "episode: 1861   score: 15.0   memory length: 100000   epsilon: 0.17348068000878863    steps: 659    lr: 2.560000000000001e-06     evaluation reward: 11.36\n",
      "episode: 1862   score: 9.0   memory length: 100000   epsilon: 0.1726075000087831    steps: 441    lr: 2.560000000000001e-06     evaluation reward: 11.39\n",
      "episode: 1863   score: 15.0   memory length: 100000   epsilon: 0.17155414000877645    steps: 532    lr: 2.560000000000001e-06     evaluation reward: 11.44\n",
      "episode: 1864   score: 16.0   memory length: 100000   epsilon: 0.17041762000876925    steps: 574    lr: 2.560000000000001e-06     evaluation reward: 11.53\n",
      "episode: 1865   score: 7.0   memory length: 100000   epsilon: 0.16967512000876456    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 11.53\n",
      "episode: 1866   score: 13.0   memory length: 100000   epsilon: 0.16862770000875793    steps: 529    lr: 2.560000000000001e-06     evaluation reward: 11.54\n",
      "episode: 1867   score: 17.0   memory length: 100000   epsilon: 0.1671387400087485    steps: 752    lr: 2.560000000000001e-06     evaluation reward: 11.6\n",
      "episode: 1868   score: 19.0   memory length: 100000   epsilon: 0.16579828000874003    steps: 677    lr: 2.560000000000001e-06     evaluation reward: 11.64\n",
      "episode: 1869   score: 9.0   memory length: 100000   epsilon: 0.16491322000873443    steps: 447    lr: 2.560000000000001e-06     evaluation reward: 11.62\n",
      "episode: 1870   score: 7.0   memory length: 100000   epsilon: 0.16412320000872943    steps: 399    lr: 2.560000000000001e-06     evaluation reward: 11.57\n",
      "episode: 1871   score: 13.0   memory length: 100000   epsilon: 0.16285006000872138    steps: 643    lr: 2.560000000000001e-06     evaluation reward: 11.59\n",
      "episode: 1872   score: 23.0   memory length: 100000   epsilon: 0.1613848600087121    steps: 740    lr: 2.560000000000001e-06     evaluation reward: 11.68\n",
      "episode: 1873   score: 7.0   memory length: 100000   epsilon: 0.16057504000870698    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 11.65\n",
      "episode: 1874   score: 3.0   memory length: 100000   epsilon: 0.16007806000870384    steps: 251    lr: 2.560000000000001e-06     evaluation reward: 11.54\n",
      "episode: 1875   score: 14.0   memory length: 100000   epsilon: 0.15873958000869537    steps: 676    lr: 2.560000000000001e-06     evaluation reward: 11.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1876   score: 11.0   memory length: 100000   epsilon: 0.15769414000868875    steps: 528    lr: 2.560000000000001e-06     evaluation reward: 11.6\n",
      "episode: 1877   score: 8.0   memory length: 100000   epsilon: 0.1568645200086835    steps: 419    lr: 2.560000000000001e-06     evaluation reward: 11.57\n",
      "episode: 1878   score: 14.0   memory length: 100000   epsilon: 0.1555696000086753    steps: 654    lr: 2.560000000000001e-06     evaluation reward: 11.62\n",
      "episode: 1879   score: 12.0   memory length: 100000   epsilon: 0.1544786200086684    steps: 551    lr: 2.560000000000001e-06     evaluation reward: 11.65\n",
      "episode: 1880   score: 15.0   memory length: 100000   epsilon: 0.15340942000866165    steps: 540    lr: 2.560000000000001e-06     evaluation reward: 11.73\n",
      "episode: 1881   score: 21.0   memory length: 100000   epsilon: 0.151882840008652    steps: 771    lr: 2.560000000000001e-06     evaluation reward: 11.89\n",
      "episode: 1882   score: 18.0   memory length: 100000   epsilon: 0.15047308000864307    steps: 712    lr: 2.560000000000001e-06     evaluation reward: 11.96\n",
      "episode: 1883   score: 21.0   memory length: 100000   epsilon: 0.14919004000863495    steps: 648    lr: 2.560000000000001e-06     evaluation reward: 12.08\n",
      "episode: 1884   score: 14.0   memory length: 100000   epsilon: 0.14793076000862698    steps: 636    lr: 2.560000000000001e-06     evaluation reward: 12.17\n",
      "episode: 1885   score: 21.0   memory length: 100000   epsilon: 0.1464814000086178    steps: 732    lr: 2.560000000000001e-06     evaluation reward: 12.27\n",
      "episode: 1886   score: 14.0   memory length: 100000   epsilon: 0.14533300000861055    steps: 580    lr: 2.560000000000001e-06     evaluation reward: 12.27\n",
      "episode: 1887   score: 13.0   memory length: 100000   epsilon: 0.14420638000860342    steps: 569    lr: 2.560000000000001e-06     evaluation reward: 12.34\n",
      "episode: 1888   score: 6.0   memory length: 100000   epsilon: 0.14353318000859916    steps: 340    lr: 2.560000000000001e-06     evaluation reward: 12.32\n",
      "episode: 1889   score: 14.0   memory length: 100000   epsilon: 0.1425451600085929    steps: 499    lr: 2.560000000000001e-06     evaluation reward: 12.31\n",
      "episode: 1890   score: 15.0   memory length: 100000   epsilon: 0.14145814000858603    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 12.33\n",
      "episode: 1891   score: 9.0   memory length: 100000   epsilon: 0.1405216000085801    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 12.3\n",
      "episode: 1892   score: 12.0   memory length: 100000   epsilon: 0.1396365400085745    steps: 447    lr: 2.560000000000001e-06     evaluation reward: 12.29\n",
      "episode: 1893   score: 14.0   memory length: 100000   epsilon: 0.1384010200085667    steps: 624    lr: 2.560000000000001e-06     evaluation reward: 12.31\n",
      "episode: 1894   score: 13.0   memory length: 100000   epsilon: 0.13727440000855956    steps: 569    lr: 2.560000000000001e-06     evaluation reward: 12.32\n",
      "episode: 1895   score: 5.0   memory length: 100000   epsilon: 0.1367279200085561    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 12.27\n",
      "episode: 1896   score: 9.0   memory length: 100000   epsilon: 0.13576564000855001    steps: 486    lr: 2.560000000000001e-06     evaluation reward: 12.22\n",
      "episode: 1897   score: 11.0   memory length: 100000   epsilon: 0.13479544000854388    steps: 490    lr: 2.560000000000001e-06     evaluation reward: 12.25\n",
      "episode: 1898   score: 12.0   memory length: 100000   epsilon: 0.133710400008537    steps: 548    lr: 2.560000000000001e-06     evaluation reward: 12.29\n",
      "episode: 1899   score: 10.0   memory length: 100000   epsilon: 0.13274812000853092    steps: 486    lr: 2.560000000000001e-06     evaluation reward: 12.31\n",
      "episode: 1900   score: 17.0   memory length: 100000   epsilon: 0.13143538000852262    steps: 663    lr: 2.560000000000001e-06     evaluation reward: 12.35\n",
      "episode: 1901   score: 15.0   memory length: 100000   epsilon: 0.13020382000851483    steps: 622    lr: 2.560000000000001e-06     evaluation reward: 12.34\n",
      "episode: 1902   score: 12.0   memory length: 100000   epsilon: 0.1289821600085071    steps: 617    lr: 2.560000000000001e-06     evaluation reward: 12.25\n",
      "episode: 1903   score: 7.0   memory length: 100000   epsilon: 0.1282396600085024    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 12.27\n",
      "episode: 1904   score: 16.0   memory length: 100000   epsilon: 0.12688930000849385    steps: 682    lr: 2.560000000000001e-06     evaluation reward: 12.3\n",
      "episode: 1905   score: 18.0   memory length: 100000   epsilon: 0.12553102000848526    steps: 686    lr: 2.560000000000001e-06     evaluation reward: 12.35\n",
      "episode: 1906   score: 21.0   memory length: 100000   epsilon: 0.12401632000848258    steps: 765    lr: 2.560000000000001e-06     evaluation reward: 12.53\n",
      "episode: 1907   score: 8.0   memory length: 100000   epsilon: 0.12318076000848314    steps: 422    lr: 2.560000000000001e-06     evaluation reward: 12.49\n",
      "episode: 1908   score: 7.0   memory length: 100000   epsilon: 0.12240658000848367    steps: 391    lr: 2.560000000000001e-06     evaluation reward: 12.52\n",
      "episode: 1909   score: 8.0   memory length: 100000   epsilon: 0.12162052000848421    steps: 397    lr: 2.560000000000001e-06     evaluation reward: 12.46\n",
      "episode: 1910   score: 14.0   memory length: 100000   epsilon: 0.120458260008485    steps: 587    lr: 2.560000000000001e-06     evaluation reward: 12.53\n",
      "episode: 1911   score: 19.0   memory length: 100000   epsilon: 0.11907424000848595    steps: 699    lr: 2.560000000000001e-06     evaluation reward: 12.65\n",
      "episode: 1912   score: 11.0   memory length: 100000   epsilon: 0.11807434000848663    steps: 505    lr: 2.560000000000001e-06     evaluation reward: 12.66\n",
      "episode: 1913   score: 15.0   memory length: 100000   epsilon: 0.11700514000848736    steps: 540    lr: 2.560000000000001e-06     evaluation reward: 12.74\n",
      "episode: 1914   score: 10.0   memory length: 100000   epsilon: 0.11608642000848798    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 12.75\n",
      "episode: 1915   score: 5.0   memory length: 100000   epsilon: 0.11550430000848838    steps: 294    lr: 2.560000000000001e-06     evaluation reward: 12.7\n",
      "episode: 1916   score: 10.0   memory length: 100000   epsilon: 0.11450242000848906    steps: 506    lr: 2.560000000000001e-06     evaluation reward: 12.68\n",
      "episode: 1917   score: 13.0   memory length: 100000   epsilon: 0.11338174000848983    steps: 566    lr: 2.560000000000001e-06     evaluation reward: 12.71\n",
      "episode: 1918   score: 16.0   memory length: 100000   epsilon: 0.1119541600084908    steps: 721    lr: 2.560000000000001e-06     evaluation reward: 12.76\n",
      "episode: 1919   score: 15.0   memory length: 100000   epsilon: 0.11072260000849164    steps: 622    lr: 2.560000000000001e-06     evaluation reward: 12.82\n",
      "episode: 1920   score: 9.0   memory length: 100000   epsilon: 0.10988506000849221    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 12.83\n",
      "episode: 1921   score: 7.0   memory length: 100000   epsilon: 0.10911484000849274    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 12.85\n",
      "episode: 1922   score: 7.0   memory length: 100000   epsilon: 0.1082931400084933    steps: 415    lr: 2.560000000000001e-06     evaluation reward: 12.83\n",
      "episode: 1923   score: 20.0   memory length: 100000   epsilon: 0.10690714000849424    steps: 700    lr: 2.560000000000001e-06     evaluation reward: 12.92\n",
      "episode: 1924   score: 15.0   memory length: 100000   epsilon: 0.10566964000849509    steps: 625    lr: 2.560000000000001e-06     evaluation reward: 12.9\n",
      "episode: 1925   score: 17.0   memory length: 100000   epsilon: 0.10438660000849596    steps: 648    lr: 2.560000000000001e-06     evaluation reward: 12.96\n",
      "episode: 1926   score: 12.0   memory length: 100000   epsilon: 0.10361638000849649    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 12.99\n",
      "episode: 1927   score: 17.0   memory length: 100000   epsilon: 0.10235512000849735    steps: 637    lr: 2.560000000000001e-06     evaluation reward: 12.93\n",
      "episode: 1928   score: 10.0   memory length: 100000   epsilon: 0.10144432000849797    steps: 460    lr: 2.560000000000001e-06     evaluation reward: 12.91\n",
      "episode: 1929   score: 17.0   memory length: 100000   epsilon: 0.10012564000849887    steps: 666    lr: 2.560000000000001e-06     evaluation reward: 12.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1930   score: 11.0   memory length: 100000   epsilon: 0.09907624000849959    steps: 530    lr: 2.560000000000001e-06     evaluation reward: 12.9\n",
      "episode: 1931   score: 8.0   memory length: 100000   epsilon: 0.09809020000850026    steps: 498    lr: 2.560000000000001e-06     evaluation reward: 12.76\n",
      "episode: 1932   score: 22.0   memory length: 100000   epsilon: 0.0967061800085012    steps: 699    lr: 2.560000000000001e-06     evaluation reward: 12.85\n",
      "episode: 1933   score: 8.0   memory length: 100000   epsilon: 0.09588052000850177    steps: 417    lr: 2.560000000000001e-06     evaluation reward: 12.85\n",
      "episode: 1934   score: 13.0   memory length: 100000   epsilon: 0.09469054000850258    steps: 601    lr: 2.560000000000001e-06     evaluation reward: 12.83\n",
      "episode: 1935   score: 8.0   memory length: 100000   epsilon: 0.09382330000850317    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 12.81\n",
      "episode: 1936   score: 11.0   memory length: 100000   epsilon: 0.09278776000850388    steps: 523    lr: 2.560000000000001e-06     evaluation reward: 12.85\n",
      "episode: 1937   score: 10.0   memory length: 100000   epsilon: 0.09182350000850453    steps: 487    lr: 2.560000000000001e-06     evaluation reward: 12.79\n",
      "episode: 1938   score: 15.0   memory length: 100000   epsilon: 0.09053848000850541    steps: 649    lr: 2.560000000000001e-06     evaluation reward: 12.84\n",
      "episode: 1939   score: 18.0   memory length: 100000   epsilon: 0.08921782000850631    steps: 667    lr: 2.560000000000001e-06     evaluation reward: 12.92\n",
      "episode: 1940   score: 10.0   memory length: 100000   epsilon: 0.08833870000850691    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 12.87\n",
      "episode: 1941   score: 6.0   memory length: 100000   epsilon: 0.08764570000850738    steps: 350    lr: 2.560000000000001e-06     evaluation reward: 12.79\n",
      "episode: 1942   score: 12.0   memory length: 100000   epsilon: 0.08649928000850816    steps: 579    lr: 2.560000000000001e-06     evaluation reward: 12.83\n",
      "episode: 1943   score: 11.0   memory length: 100000   epsilon: 0.08540434000850891    steps: 553    lr: 2.560000000000001e-06     evaluation reward: 12.73\n",
      "episode: 1944   score: 10.0   memory length: 100000   epsilon: 0.08462422000850944    steps: 394    lr: 2.560000000000001e-06     evaluation reward: 12.77\n",
      "episode: 1945   score: 10.0   memory length: 100000   epsilon: 0.08370748000851007    steps: 463    lr: 2.560000000000001e-06     evaluation reward: 12.76\n",
      "episode: 1946   score: 18.0   memory length: 100000   epsilon: 0.08237098000851098    steps: 675    lr: 2.560000000000001e-06     evaluation reward: 12.71\n",
      "episode: 1947   score: 18.0   memory length: 100000   epsilon: 0.0810127000085119    steps: 686    lr: 2.560000000000001e-06     evaluation reward: 12.76\n",
      "episode: 1948   score: 17.0   memory length: 100000   epsilon: 0.07978510000851274    steps: 620    lr: 2.560000000000001e-06     evaluation reward: 12.75\n",
      "episode: 1949   score: 8.0   memory length: 100000   epsilon: 0.0788129200085134    steps: 491    lr: 2.560000000000001e-06     evaluation reward: 12.72\n",
      "episode: 1950   score: 9.0   memory length: 100000   epsilon: 0.07787044000851405    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 12.7\n",
      "episode: 1951   score: 19.0   memory length: 100000   epsilon: 0.076472560008515    steps: 706    lr: 2.560000000000001e-06     evaluation reward: 12.78\n",
      "episode: 1952   score: 13.0   memory length: 100000   epsilon: 0.07527466000851582    steps: 605    lr: 2.560000000000001e-06     evaluation reward: 12.65\n",
      "episode: 1953   score: 7.0   memory length: 100000   epsilon: 0.07452226000851633    steps: 380    lr: 2.560000000000001e-06     evaluation reward: 12.57\n",
      "episode: 1954   score: 17.0   memory length: 100000   epsilon: 0.07323526000851721    steps: 650    lr: 2.560000000000001e-06     evaluation reward: 12.66\n",
      "episode: 1955   score: 9.0   memory length: 100000   epsilon: 0.07235020000851781    steps: 447    lr: 2.560000000000001e-06     evaluation reward: 12.68\n",
      "episode: 1956   score: 13.0   memory length: 100000   epsilon: 0.07126912000851855    steps: 546    lr: 2.560000000000001e-06     evaluation reward: 12.67\n",
      "episode: 1957   score: 19.0   memory length: 100000   epsilon: 0.06984550000851952    steps: 719    lr: 2.560000000000001e-06     evaluation reward: 12.74\n",
      "episode: 1958   score: 12.0   memory length: 100000   epsilon: 0.06877036000852026    steps: 543    lr: 2.560000000000001e-06     evaluation reward: 12.78\n",
      "episode: 1959   score: 11.0   memory length: 100000   epsilon: 0.06771502000852098    steps: 533    lr: 2.560000000000001e-06     evaluation reward: 12.71\n",
      "episode: 1960   score: 22.0   memory length: 100000   epsilon: 0.0663508000085219    steps: 689    lr: 2.560000000000001e-06     evaluation reward: 12.82\n",
      "episode: 1961   score: 12.0   memory length: 100000   epsilon: 0.0653390200085226    steps: 511    lr: 2.560000000000001e-06     evaluation reward: 12.79\n",
      "episode: 1962   score: 8.0   memory length: 100000   epsilon: 0.06457870000852312    steps: 384    lr: 2.560000000000001e-06     evaluation reward: 12.78\n",
      "episode: 1963   score: 9.0   memory length: 100000   epsilon: 0.06363622000852376    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 12.72\n",
      "episode: 1964   score: 6.0   memory length: 100000   epsilon: 0.06296500000852422    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 12.62\n",
      "episode: 1965   score: 12.0   memory length: 100000   epsilon: 0.06172354000852506    steps: 627    lr: 2.560000000000001e-06     evaluation reward: 12.67\n",
      "episode: 1966   score: 15.0   memory length: 100000   epsilon: 0.060343480008526004    steps: 697    lr: 2.560000000000001e-06     evaluation reward: 12.69\n",
      "episode: 1967   score: 23.0   memory length: 100000   epsilon: 0.05885254000852702    steps: 753    lr: 2.560000000000001e-06     evaluation reward: 12.75\n",
      "episode: 1968   score: 12.0   memory length: 100000   epsilon: 0.05775166000852777    steps: 556    lr: 2.560000000000001e-06     evaluation reward: 12.68\n",
      "episode: 1969   score: 14.0   memory length: 100000   epsilon: 0.05641912000852868    steps: 673    lr: 2.560000000000001e-06     evaluation reward: 12.73\n",
      "episode: 1970   score: 22.0   memory length: 100000   epsilon: 0.05484106000852976    steps: 797    lr: 2.560000000000001e-06     evaluation reward: 12.88\n",
      "episode: 1971   score: 9.0   memory length: 100000   epsilon: 0.05383720000853044    steps: 507    lr: 2.560000000000001e-06     evaluation reward: 12.84\n",
      "episode: 1972   score: 11.0   memory length: 100000   epsilon: 0.05280166000853115    steps: 523    lr: 2.560000000000001e-06     evaluation reward: 12.72\n",
      "episode: 1973   score: 10.0   memory length: 100000   epsilon: 0.051766120008531855    steps: 523    lr: 2.560000000000001e-06     evaluation reward: 12.75\n",
      "episode: 1974   score: 14.0   memory length: 100000   epsilon: 0.0505306000085327    steps: 624    lr: 2.560000000000001e-06     evaluation reward: 12.86\n",
      "episode: 1975   score: 17.0   memory length: 100000   epsilon: 0.04914856000853364    steps: 698    lr: 2.560000000000001e-06     evaluation reward: 12.89\n",
      "episode: 1976   score: 11.0   memory length: 100000   epsilon: 0.04808926000853436    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 12.89\n",
      "episode: 1977   score: 11.0   memory length: 100000   epsilon: 0.04710718000853503    steps: 496    lr: 2.560000000000001e-06     evaluation reward: 12.92\n",
      "episode: 1978   score: 16.0   memory length: 100000   epsilon: 0.045624160008536044    steps: 749    lr: 2.560000000000001e-06     evaluation reward: 12.94\n",
      "episode: 1979   score: 16.0   memory length: 100000   epsilon: 0.04446784000853683    steps: 584    lr: 2.560000000000001e-06     evaluation reward: 12.98\n",
      "episode: 1980   score: 22.0   memory length: 100000   epsilon: 0.04298878000853784    steps: 747    lr: 2.560000000000001e-06     evaluation reward: 13.05\n",
      "episode: 1981   score: 14.0   memory length: 100000   epsilon: 0.041709700008538714    steps: 646    lr: 2.560000000000001e-06     evaluation reward: 12.98\n",
      "episode: 1982   score: 21.0   memory length: 100000   epsilon: 0.040375180008539624    steps: 674    lr: 2.560000000000001e-06     evaluation reward: 13.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1983   score: 9.0   memory length: 100000   epsilon: 0.039537640008540195    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 12.89\n",
      "episode: 1984   score: 11.0   memory length: 100000   epsilon: 0.03841696000854096    steps: 566    lr: 2.560000000000001e-06     evaluation reward: 12.86\n",
      "episode: 1985   score: 10.0   memory length: 100000   epsilon: 0.03742300000854164    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 12.75\n",
      "episode: 1986   score: 12.0   memory length: 100000   epsilon: 0.03621520000854246    steps: 610    lr: 2.560000000000001e-06     evaluation reward: 12.73\n",
      "episode: 1987   score: 13.0   memory length: 100000   epsilon: 0.03499948000854329    steps: 614    lr: 2.560000000000001e-06     evaluation reward: 12.73\n",
      "episode: 1988   score: 11.0   memory length: 100000   epsilon: 0.03388672000854405    steps: 562    lr: 2.560000000000001e-06     evaluation reward: 12.78\n",
      "episode: 1989   score: 32.0   memory length: 100000   epsilon: 0.03236608000854509    steps: 768    lr: 2.560000000000001e-06     evaluation reward: 12.96\n",
      "episode: 1990   score: 7.0   memory length: 100000   epsilon: 0.03152062000854566    steps: 427    lr: 2.560000000000001e-06     evaluation reward: 12.88\n",
      "episode: 1991   score: 9.0   memory length: 100000   epsilon: 0.030627640008546272    steps: 451    lr: 2.560000000000001e-06     evaluation reward: 12.88\n",
      "episode: 1992   score: 11.0   memory length: 100000   epsilon: 0.029512900008547033    steps: 563    lr: 2.560000000000001e-06     evaluation reward: 12.87\n",
      "episode: 1993   score: 11.0   memory length: 100000   epsilon: 0.028471420008547743    steps: 526    lr: 2.560000000000001e-06     evaluation reward: 12.84\n",
      "episode: 1994   score: 14.0   memory length: 100000   epsilon: 0.027251740008548575    steps: 616    lr: 2.560000000000001e-06     evaluation reward: 12.85\n",
      "episode: 1995   score: 24.0   memory length: 100000   epsilon: 0.025770700008549585    steps: 748    lr: 2.560000000000001e-06     evaluation reward: 13.04\n",
      "episode: 1996   score: 13.0   memory length: 100000   epsilon: 0.024642100008550355    steps: 570    lr: 2.560000000000001e-06     evaluation reward: 13.08\n",
      "episode: 1997   score: 13.0   memory length: 100000   epsilon: 0.023412520008551194    steps: 621    lr: 2.560000000000001e-06     evaluation reward: 13.1\n",
      "episode: 1998   score: 11.0   memory length: 100000   epsilon: 0.02241856000855187    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 13.09\n",
      "episode: 1999   score: 9.0   memory length: 100000   epsilon: 0.021450340008552532    steps: 489    lr: 2.560000000000001e-06     evaluation reward: 13.08\n",
      "episode: 2000   score: 13.0   memory length: 100000   epsilon: 0.020111860008553445    steps: 676    lr: 2.560000000000001e-06     evaluation reward: 13.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_200/2814442217.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# Start training after random sample generation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[1;31m# Update the target network only for Double DQN only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\assignment5\\assignment5_materials\\agent_double.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_ddqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_ddqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net이, f\"./save_model/breakout_DDQN_2000_episodes.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
